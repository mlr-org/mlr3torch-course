<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Deep Learning with mlr3 &amp; torch - Optimization &amp; Regularization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Deep Learning with mlr3 &amp; torch</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-general" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">General</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-general">    
        <li>
    <a class="dropdown-item" href="../notebooks/setup-guide.html" rel="" target="">
 <span class="dropdown-text">Setup</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/resources.html" rel="" target="">
 <span class="dropdown-text">Resources</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-tutorials" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Tutorials</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-tutorials">    
        <li>
    <a class="dropdown-item" href="../notebooks/1-tensor.html" rel="" target="">
 <span class="dropdown-text">1. Tensors</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/2-autograd.html" rel="" target="">
 <span class="dropdown-text">2. Autograd</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/3-modules-data.html" rel="" target="">
 <span class="dropdown-text">3. Modules and Data</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/4-optimizer.html" rel="" target="">
 <span class="dropdown-text">4. Optimization &amp; Regularization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/5-cnn.html" rel="" target="">
 <span class="dropdown-text">5. CNNs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/6-mlr3torch.html" rel="" target="">
 <span class="dropdown-text">6. mlr3torch</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/7-training-efficiency.html" rel="" target="">
 <span class="dropdown-text">7. Training Efficiency</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-exercises" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Exercises</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-exercises">    
        <li>
    <a class="dropdown-item" href="../notebooks/0-exercise-intro.html" rel="" target="">
 <span class="dropdown-text">Intro</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../notebooks/1-tensor-exercise-task.html" rel="" target="">
 <span class="dropdown-text">1. Tensors</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/2-autograd-exercise-task.html" rel="" target="">
 <span class="dropdown-text">2. Autograd</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/3-modules-data-exercise-task.html" rel="" target="">
 <span class="dropdown-text">3. Modules and Data</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/4-optimizer-exercise-task.html" rel="" target="">
 <span class="dropdown-text">4. Optimization &amp; Regularization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/5-cnn-exercise-task.html" rel="" target="">
 <span class="dropdown-text">5. CNNs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/6-mlr3torch-exercise-task.html" rel="" target="">
 <span class="dropdown-text">6. mlr3torch</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/7-training-efficiency-exercise-task.html" rel="" target="">
 <span class="dropdown-text">7. Training Efficiency</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-solutions" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Solutions</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-solutions">    
        <li>
    <a class="dropdown-item" href="../notebooks/1-tensor-exercise-solution.html" rel="" target="">
 <span class="dropdown-text">1. Tensors</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/2-autograd-exercise-solution.html" rel="" target="">
 <span class="dropdown-text">2. Autograd</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/3-modules-data-exercise-solution.html" rel="" target="">
 <span class="dropdown-text">3. Modules and Data</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/4-optimizer-exercise-solution.html" rel="" target="">
 <span class="dropdown-text">4. Optimization &amp; Regularization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/5-cnn-exercise-solution.html" rel="" target="">
 <span class="dropdown-text">5. CNNs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/6-mlr3torch-exercise-solution.html" rel="" target="">
 <span class="dropdown-text">6. mlr3torch</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../notebooks/7-training-efficiency-exercise-solution.html" rel="" target="">
 <span class="dropdown-text">7. Training Efficiency</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mlr-org/mlr3torch-course" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Optimization &amp; Regularization</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In this notebook, we will focus on the <em>optimization</em> and <em>regularization</em> aspects of deep learning.</p>
<p>Optimizers are algorithms that iteratively adjust the parameters of a neural network to minimize the loss function during training. They define how a network <em>learns</em> from data.</p>
<p>Let’s denote <span class="math inline">\(\hat{\mathcal{R}}(\theta)\)</span> as the empirical risk function, which assigns the empirical risk given data <span class="math inline">\(\{(x^{(i)}, y^{(i)})\}_{i = 1}^n\)</span> to a parameter vector <span class="math inline">\(\theta\)</span>. Here, <span class="math inline">\(f_\theta\)</span> is the model’s prediction function, <span class="math inline">\(x^{(i)}\)</span> is the <span class="math inline">\(i\)</span>-th sample in the training data, and <span class="math inline">\(y^{(i)}\)</span> is the corresponding target value. <span class="math display">\[\hat{\mathcal{R}}(\theta) = \frac{1}{n} \sum_{i=1}^n L(f_\theta(x^{(i)}), y^{(i)}) \]</span></p>
<p>Often, the empirical risk function is extended with a <strong>regularization term</strong>. Regularization in machine learning and statistics is used to prevent overfitting by adding a penalty term to the risk function, which discourages overly complex models that might fit noise in the training data. It helps improve generalization to unseen data. One common regularizer is the L2 norm of the parameter vector, which penalizes large coefficients by adding the squared magnitude of the coefficients to the loss function:</p>
<p><span class="math display">\[
\hat{\mathcal{R}}_{\text{reg}}(\theta) = \hat{\mathcal{R}}(\theta) + \lambda \sum_{j=1}^p \theta_j^2
\]</span></p>
<p>Here, <span class="math inline">\(\lambda\)</span> controls the strength of the regularization, i.e., the trade-off between fitting the training data and keeping the parameters small. This encourages the model to prefer less complex solutions, where complexity is measured by the L2 norm of the coefficients. As a result, parameter vectors will have entries closer to the zero vector, a concept known as parameter shrinkage.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quiz: Regularization
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Question 1</strong>: Different Scales: Is it a problem for L2-regularization when the features have different scales? What can we do about it?</p>
<details>
<summary>
Click for answer
</summary>
Yes, because the size of the parameter vector depends on the scale of the features. Consider the marginal effect of an additional cent on GDP vs.&nbsp;the effect of an additional euro. To avoid this, we can normalize the features so they all have the same scale.
</details>
</div>
</div>
<p>While the goal of the risk function is to define what we want, it’s the optimizer’s job to find the parameter vector <span class="math inline">\(\theta^*\)</span> that minimizes the empirical risk function. For simplicity, we will now refer to both the regularized and unregularized risk function as <span class="math inline">\(\hat{\mathcal{R}}\)</span>.</p>
<p><span class="math display">\[\theta^* = \arg \min_\theta \hat{\mathcal{R}}(\theta)\]</span></p>
<p>This is done by iteratively updating the parameter vector <span class="math inline">\(\theta\)</span> using the gradient of the risk function with respect to the parameter vector. The simplified update formula for a parameter <span class="math inline">\(\theta\)</span> at time step <span class="math inline">\(t\)</span> is given by:</p>
<p><span class="math display">\[\theta_{t+1} = \theta_t - \eta \nabla_{\theta_t} \hat{\mathcal{R}}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\theta_t\)</span> is the current value of the parameter vector at time step <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(\theta_{t+1}\)</span> is the new value of the parameter after the update.</li>
<li><span class="math inline">\(\eta\)</span> (eta) is the learning rate, which controls the step size.</li>
<li><span class="math inline">\(\nabla_{\theta_t} \hat{\mathcal{R}}\)</span> is the gradient of the empirical risk function <span class="math inline">\(\hat{\mathcal{R}}\)</span> with respect to parameter <span class="math inline">\(\theta\)</span>, evaluated at <span class="math inline">\(\theta_t\)</span>.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quiz: Learning Rate
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Question 1</strong>: Can you guess what happens when the learning rate is too high? What happens when it is too low?</p>
<details>
<summary>
Click for answer
</summary>
<p>A too high learning rate will cause the parameters to overshoot the minimum and diverge. A too low learning rate will cause the parameters to converge slowly. <img src="../assets/lr_size.png" class="img-fluid"></p>
<a href="https://stackoverflow.com/questions/62690725/small-learning-rate-vs-big-learning-rate">Source</a>
</details>
</div>
</div>
<p>The optimizers used in practice differ from the above formula, as:</p>
<ol type="1">
<li>The gradient is estimated from a batch rather than the entire training dataset.</li>
<li>The simplistic update formula is extended with:
<ul>
<li>Weight decay</li>
<li>Momentum</li>
<li>Adaptive learning rates</li>
</ul></li>
</ol>
<p>Before we cover these more advanced approaches (specifically their implementation in AdamW), we will first focus on the vanilla version of Stochastic Gradient Descent (SGD).</p>
<section id="mini-batch-effects-in-sgd" class="level2">
<h2 class="anchored" data-anchor-id="mini-batch-effects-in-sgd">Mini-Batch Effects in SGD</h2>
<p>When using mini-batches, the gradient becomes a noisy estimate of the gradient over the full dataset. With <span class="math inline">\(\nabla_{\theta_t} \hat{\mathcal{R}}^{(i)}\)</span> being the gradient of the risk function with respect to the entire parameter vector estimated using <span class="math inline">\((x^{(i)}, y^{(i)})\)</span>, the mini-batch gradient is given by:</p>
<p><span class="math display">\[\nabla_{\theta_t} \hat{\mathcal{R}}^B = \frac{1}{|B|} \sum_{i \in B} \nabla_{\theta_t} \hat{\mathcal{R}}^{(i)}\]</span></p>
<p>where <span class="math inline">\(B\)</span> is the batch of samples and <span class="math inline">\(|B|\)</span> is the batch size.</p>
<p>The update formula for SGD is then given by:</p>
<p><span class="math display">\[\theta_{t+1} = \theta_t - \eta \nabla_{\theta_t} \hat{\mathcal{R}}^{B_t}\]</span></p>
<p>The difference between the full gradient and the mini-batch gradient is visualized in the image below:</p>
<p><img src="../assets/gd_vs_sgd.png" class="img-fluid"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quiz: Vanilla SGD
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Question 1:</strong> What happens when the batch size is too small or too large?</p>
<details>
<summary>
Click for answer
</summary>
<p><strong>Trade-offs with Batch Size</strong>:</p>
<ul>
<li>Larger batches provide more accurate gradient estimates.</li>
<li>Smaller batches introduce more noise but allow more frequent parameter updates.</li>
</ul>
</details>
<p><strong>Question 2:</strong> The mini-batch gradient is an approximation of the gradient over the full dataset. Does the latter also approximate something? If so, what?</p>
<details>
<summary>
Click for answer
</summary>
<p>In machine learning, we assume that the data is drawn from a distribution <span class="math inline">\(\mathbb{E}\)</span>. The gradient over the full dataset approximates the expectation over this distribution:</p>
<span class="math display">\[\nabla_\theta \mathcal{R} = \mathbb{E} [\nabla_\theta L(f_\theta(x), y)]\]</span>
</details>
</div>
</div>
<p>Because deep learning models can have many parameters and computing gradients is expensive, understanding the effects of different batch sizes and convergence is important. The computational cost (which we define as the time it takes to perform one optimization step) of a gradient update using a batch size <span class="math inline">\(B\)</span> consists of:</p>
<ol type="1">
<li>Loading the batch into memory (if the data does not fit into RAM).</li>
<li>The forward pass of the model.</li>
<li>The backward pass of the model.</li>
<li>The update of the parameters.</li>
</ol>
<p>We will discuss point 1 later, and point 4 does not depend on the batch size, so we can ignore it.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quiz: Bang for Your Buck
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Question 1:</strong> True or false: The cost (duration) of performing a gradient update using a batch size of <span class="math inline">\(2\)</span> is twice the cost of a batch size of <span class="math inline">\(1\)</span>.</p>
<details>
<summary>
Click for answer
</summary>
False. Because GPUs can perform many operations simultaneously, the cost of performing a gradient update using a batch size of <span class="math inline">\(2\)</span> is not twice the cost of a batch size of <span class="math inline">\(1\)</span>. The cost depends on many factors, but if the model is small, the cost of a batch with 2 observations might be almost the same as one with one observation.
</details>
<p><strong>Question 2:</strong> The standard error of the mini-batch gradient estimate (which characterizes the precision of the gradient estimate) can be written as:</p>
<p><span class="math display">\[\text{SE}_{\nabla_{\theta_t} \hat{\mathcal{R}}^B} = \frac{\sigma_{\nabla_{\theta_t} \hat{\mathcal{R}}^{(i)}}}{\sqrt{|B|}}\]</span></p>
<p>where <span class="math inline">\(\sigma_{\nabla_{\theta_t} \hat{\mathcal{R}}^{(i)}}\)</span> is the standard deviation of the gradient estimate using a single observation.</p>
<p>Describe the dynamics of the standard error when increasing the batch size: How do you need to increase a batch size from <span class="math inline">\(1\)</span> to achieve half the standard error? What about increasing a batch size from <span class="math inline">\(100\)</span>?</p>
<details>
<summary>
Click for answer
</summary>
<p>The standard error decreases as the batch size increases, but with diminishing returns. To halve the standard error:</p>
<ul>
<li>Increase the batch size from <span class="math inline">\(1\)</span> to <span class="math inline">\(4\)</span>.</li>
<li>Increase the batch size from <span class="math inline">\(100\)</span> to <span class="math inline">\(400\)</span>.</li>
</ul>
This is because the standard error is inversely proportional to the square root of the batch size.
</details>
</div>
</div>
</section>
<section id="mini-batch-gradient-descent-its-not-all-about-runtime" class="level2">
<h2 class="anchored" data-anchor-id="mini-batch-gradient-descent-its-not-all-about-runtime">Mini-Batch Gradient Descent: It’s not all about runtime</h2>
<p>As we have now covered some of the dynamics of a simple gradient-based optimizer, we can examine the final parameter vector <span class="math inline">\(\theta^*\)</span> that the optimizer converges to. When using a gradient-based optimizer, the updates will stop once the gradient is close to zero. We will now discuss the type of solutions where this is true and their properties.</p>
<p>We need to distinguish <em>saddle points</em> from <em>local minima</em> from <em>global minima</em>:</p>
<p><img src="../assets/minimum_vs_saddlepoint.png" class="img-fluid"></p>
<p>In deep learning, where high-dimensional parameter spaces are common, saddle points are more likely to occur than local minima. However, due to the stochastic nature of SGD, optimizers will find local minima instead of saddle points.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quiz: Local vs.&nbsp;Global Minima, Generalization
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Question 1</strong>: Do you believe SGD will find local or global minima? Explain your reasoning.</p>
<details>
<summary>
Click for answer
</summary>
Because the gradient only has <strong>local</strong> information about the loss function, SGD finds local minima.
</details>
<p><strong>Question 2</strong>: Assuming we have found a <span class="math inline">\(\theta^*\)</span> that has low training loss, does this ensure that we have found a good model?</p>
<details>
<summary>
Click for answer
</summary>
No, because we only know that the model has low training loss, but not necessarily low test loss.
</details>
</div>
</div>
<p>SGD has been empirically shown to find solutions that generalize well to unseen data. This phenomenon is attributed to the implicit regularization effects of SGD, where the noise introduced by mini-batch sampling helps guide the optimizer towards broader minima with smaller L2 norms. These broader minima are typically associated with better generalization performance compared to sharp minima.</p>
<p><img src="../assets/flat_minima_generalization.png" class="img-fluid"></p>
<p><a href="https://www.researchgate.net/figure/Flat-minima-results-in-better-generalization-compared-to-sharp-minima-Pruning-neural_fig2_353068686">Source</a></p>
<p>These properties are also known as <em>implicit regularization</em> of SGD. Regularization generally refers to techniques that prevent overfitting and improve generalization. There are also explicit regularization techniques, which we will cover next.</p>
<section id="weight-decay" class="level3">
<h3 class="anchored" data-anchor-id="weight-decay">Weight Decay</h3>
<p>One modification to the SGD update formula is the so-called <em>weight decay</em>, which is equivalent to adding a regularization penalty term to the loss function as we have seen earlier.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>For more complex optimizers such as Adam, weight decay is not equivalent to adding a regularization penalty term to the loss function <span class="citation" data-cites="loshchilov2017decoupled">(<a href="#ref-loshchilov2017decoupled" role="doc-biblioref">Loshchilov 2017</a>)</span>. However, the main idea of both approaches is still to shrink the weights to <span class="math inline">\(0\)</span> during training.</p>
</div>
</div>
<p>If we integrate weight decay into the gradient update formula, we get the following:</p>
<p><span class="math display">\[\theta_{t+1} = \theta_t - \eta \big(\nabla_{\theta_t} \hat{\mathcal{R}}^B- \lambda \theta_t\big)\]</span></p>
<p>This formula shows that the weight decay term (<span class="math inline">\(- \lambda \theta_t\)</span>) effectively shrinks the weights during each update, helping to prevent overfitting.</p>
<p><img src="../assets/regularization.png" class="img-fluid"></p>
<p><a href="https://www.linkedin.com/pulse/understanding-regularization-techniques-machine-javier-icaza-santos">Source</a></p>
</section>
<section id="momentum" class="level3">
<h3 class="anchored" data-anchor-id="momentum">Momentum</h3>
<p>Momentum is a technique that helps accelerate gradient descent by using an exponential moving average of past gradients. Like a ball rolling down a hill, momentum helps the optimizer:</p>
<ul>
<li>Move faster through areas of consistent gradient direction.</li>
<li>Push through sharp local minima and saddle points.</li>
<li>Dampen oscillations in areas where the gradient frequently changes direction.</li>
</ul>
<p>The exponential moving momentum update can be expressed mathematically as:</p>
<p><span class="math display">\[
(1 - \beta) \sum_{\tau=1}^{t} \beta^{t-\tau} \nabla_{\theta_{\tau - 1}} \hat{\mathcal{R}}^{B_{\tau - 1}}
\]</span></p>
<p>In order to avoid having to keep track of all the gradients, we can calculate the update in two steps as follows:</p>
<p><span class="math display">\[
v_t = \beta_1 v_{t-1} + (1 - \beta_1) \nabla_{\theta_t} \hat{\mathcal{R}}^{B_t}
\]</span></p>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - \eta \frac{v_t}{1 - \beta_1^t}
\]</span></p>
<p>The hyperparameter <span class="math inline">\(\beta_1\)</span> is the momentum decay rate (typically 0.9), <span class="math inline">\(v_t\)</span> is the exponential moving average of gradients, and <span class="math inline">\(\eta\)</span> is the learning rate as before. Note that dividing by <span class="math inline">\(1 - \beta_1^t\)</span> counteracts a bias because <span class="math inline">\(v_0\)</span> is initialized to <span class="math inline">\(0\)</span>.</p>
<p><img src="../assets/momentum.png" class="img-fluid" style="width:60.0%"></p>
<p><a href="https://eloquentarduino.github.io/2020/04/stochastic-gradient-descent-on-your-microcontroller/">Source</a></p>
</section>
<section id="adaptive-learning-rates" class="level3">
<h3 class="anchored" data-anchor-id="adaptive-learning-rates">Adaptive Learning Rates</h3>
<p>Adaptive learning rate methods automatically adjust the learning rate for each parameter during training. This is particularly useful because:</p>
<ol type="1">
<li>Different parameters may require different learning rates.</li>
<li>The optimal learning rate often changes during training.</li>
</ol>
<p>Before, we had one global learning rate <span class="math inline">\(\eta\)</span> for all parameters. However, learning rates are now allowed to:</p>
<ol type="1">
<li>Change over time.</li>
<li>Be different for different parameters.</li>
</ol>
<p>Our vanilla SGD update formula is now generalized to handle adaptive learning rates:</p>
<p><span class="math display">\[\theta_{t+1} = \theta_t - \eta_t \cdot \nabla_{\theta_t} \hat{\mathcal{R}}^{B_t}\]</span></p>
<p>Here, <span class="math inline">\(\eta_t\)</span> is now not a scalar learning rate, but a vector of learning rates for each parameter, and ‘<span class="math inline">\(\cdot\)</span>’ denotes the element-wise multiplication. Further, <span class="math inline">\(\epsilon\)</span> is a small constant for numerical stability.</p>
<p>In AdamW, the adaptive learning rate is controlled by the second moment estimate (squared gradients):</p>
<p><span class="math display">\[g_t = \beta_2 g_{t-1} + (1-\beta_2)(\nabla_{\theta_t} \hat{\mathcal{R}}^{B_t})^2\]</span> <span class="math display">\[\hat{\eta}_t = \frac{\eta}{\sqrt{\frac{g_t}{1 - \beta_2^t}} + \epsilon}\]</span></p>
<p>In words, this means: In steep directions where the gradient is large, the learning rate is small and vice versa. The parameters <span class="math inline">\(\beta_2\)</span> and <span class="math inline">\(\epsilon\)</span> are hyperparameters that control the decay rate and numerical stability of the second moment estimate.</p>
<p><img src="../assets/adagrad.png" class="img-fluid"></p>
<p>When combining weight decay, adaptive learning rates, and momentum, we get the AdamW optimizer. It therefore has parameters:</p>
<ul>
<li><code>lr</code>: The learning rate.</li>
<li><code>weight_decay</code>: The weight decay parameter.</li>
<li><code>betas</code>: The momentum parameters (<span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>).</li>
<li><code>eps</code>: The numerical stability parameter.</li>
</ul>
<p>Note that AdamW also has another configuration parameter <code>amsgrad</code>, which is disabled by default in <code>torch</code>, but which can help with convergence.</p>
</section>
</section>
<section id="optimizers-in-torch" class="level1">
<h1>Optimizers in torch</h1>
<p><code>torch</code> provides several common optimizers, including SGD, Adam, AdamW, RMSprop, and Adagrad. The main optimizer API consists of:</p>
<ol type="1">
<li>Initializing the optimizer, which requires passing the parameters of the module to be optimized and setting the optimizer’s hyperparameters such as the learning rate.</li>
<li><code>step()</code>: Update parameters using current gradients.</li>
<li><code>zero_grad()</code>: Reset gradients of all the parameters to zero before each backward pass.</li>
<li>Just like <code>nn_module</code>s, they have a <code>$state_dict()</code> which can, for example, be saved to later load it using <code>$load_state_dict()</code>.</li>
</ol>
<p>We will focus on the AdamW optimizer, but the others work analogously.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torch)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">formals</span>(optim_adamw)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$params


$lr
[1] 0.001

$betas
c(0.9, 0.999)

$eps
[1] 1e-08

$weight_decay
[1] 0.01

$amsgrad
[1] FALSE</code></pre>
</div>
</div>
<p>To construct it, we first need to create a model and then pass the parameters of the model to the optimizer so it knows which parameters to optimize.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">nn_linear</span>(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">&lt;-</span> <span class="fu">optim_adamw</span>(model<span class="sc">$</span>parameters, <span class="at">lr =</span> <span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To illustrate the optimizer, we will again generate some synthetic training data:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">torch_manual_seed</span>(<span class="dv">1</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="dv">1000</span>, <span class="dv">1</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">torch_randn</span>(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> X <span class="sc">*</span> beta <span class="sc">+</span> <span class="fu">torch_randn</span>(<span class="dv">1000</span>, <span class="dv">1</span>) <span class="sc">*</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This represents data from a simple linear model with some noise:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="4-optimizer_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Performing a (full) gradient update using the AdamW optimizer consists of:</p>
<ol type="1">
<li><p>Calculating the forward pass</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">=</span> <span class="fu">model</span>(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></li>
<li><p>Calculating the loss</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>loss <span class="ot">=</span> <span class="fu">mean</span>((y_hat <span class="sc">-</span> Y)<span class="sc">^</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></li>
<li><p>Performing a backward pass</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>loss<span class="sc">$</span><span class="fu">backward</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></li>
<li><p>Applying the update rule</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>opt<span class="sc">$</span><span class="fu">step</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></li>
</ol>
<p>Note that after the optimizer step, the gradients are not reset to zero but are unchanged.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>model<span class="sc">$</span>weight<span class="sc">$</span>grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch_tensor
 0.5993
[ CPUFloatType{1,1} ]</code></pre>
</div>
</div>
<p>If we were to perform another backward pass, the gradient would be added to the current gradient. If this is not desired, we can set an individual gradient of a tensor to zero:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>model<span class="sc">$</span>weight<span class="sc">$</span>grad<span class="sc">$</span><span class="fu">zero_</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch_tensor
 0
[ CPUFloatType{1,1} ]</code></pre>
</div>
</div>
<p>Optimizers also offer a convenient way to set all gradients of the parameters managed by them to zero using <code>$zero_grad()</code>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>opt<span class="sc">$</span><span class="fu">zero_grad</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quiz: Guess which parameter is varied
</div>
</div>
<div class="callout-body-container callout-body">
<p>We will now show some real trajectories of the AdamW optimizer applied to the linear regression problem from above where one specific parameter is varied. Recall that:</p>
<ul>
<li><span class="math inline">\(\eta\)</span>: The learning rate controls the step size of the optimizer.</li>
<li><span class="math inline">\(\lambda\)</span>: The weight decay parameter controls the bias of the optimization towards a parameter being close to zero. A value of <span class="math inline">\(0\)</span> means no weight decay.</li>
<li><span class="math inline">\(\beta_1\)</span>: The momentum parameter. A value of <span class="math inline">\(0\)</span> means no momentum.</li>
<li><span class="math inline">\(\beta_2\)</span>: The second moment parameter. A value of <span class="math inline">\(0\)</span> means no second moment adjustment.</li>
</ul>
<p>The plots below show contour lines of the empirical loss function, i.e., two values that are on the same contour line have the same loss.</p>
<p><strong>Question 1</strong>: Which parameter is varied here? Explain your reasoning.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="4-optimizer_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<details>
<summary>
Click for answer
</summary>
The learning rate is varied. This can be seen as the gradient updates for the right trajectory are larger than for the left trajectory.
</details>
<p><strong>Question 2</strong>: Which parameter is varied below? Explain your reasoning.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="4-optimizer_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<details>
<summary>
Click for answer
</summary>
The weight decay is varied. We can see this as the final parameter value for the right trajectory is closer to zero than for the left trajectory.
</details>
<p><strong>Question 3</strong>: Which parameter is varied below? Explain your reasoning.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="4-optimizer_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<details>
<summary>
Click for answer
</summary>
The momentum parameter <span class="math inline">\(\beta_1\)</span> is varied. There is no momentum on the left side, so the gradient steps are more noisy. On the right side, the momentum is set to <span class="math inline">\(0.9\)</span>, so over time, momentum in the ‘correct’ direction is accumulated.
</details>
<p><strong>Question 4</strong>: Which parameter is varied below? Explain your reasoning.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="4-optimizer_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<details>
<summary>
Click for answer
</summary>
The <span class="math inline">\(\beta_2\)</span> parameter is varied. There is no second moment adjustment on the left side, but there is on the right side. Because the gradients in the direction of the bias are larger than in the direction of the weight, the second moment adjustment helps to reduce the learning rate in the direction of the bias.
</details>
</div>
</div>
<section id="learning-rate-schedules" class="level2">
<h2 class="anchored" data-anchor-id="learning-rate-schedules">Learning Rate Schedules</h2>
<p>While we have already covered dynamic learning rates, it can still be beneficial to use a <strong>learning rate scheduler</strong> to further improve convergence. Like for adaptive learning rates, the learning rate is then not a constant scalar, but a function of the current epoch or iteration. Note that the learning rate schedulers discussed here can also be combined with adaptive learning rates such as in AdamW and are not mutually exclusive.</p>
<p><strong>Decaying learning rates</strong>:</p>
<p>This includes gradient decay, cosine annealing, and cyclical learning rates. The general idea is to start with a high learning rate and then gradually decrease it over time.</p>
<p><strong>Warmup</strong>:</p>
<p>Warmup is a technique that gradually increases the learning rate from a small value to a larger value over a specified number of epochs. This ensures that in the beginning, where the weights are randomly initialized, the learning rate is not too high.</p>
<p><strong>Cyclical Learning Rates</strong>:</p>
<p>Cyclical learning rates are a technique that involves periodically increasing and decreasing the learning rate. This can help the optimizer to traverse saddle points faster and find better solutions.</p>
<p>The different schedules are visualized below:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="4-optimizer_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>In <code>torch</code>, learning rate schedulers are prefixed by <code>lr_</code>, such as the simple <code>lr_step</code>, where the learning rate is multiplied by a factor of <code>gamma</code> every <code>step_size</code> epochs. In order to use them, we need to pass the optimizer to the scheduler and specify additional arguments.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>scheduler <span class="ot">=</span> <span class="fu">lr_step</span>(opt, <span class="at">step_size =</span> <span class="dv">2</span>, <span class="at">gamma =</span> <span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The main API of a learning rate scheduler is the <code>$step()</code> method, which updates the learning rate. For some schedulers, this needs to be called after each optimization step, for others after each epoch. You can find this out by consulting the documentation of the specific scheduler.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>opt<span class="sc">$</span>param_groups[[1L]]<span class="sc">$</span>lr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>scheduler<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>opt<span class="sc">$</span>param_groups[[1L]]<span class="sc">$</span>lr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>scheduler<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>opt<span class="sc">$</span>param_groups[[1L]]<span class="sc">$</span>lr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.02</code></pre>
</div>
</div>
</section>
<section id="saving-an-optimizer" class="level2">
<h2 class="anchored" data-anchor-id="saving-an-optimizer">Saving an Optimizer</h2>
<p>In order to resume training at a later stage, we can save the optimizer’s state which is accessible via <code>$state_dict()</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>state_dict <span class="ot">=</span> opt<span class="sc">$</span><span class="fu">state_dict</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This state dictionary contains:</p>
<ol type="1">
<li>The <code>$param_groups</code> which contains the hyperparameters for each parameter group.</li>
<li>The <code>$state</code> which contains the optimizer’s internal state, such as the momentum and second moment estimates.</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>state_dict<span class="sc">$</span>param_groups[[1L]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$params
[1] 1 2

$lr
[1] 0.02

$betas
[1] 0.900 0.999

$eps
[1] 1e-08

$weight_decay
[1] 0.01

$amsgrad
[1] FALSE

$initial_lr
[1] 0.2</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>It is possible to set different parameters (such as learning rate) for different parameter groups.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>o2 <span class="ot">=</span> <span class="fu">optim_adamw</span>(<span class="fu">list</span>(</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">params =</span> <span class="fu">torch_tensor</span>(<span class="dv">1</span>), <span class="at">lr =</span> <span class="dv">1</span>),</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">params =</span> <span class="fu">torch_tensor</span>(<span class="dv">2</span>), <span class="at">lr =</span> <span class="dv">2</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>o2<span class="sc">$</span>param_groups[[1L]]<span class="sc">$</span>lr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>o2<span class="sc">$</span>param_groups[[2L]]<span class="sc">$</span>lr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2</code></pre>
</div>
</div>
</div>
</div>
<p>The <code>$state</code> field contains the state for each parameter:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>state_dict<span class="sc">$</span>state</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$`1`
$`1`$step
torch_tensor
1
[ CPUFloatType{} ]

$`1`$exp_avg
torch_tensor
0.01 *
 5.9926
[ CPUFloatType{1,1} ]

$`1`$exp_avg_sq
torch_tensor
0.0001 *
 3.5912
[ CPUFloatType{1,1} ]


$`2`
$`2`$step
torch_tensor
1
[ CPUFloatType{} ]

$`2`$exp_avg
torch_tensor
0.01 *
 2.1779
[ CPUFloatType{1} ]

$`2`$exp_avg_sq
torch_tensor
1e-05 *
 4.7433
[ CPUFloatType{1} ]</code></pre>
</div>
</div>
<p>Just like for the <code>nn_module</code>, we can save the optimizer state using <code>torch_save()</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>pth <span class="ot">=</span> <span class="fu">tempfile</span>(<span class="at">fileext =</span> <span class="st">".pth"</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="fu">torch_save</span>(state_dict, pth)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Generally, we don’t want to save the whole optimizer, as this also contains the weight tensors of the model that one usually wants to save separately.</p>
</div>
</div>
<p>We can load the optimizer state again using <code>torch_load()</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>state_dict2 <span class="ot">=</span> <span class="fu">torch_load</span>(pth)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>opt2 <span class="ot">&lt;-</span> <span class="fu">optim_adamw</span>(model<span class="sc">$</span>parameters, <span class="at">lr =</span> <span class="fl">0.2</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>opt2<span class="sc">$</span><span class="fu">load_state_dict</span>(state_dict2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="embedded-regularization-techniques" class="level2">
<h2 class="anchored" data-anchor-id="embedded-regularization-techniques">Embedded Regularization Techniques</h2>
<p>Besides the explicit regularization effects of weight decay and the implicit regularization effects of mini-batch gradient descent, there are also other techniques that improve generalization of deep neural networks. Here, we focus on dropout and batch normalization, which are both embedded in neural network architectures.</p>
<section id="dropout" class="level3">
<h3 class="anchored" data-anchor-id="dropout">Dropout</h3>
<p>Dropout is a regularization technique used to prevent overfitting in neural networks. During each training iteration, dropout randomly “drops” a subset of neurons by setting their activations to zero with a specified probability. This forces the network to distribute the learned representations more evenly across neurons. Dropout is most commonly used in the context of fully connected layers.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/dropout.png" class="img-fluid figure-img" style="width:100.0%"></p>
</figure>
</div>
<p><a href="https://medium.com/konvergen/understanding-dropout-ddb60c9f98aa">Source</a></p>
<p>Note that neurons are only dropped when the module is in <em>train</em> mode, not in <em>eval</em> mode.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">torch_randn</span>(<span class="dv">10</span>, <span class="dv">5</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>dropout <span class="ot">=</span> <span class="fu">nn_dropout</span>(<span class="at">p =</span> <span class="fl">0.5</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="fu">dropout</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch_tensor
-0.0000  3.7545  4.3769  0.0000  0.0000
 0.4759  0.7142  1.8560 -1.8215  0.0000
 0.0000  0.0000  0.8545 -1.1291 -0.0000
-0.0383  2.3208 -0.3911  2.5771  0.0000
 0.0000 -0.0000  0.0000  0.0000  0.0000
 0.6956  0.0000 -0.0000  2.9860  0.2500
-0.9978 -2.0387  0.0000  0.5153 -0.0000
-0.0000  0.0000 -0.9363 -0.0000 -0.7703
-0.9378 -0.2968  3.6971  1.6708 -0.0000
-0.0000  1.2085  0.0000  0.0000 -2.8396
[ CPUFloatType{10,5} ]</code></pre>
</div>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>dropout<span class="sc">$</span><span class="fu">eval</span>()</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dropout</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch_tensor
-0.4076  1.8772  2.1884  0.3678  0.0293
 0.2379  0.3571  0.9280 -0.9107  0.5313
 0.1559  0.1716  0.4273 -0.5645 -0.1329
-0.0192  1.1604 -0.1956  1.2885  0.3373
 0.3247 -1.6603  1.3701  1.0716  0.2485
 0.3478  2.6682 -0.4942  1.4930  0.1250
-0.4989 -1.0193  0.3470  0.2577 -0.4072
-0.4410  0.8347 -0.4681 -0.0240 -0.3851
-0.4689 -0.1484  1.8486  0.8354 -1.1521
-0.2673  0.6042  0.4407  0.5085 -1.4198
[ CPUFloatType{10,5} ]</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quiz: Dropout
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Question 1</strong>: Worse Training Loss: You are training a neural network with and without dropout. The training loss is higher with dropout, is this a bug?</p>
<details>
<summary>
Click for answer
</summary>
Not necessarily, as dropout is a regularization technique that prevents overfitting. Its goal is to reduce the generalization performance of the model and not to improve training performance.
</details>
</div>
</div>
</section>
<section id="batch-normalization" class="level3">
<h3 class="anchored" data-anchor-id="batch-normalization">Batch Normalization</h3>
<p>Batch Normalization is an important technique in deep learning that contributed significantly to speeding up the training process, especially in convolutional neural networks that are covered in the next chapter. During training, batch normalization introduces noise into the network by normalizing each mini-batch independently. Besides faster congerence, batch normalization also acts as a regularizer, where the model learns to be less sensitive to the specific details of the training data, thus reducing overfitting.</p>
<p>The formula for batch normalization (during training) is given by:</p>
<p><span class="math display">\[
\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\hat{x}\)</span> is the normalized output,</li>
<li><span class="math inline">\(x\)</span> is the input,</li>
<li><span class="math inline">\(\mu_B\)</span> is the mean of the batch,</li>
<li><span class="math inline">\(\sigma_B^2\)</span> is the variance of the batch,</li>
<li><span class="math inline">\(\epsilon\)</span> is a small constant added for numerical stability.</li>
</ul>
<p>During inference (i.e.&nbsp;in eval mode), the module uses the running mean and variance of the training data to normalize the input.</p>
<p>In <code>torch</code>, different versions of batch normalization exist for different dimensions of the input tensor. Below, we illustrate the batch normalization module using a 1D input tensor (the batch dimension does not count here):</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">torch_randn</span>(<span class="dv">10</span>, <span class="dv">5</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>bn <span class="ot">=</span> <span class="fu">nn_batch_norm1d</span>(<span class="at">num_features =</span> <span class="dv">5</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="fu">bn</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch_tensor
 1.2203  1.1179  1.1652 -1.3292  0.5830
 0.6674 -1.1404 -0.1058  0.1933 -0.4474
 0.2395  0.6881  1.0513 -1.2640 -0.2211
-1.9849  0.2075 -0.3904  0.5686  1.6812
-0.5494 -0.9593 -1.6051 -1.3635  0.3326
 0.4905  1.1264  0.4455  0.8893 -0.3957
-0.9948 -0.0520 -0.2290  0.1912 -1.1758
-0.5657 -1.3184 -1.8173  1.1717 -1.8676
-0.0116  1.3793  0.4786  1.4124  1.1464
 1.4886 -1.0491  1.0071 -0.4698  0.3644
[ CPUFloatType{10,5} ][ grad_fn = &lt;NativeBatchNormBackward0&gt; ]</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quiz: Batch Normalization
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Question 1</strong>: Earlier we have learned that <code>nn_module</code>s have buffers and parameters, where only the latter are learned with gradient descent. Do you think the mean and variance are parameters or buffers?</p>
<details>
<summary>
Click for answer
</summary>
They are both buffers as they only store the variance and running mean of all training samples seen, i.e., they are not updated using gradient information.
</details>
<p><strong>Question 2</strong>: Training vs.&nbsp;Evaluation Mode: While many <code>nn_module</code>s behave the same way irrespective of their mode, batch normalization is an example of a module that behaves differently during training and evaluation. During training, the module uses the mean and variance of the current batch, while during evaluation, it uses the running mean and variance of all training samples seen.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bn</span>(x[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, ])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch_tensor
 1.2203  1.1179  1.1652 -1.3292  0.5830
 0.6674 -1.1404 -0.1058  0.1933 -0.4474
 0.2395  0.6881  1.0513 -1.2640 -0.2211
-1.9849  0.2075 -0.3904  0.5686  1.6812
-0.5494 -0.9593 -1.6051 -1.3635  0.3326
 0.4905  1.1264  0.4455  0.8893 -0.3957
-0.9948 -0.0520 -0.2290  0.1912 -1.1758
-0.5657 -1.3184 -1.8173  1.1717 -1.8676
-0.0116  1.3793  0.4786  1.4124  1.1464
 1.4886 -1.0491  1.0071 -0.4698  0.3644
[ CPUFloatType{10,5} ][ grad_fn = &lt;NativeBatchNormBackward0&gt; ]</code></pre>
</div>
</div>
<p>Which of the following statements is true and why?</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>bn<span class="sc">$</span><span class="fu">eval</span>()</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>equal1 <span class="ot">=</span> <span class="fu">torch_equal</span>(</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">torch_cat</span>(<span class="fu">list</span>(<span class="fu">bn</span>(x[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, ]), <span class="fu">bn</span>(x[<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>, ]))),</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bn</span>(x[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, ])</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>bn<span class="sc">$</span><span class="fu">train</span>()</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>equal2 <span class="ot">=</span> <span class="fu">torch_equal</span>(</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">torch_cat</span>(<span class="fu">list</span>(<span class="fu">bn</span>(x[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, ]), <span class="fu">bn</span>(x[<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>, ]))),</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bn</span>(x[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, ])</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<details>
<summary>
Click for answer
</summary>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(equal1, equal2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  TRUE FALSE</code></pre>
</div>
</div>
The first statement is true because, in evaluation mode, the module uses the running mean and variance of all training samples seen. The second statement is false because the first tensor uses different means and variances for rows 1-2 and 3-4, while the second tensor uses the same mean and variance for all rows.
</details>
</div>
</div>
<p>To demonstrate these methods, we apply them to a simple spam classification task. The data has one binary target variable <code>type</code> (spam or no spam) and 57 numerical features.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">nrow</span>(spam), <span class="fu">ncol</span>(spam))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4601   58</code></pre>
</div>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(spam<span class="sc">$</span>type)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
   spam nonspam 
   1813    2788 </code></pre>
</div>
</div>
<p>Below, we create a simple neural network with two hidden layers of dimension 100, ReLU activation and optionally dropout and batch normalization.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>nn_reg <span class="ot">&lt;-</span> <span class="fu">nn_module</span>(<span class="st">"nn_reg"</span>,</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(dropout, batch_norm) {</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>net <span class="ot">&lt;-</span> <span class="fu">nn_sequential</span>(</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>      <span class="fu">nn_linear</span>(<span class="at">in_features =</span> <span class="dv">57</span>, <span class="at">out_features =</span> <span class="dv">100</span>),</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> (batch_norm) <span class="fu">nn_batch_norm1d</span>(<span class="at">num_features =</span> <span class="dv">100</span>) <span class="cf">else</span> <span class="fu">nn_identity</span>(),</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> (dropout) <span class="fu">nn_dropout</span>(<span class="at">p =</span> <span class="fl">0.5</span>) <span class="cf">else</span> <span class="fu">nn_identity</span>(),</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>      <span class="fu">nn_relu</span>(),</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>      <span class="fu">nn_linear</span>(<span class="at">in_features =</span> <span class="dv">100</span>, <span class="at">out_features =</span> <span class="dv">100</span>),</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> (batch_norm) <span class="fu">nn_batch_norm1d</span>(<span class="at">num_features =</span> <span class="dv">100</span>) <span class="cf">else</span> <span class="fu">nn_identity</span>(),</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> (dropout) <span class="fu">nn_dropout</span>(<span class="at">p =</span> <span class="fl">0.5</span>) <span class="cf">else</span> <span class="fu">nn_identity</span>(),</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>      <span class="fu">nn_relu</span>(),</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>      <span class="fu">nn_linear</span>(<span class="at">in_features =</span> <span class="dv">100</span>, <span class="at">out_features =</span> <span class="dv">2</span>)</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>  },</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">forward =</span> <span class="cf">function</span>(x) {</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span><span class="fu">net</span>(x)</span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>nn_drop <span class="ot">&lt;-</span> <span class="fu">nn_reg</span>(<span class="at">dropout =</span> <span class="cn">TRUE</span>, <span class="at">batch_norm =</span> <span class="cn">FALSE</span>)</span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>nn_batch <span class="ot">&lt;-</span> <span class="fu">nn_reg</span>(<span class="at">dropout =</span> <span class="cn">FALSE</span>, <span class="at">batch_norm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a>nn_both <span class="ot">&lt;-</span> <span class="fu">nn_reg</span>(<span class="at">dropout =</span> <span class="cn">TRUE</span>, <span class="at">batch_norm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a>nn_vanilla <span class="ot">&lt;-</span> <span class="fu">nn_reg</span>(<span class="at">dropout =</span> <span class="cn">FALSE</span>, <span class="at">batch_norm =</span> <span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We evaluate the performance of the four neural networks created above using subsampling with 10 repetitions and an 80/20 train/test split. We don’t show the specific training code here, but only the resulting confidence intervals for the accuracy. While the intervals are too wide to be able to draw any final conclusions, dropout here seems to have a positive effect on the generalization performance of the model, whereas batch normalization does not (which is not too surprising as it is usually used with convolutional layers).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="4-optimizer_files/figure-html/unnamed-chunk-35-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>



</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-loshchilov2017decoupled" class="csl-entry" role="listitem">
Loshchilov, I. 2017. <span>“Decoupled Weight Decay Regularization.”</span> <em>arXiv Preprint arXiv:1711.05101</em>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">CC BY SA 4.0</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>