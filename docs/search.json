[
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "Deep Learning with (mlr3)torch in R :fire:",
    "section": "",
    "text": "This is a course, containing seven tutorials and corresponding exercises with solutions on torch and mlr3torch.\nThe seven topics are:\n\nTorch Tensors\nAutograd\nModules and Data\nOptimization & Regularization\nConvolutional Neural Networks\nIntro to mlr3torch (and mlr3 recap)\nTraining Efficiency\n\n\n\n\nAfter editing the content, e.g. in the notebooks folder, run quarto render to render the website. This will render the website into the docs/ folder. Upon pushing the changes to GitHub, the content of the docs/ folder is automatically deployed to GitHub Pages.\n\n\n\nSome of the content is based on the book Deep Learning and Scientific Computing with R torch by Sigrid Keydana. This course has been funded by Essential Data Science Training."
  },
  {
    "objectID": "README.html#overview-book",
    "href": "README.html#overview-book",
    "title": "Deep Learning with (mlr3)torch in R :fire:",
    "section": "",
    "text": "This is a course, containing seven tutorials and corresponding exercises with solutions on torch and mlr3torch.\nThe seven topics are:\n\nTorch Tensors\nAutograd\nModules and Data\nOptimization & Regularization\nConvolutional Neural Networks\nIntro to mlr3torch (and mlr3 recap)\nTraining Efficiency"
  },
  {
    "objectID": "README.html#contributing",
    "href": "README.html#contributing",
    "title": "Deep Learning with (mlr3)torch in R :fire:",
    "section": "",
    "text": "After editing the content, e.g. in the notebooks folder, run quarto render to render the website. This will render the website into the docs/ folder. Upon pushing the changes to GitHub, the content of the docs/ folder is automatically deployed to GitHub Pages."
  },
  {
    "objectID": "README.html#credit",
    "href": "README.html#credit",
    "title": "Deep Learning with (mlr3)torch in R :fire:",
    "section": "",
    "text": "Some of the content is based on the book Deep Learning and Scientific Computing with R torch by Sigrid Keydana. This course has been funded by Essential Data Science Training."
  },
  {
    "objectID": "notebooks/resources.html",
    "href": "notebooks/resources.html",
    "title": "Resources",
    "section": "",
    "text": "The torch website: https://torch.mlverse.org/\nThe torch package website: https://torch.mlverse.org/docs/\nBook by Sigrid Keydana: Deep Learning and Scientific Computing with R torch on which parts of this course are based.\nmlr3torch package website: https://mlr3torch.mlr-org.com/\nmlr3 book: https://mlr3book.mlr-org.com/\nmlr3 website: https://mlr-org.com/\nSince torch mimics PyTorch and the latter has a larger community, you can often learn from the PyTorch documentation."
  },
  {
    "objectID": "notebooks/8-usecase-exercise.html",
    "href": "notebooks/8-usecase-exercise.html",
    "title": "Practical Use Case",
    "section": "",
    "text": "In this notebook, the goal is to solve an actual problem using a neural network. As a baseline model, we will use a simple tree-based model.\nYou are free to use either only torch or mlr3torch."
  },
  {
    "objectID": "notebooks/7-training-efficiency-exercise.html",
    "href": "notebooks/7-training-efficiency-exercise.html",
    "title": "Training Efficiency",
    "section": "",
    "text": "Question 1: Validation\nIn this exercise, we will once again train a simple multi-layer perceptron on the Indian Liver Patient Dataset (ILPD). Create a learner that:\n\nUses 2 hidden layers with 100 neurons each.\nUtilizes a batch size of 128.\nTrains for 200 epochs.\nEmploys a validation set co mprising 30% of the data.\nTrack the validation log-loss.\nUtilizes trace-jitting to speed up the training process.\nEmploys the history callback to record the training and validation log-loss during training.\n\nAfterward, plot the validation log-loss, which is accessible via learner$model$callbacks$history.\nBelow, we create the task and remove the gender feature again for simplicity.\n\nlibrary(mlr3verse)\nlibrary(mlr3torch)\nilpd_num &lt;- tsk(\"ilpd\")\nilpd_num$select(setdiff(ilpd_num$feature_names, \"gender\"))\nilpd_num\n\n&lt;TaskClassif:ilpd&gt; (583 x 10): Indian Liver Patient Data\n* Target: diseased\n* Properties: twoclass\n* Features (9):\n  - dbl (5): albumin, albumin_globulin_ratio, direct_bilirubin, total_bilirubin, total_protein\n  - int (4): age, alanine_transaminase, alkaline_phosphatase, aspartate_transaminase\n\n\nQuestion 2: Early Stopping\nEnable early stopping to prevent overfitting and re-train the learner (using a patience of 10). Print the final validation performance of the learner and the early stopped results. You can consult the documentation of LearnerTorch on how to access these (see section Active Bindings).\n\n\nHint\n\nYou can enable early stopping by setting the patience parameter.\n\nQuestion 3: Early Stopping and Dropout Tuning\nWhile early stopping in itself is already useful, mlr3torch also allows you to simultaneously tune the number of epochs using early stopping while tuning other hyperparameters via traditional hyperparameter tuning from mlr3tuning.\nOne thing we have not mentioned so far is that the MLP learner also uses a dropout layer. The dropout probability can be configured via the p parameter.\nYour task is to tune the dropout probability p in the range \\([0, 1]\\) and the epochs using early stopping (using the configuration from the previous exercise) with an upper bound of 100 epochs.\nTo adapt this to work with early stopping, you need to set the:\n\nepochs to to_tune(upper = &lt;value&gt;, internal = TRUE): This tells the Tuner that the learner will tune the number of epochs itself.\n$validate field of the \"test\" so the same data is used for tuning and validation.\nTuning measure to msr(\"internal_valid_score\", minimize = TRUE). We set minimize to TRUE because we have used the log-loss as a validation measure.\n\nApart from this, the tuning works just like in tutorial 5. Use 3-fold cross-validation and evaluate 10 configurations using random search. Finally, print the optimal configuration."
  },
  {
    "objectID": "notebooks/7-training-efficiency-exercise-solution.html",
    "href": "notebooks/7-training-efficiency-exercise-solution.html",
    "title": "Training Efficiency",
    "section": "",
    "text": "Question 1: Validation\nIn this exercise, we will once again train a simple multi-layer perceptron on the Indian Liver Patient Dataset (ILPD). Create a learner that:\n\nUses 2 hidden layers with 100 neurons each.\nUtilizes a batch size of 128.\nTrains for 200 epochs.\nEmploys a validation set co mprising 30% of the data.\nTrack the validation log-loss.\nUtilizes trace-jitting to speed up the training process.\nEmploys the history callback to record the training and validation log-loss during training.\n\nAfterward, plot the validation log-loss, which is accessible via learner$model$callbacks$history.\nBelow, we create the task and remove the gender feature again for simplicity.\n\nlibrary(mlr3verse)\nlibrary(mlr3torch)\nilpd_num &lt;- tsk(\"ilpd\")\nilpd_num$select(setdiff(ilpd_num$feature_names, \"gender\"))\nilpd_num\n\n&lt;TaskClassif:ilpd&gt; (583 x 10): Indian Liver Patient Data\n* Target: diseased\n* Properties: twoclass\n* Features (9):\n  - dbl (5): albumin, albumin_globulin_ratio, direct_bilirubin, total_bilirubin, total_protein\n  - int (4): age, alanine_transaminase, alkaline_phosphatase, aspartate_transaminase\n\n\nSolution\n\nlibrary(ggplot2)\n\nmlp &lt;- lrn(\"classif.mlp\",\n  neurons = c(100, 100),\n  batch_size = 128,\n  epochs = 200,\n  predict_type = \"prob\",\n  validate = 0.3,\n  jit_trace = TRUE,\n  callbacks = t_clbk(\"history\"),\n  measures_valid = msr(\"classif.logloss\")\n)\n\nmlp$train(ilpd_num)\nhead(mlp$model$callbacks$history)\n\n   epoch valid.classif.logloss\n   &lt;num&gt;                 &lt;num&gt;\n1:     1             2.8418245\n2:     2             4.4608140\n3:     3             3.5882424\n4:     4             1.8290476\n5:     5             0.8181607\n6:     6             0.6774717\n\nggplot(mlp$model$callbacks$history) +\n  geom_line(aes(x = epoch, y = valid.classif.logloss)) +\n  labs(\n    y = \"Log-Loss (Validation)\",\n    x = \"Epoch\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nQuestion 2: Early Stopping\nEnable early stopping to prevent overfitting and re-train the learner (using a patience of 10). Print the final validation performance of the learner and the early stopped results. You can consult the documentation of LearnerTorch on how to access these (see section Active Bindings).\n\n\nHint\n\nYou can enable early stopping by setting the patience parameter.\n\nSolution\n\nmlp$configure(\n  patience = 10\n)\nmlp$train(ilpd_num)\nmlp$internal_tuned_values\n\n$epochs\n[1] 38\n\nmlp$internal_valid_scores\n\n$classif.logloss\n[1] 0.5938377\n\n\nQuestion 3: Early Stopping and Dropout Tuning\nWhile early stopping in itself is already useful, mlr3torch also allows you to simultaneously tune the number of epochs using early stopping while tuning other hyperparameters via traditional hyperparameter tuning from mlr3tuning.\nOne thing we have not mentioned so far is that the MLP learner also uses a dropout layer. The dropout probability can be configured via the p parameter.\nYour task is to tune the dropout probability p in the range \\([0, 1]\\) and the epochs using early stopping (using the configuration from the previous exercise) with an upper bound of 100 epochs.\nTo adapt this to work with early stopping, you need to set the:\n\nepochs to to_tune(upper = &lt;value&gt;, internal = TRUE): This tells the Tuner that the learner will tune the number of epochs itself.\n$validate field of the \"test\" so the same data is used for tuning and validation.\nTuning measure to msr(\"internal_valid_score\", minimize = TRUE). We set minimize to TRUE because we have used the log-loss as a validation measure.\n\nApart from this, the tuning works just like in tutorial 5. Use 3-fold cross-validation and evaluate 10 configurations using random search. Finally, print the optimal configuration.\nSolution\n\nlibrary(mlr3torch)\n\nmlp$configure(\n  epochs = to_tune(upper = 100, internal = TRUE),\n  p = to_tune(lower = 0, upper = 1),\n  validate = \"test\"\n)\n\ntuner &lt;- tnr(\"random_search\")\nresampling &lt;- rsmp(\"cv\", folds = 3)\nmeasure &lt;- msr(\"internal_valid_score\", minimize = TRUE)\n\nti &lt;- tune(\n  tuner = tuner,\n  task = ilpd_num,\n  learner = mlp,\n  resampling = resampling,\n  measure = measure,\n  term_evals = 10\n)\n\nti$result_learner_param_vals\n\n$epochs\n[1] 30\n\n$device\n[1] \"auto\"\n\n$num_threads\n[1] 1\n\n$num_interop_threads\n[1] 1\n\n$seed\n[1] \"random\"\n\n$jit_trace\n[1] TRUE\n\n$eval_freq\n[1] 1\n\n$measures_train\nlist()\n\n$measures_valid\nlist()\n\n$patience\n[1] 0\n\n$min_delta\n[1] 0\n\n$batch_size\n[1] 128\n\n$shuffle\n[1] TRUE\n\n$tensor_dataset\n[1] FALSE\n\n$neurons\n[1] 100 100\n\n$p\n[1] 0.2969339\n\n$activation\n&lt;nn_relu&gt; object generator\n  Inherits from: &lt;inherit&gt;\n  Public:\n    .classes: nn_relu nn_module\n    initialize: function (inplace = FALSE) \n    forward: function (input) \n    clone: function (deep = FALSE, ..., replace_values = TRUE) \n  Private:\n    .__clone_r6__: function (deep = FALSE) \n  Parent env: &lt;environment: 0x11da66d18&gt;\n  Locked objects: FALSE\n  Locked class: FALSE\n  Portable: TRUE\n\n$activation_args\nlist()"
  },
  {
    "objectID": "notebooks/6-mlr3torch-exercise.html",
    "href": "notebooks/6-mlr3torch-exercise.html",
    "title": "Training Neural Networks with mlr3torch",
    "section": "",
    "text": "Question 1: Hello World!\nIn this exercise, you will train your first neural network with mlr3torch.\nAs a task, we will use the ‘Indian Liver Patient’ dataset where the goal is to predict whether a patient has liver disease or not.\n\nlibrary(mlr3verse)\nlibrary(mlr3torch)\nilpd &lt;- tsk(\"ilpd\")\nilpd\n\n&lt;TaskClassif:ilpd&gt; (583 x 11): Indian Liver Patient Data\n* Target: diseased\n* Properties: twoclass\n* Features (10):\n  - dbl (5): albumin, albumin_globulin_ratio, direct_bilirubin, total_bilirubin, total_protein\n  - int (4): age, alanine_transaminase, alkaline_phosphatase, aspartate_transaminase\n  - fct (1): gender\n\nautoplot(ilpd)\n\n\n\n\n\n\n\n\nWe remove the gender column from the task, so we need to only deal with numeric features for now.\n\nilpd_num = ilpd$clone(deep = TRUE)\nilpd_num$select(setdiff(ilpd_num$feature_names, \"gender\"))\n\nYour task is to train a simple multi layer perceptron (lrn(\"classif.mlp\")) with 2 hidden layers with 100 neurons each. Set the batch size to 32, the learning rate to 0.001 and the number of epochs to 20. Then, resample the learner on the task with a cross-validation with 5 folds and evaluate the results using classification error and false positive rate (FPR). Is the result good?\n\n\nHint\n\nThe parameter for the learning rate is opt.lr\n\nQuestion 2: Preprocessing\nIn the previous question, we have operated on the ilpd_num task where we excluded the categorical gender column. This was done because the MLP learner operates on numeric features only. You will now create a more complex GraphLearner that also incudes one-hot encoding of the gender column before applying the MLP. Resample this learner on the original ilpd task and evaluate the results using the same measures as before.\n\n\nHint\n\nConcatenate po(\"encode\") with a lrn(\"classif.mlp\") using %&gt;&gt;% to create the GraphLearner. For available options on the encoding, see po(\"encode\")$help().\n\nQuestion 3: Benchmarking\nInstead of resampling a single learner, the goal is now to compare the performance of the MLP with a simple classification tree. Create a benchmark design and compare the performance of the two learners.\n\n\nHint\n\nCreate a classification tree via lrn(\"classif.rpart\"). A benchmark design can be created via benchmark_grid(). To run a benchmark, pass the design to benchmark().\n\nQuestion 4: Iris as a Lazy Tensor\nCreate a version of the iris task where the 4 features are represented as a single lazy tensor column.\nQuestion 5: Custom Architecture\nCreate a network with one hidden layer with 100 neurons and a sigmoid activation function by assempling PipeOps in a Graph. Convert the Graph to a Learner and train the network for 10 epochs using Adam with a learning rate of 0.001 and a batch size of 32."
  },
  {
    "objectID": "notebooks/6-mlr3torch-exercise-solution.html",
    "href": "notebooks/6-mlr3torch-exercise-solution.html",
    "title": "Training Neural Networks with mlr3torch",
    "section": "",
    "text": "Question 1: Hello World!\nIn this exercise, you will train your first neural network with mlr3torch.\nAs a task, we will use the ‘Indian Liver Patient’ dataset where the goal is to predict whether a patient has liver disease or not.\n\nlibrary(mlr3verse)\nlibrary(mlr3torch)\nilpd &lt;- tsk(\"ilpd\")\nilpd\n\n&lt;TaskClassif:ilpd&gt; (583 x 11): Indian Liver Patient Data\n* Target: diseased\n* Properties: twoclass\n* Features (10):\n  - dbl (5): albumin, albumin_globulin_ratio, direct_bilirubin, total_bilirubin, total_protein\n  - int (4): age, alanine_transaminase, alkaline_phosphatase, aspartate_transaminase\n  - fct (1): gender\n\nautoplot(ilpd)\n\n\n\n\n\n\n\n\nWe remove the gender column from the task, so we need to only deal with numeric features for now.\n\nilpd_num = ilpd$clone(deep = TRUE)\nilpd_num$select(setdiff(ilpd_num$feature_names, \"gender\"))\n\nYour task is to train a simple multi layer perceptron (lrn(\"classif.mlp\")) with 2 hidden layers with 100 neurons each. Set the batch size to 32, the learning rate to 0.001 and the number of epochs to 20. Then, resample the learner on the task with a cross-validation with 5 folds and evaluate the results using classification error and false positive rate (FPR). Is the result good?\n\n\nHint\n\nThe parameter for the learning rate is opt.lr\n\n\nmlp &lt;- lrn(\"classif.mlp\",\n  neurons = c(100, 100),\n  batch_size = 32,\n  epochs = 20,\n  opt.lr = 0.001\n)\ncv10 &lt;- rsmp(\"cv\", folds = 10)\nrr1 &lt;- resample(task = ilpd_num, learner = mlp, resampling = cv10)\nrr1$aggregate(msrs(c(\"classif.ce\", \"classif.fpr\")))\n\n classif.ce classif.fpr \n  0.3671537   0.6417584 \n\n\nWhile the classification error is low, this is not a good measure due to the imbalanced class distribution. This is confirmed by the FPR, which is relatively high.\nQuestion 2: Preprocessing\nIn the previous question, we have operated on the ilpd_num task where we excluded the categorical gender column. This was done because the MLP learner operates on numeric features only. You will now create a more complex GraphLearner that also incudes one-hot encoding of the gender column before applying the MLP. Resample this learner on the original ilpd task and evaluate the results using the same measures as before.\n\n\nHint\n\nConcatenate po(\"encode\") with a lrn(\"classif.mlp\") using %&gt;&gt;% to create the GraphLearner. For available options on the encoding, see po(\"encode\")$help().\n\n\nencoder &lt;- po(\"encode\", method = \"one-hot\")\nglrn &lt;- as_learner(encoder %&gt;&gt;% mlp)\nrr2 &lt;- resample(ilpd, glrn, cv10)\nrr2$aggregate(msrs(c(\"classif.ce\", \"classif.fpr\")))\n\n classif.ce classif.fpr \n  0.2864115   0.8770833 \n\n\nQuestion 3: Benchmarking\nInstead of resampling a single learner, the goal is now to compare the performance of the MLP with a simple classification tree. Create a benchmark design and compare the performance of the two learners.\n\n\nHint\n\nCreate a classification tree via lrn(\"classif.rpart\"). A benchmark design can be created via benchmark_grid(). To run a benchmark, pass the design to benchmark().\n\n\ndesign &lt;- benchmark_grid(\n  task = ilpd,\n  learner = list(glrn, lrn(\"classif.rpart\", predict_type = \"prob\")),\n  resampling = cv10\n)\nbmr &lt;- benchmark(design)\nbmr$aggregate(msrs(c(\"classif.ce\", \"classif.tpr\")))\n\n      nr task_id         learner_id resampling_id iters classif.ce classif.tpr\n   &lt;int&gt;  &lt;char&gt;             &lt;char&gt;        &lt;char&gt; &lt;int&gt;      &lt;num&gt;       &lt;num&gt;\n1:     1    ilpd encode.classif.mlp            cv    10  0.2966686   0.8249284\n2:     2    ilpd      classif.rpart            cv    10  0.3035652   0.8301628\nHidden columns: resample_result\n\n\nQuestion 4: Iris as a Lazy Tensor\nCreate a version of the iris task where the 4 features are represented as a single lazy tensor column.\nSolution:\n\nX &lt;- as.matrix(iris[, 1:4])\ndt_class &lt;- dataset(\"iris\",\n  initialize = function(x) {\n    self$x &lt;- torch_tensor(x)\n  },\n  .getbatch = function(index) {\n    list(x = self$x[index, , drop = FALSE])\n  },\n  .length = function() {\n    nrow(self$x)\n  }\n)\ndt &lt;- dt_class(X)\nlt &lt;- as_lazy_tensor(dt, dataset_shapes = list(x = c(NA, 4)))\ndt &lt;- data.table(\n  x = lt,\n  Species = iris$Species\n)\ntask &lt;- as_task_classif(dt, target = \"Species\")\ntask\n\n&lt;TaskClassif:dt&gt; (150 x 2)\n* Target: Species\n* Properties: multiclass\n* Features (1):\n  - lt (1): x\n\n\nQuestion 5: Custom Architecture\nCreate a network with one hidden layer with 100 neurons and a sigmoid activation function by assempling PipeOps in a Graph. Convert the Graph to a Learner and train the network for 10 epochs using Adam with a learning rate of 0.001 and a batch size of 32.\nSolution:\n\ngraph &lt;- po(\"torch_ingress_ltnsr\") %&gt;&gt;%\n  po(\"nn_linear\", out_features = 100) %&gt;&gt;%\n  po(\"nn_sigmoid\") %&gt;&gt;%\n  po(\"nn_head\") %&gt;&gt;%\n  po(\"torch_loss\", t_loss(\"cross_entropy\")) %&gt;&gt;%\n  po(\"torch_optimizer\", optimizer = t_opt(\"adam\"), lr = 0.001) %&gt;&gt;%\n  po(\"torch_model_classif\", batch_size = 32, epochs = 10)\n\nglrn &lt;- as_learner(graph)\nglrn$train(task)"
  },
  {
    "objectID": "notebooks/5-cnn-exercise.html",
    "href": "notebooks/5-cnn-exercise.html",
    "title": "CNN Exercises",
    "section": "",
    "text": "Question 1: Manual Convolution\nIn this exercise, your task is to implement a function that performs a 2D convolution operation manually on a 3D input image using a given 3D kernel.\nThe input is a tensor with dimensions [channels, height, width] and the kernel is a tensor with dimensions [channels, kH, kW]. Your goal is to produce an output tensor of shape [height - kH + 1, width - kW + 1] by applying the convolution operation. Recall that output element is computed as the sum of the element-wise multiplication of the kernel and the corresponding patch of the input image.\n\nfconv2d &lt;- function(image, kernel, bias) {\n  ...\n}\n\nYou can use the code below to check your implementation against the conv2d module from the torch package.\n\nlibrary(torch)\nconv &lt;- nn_conv2d(3, 1, kernel_size = c(3, 3))\nkernel &lt;- conv$parameters$weight\nbias &lt;- conv$parameters$bias\n\ninput &lt;- torch_randn(1, 3, 28, 28)\ntorch_allclose(\n  fconv2d(input$squeeze(), kernel$squeeze(), bias$squeeze()),\n  conv(input),\n  atol = 1e-5\n)\n\n[1] TRUE\n\n\n\n\nHint\n\n\nAllocate a new empty tensor of the correct size to store the output.\nUsing a nested loop, iterate over each valid spatial location in the input and multiply the corresponding patch of the input with the kernel (torch_sum(patch * kernel)) and add the bias.\n\n\nQuestion 2: Be edgey\nConstruct a convolutional 2x2 kernel that extracts the edges of an image. Apply it using the fconv2d function from the previous exercise.\nAs an input, we use an image from MNIST. You can use the plot_2d_image function from the helper script to plot the image.\n\nlibrary(torchvision)\nsource(here::here(\"scripts/helper.R\"))\nmnist &lt;- mnist_dataset(root = \"data\", download = TRUE)\nimage &lt;- mnist$.getitem(13)$x\nplot_2d_image(image)\n\n\n\n\n\n\n\n\nTo get started, use the code below and modify the values of the kernel.\n\nkernel &lt;- matrix(c(0.53, 0.34, 0.22, 0.1), byrow = TRUE, nrow = 2)\nkernel\n\n     [,1] [,2]\n[1,] 0.53 0.34\n[2,] 0.22 0.10\n\nkernel &lt;- torch_tensor(kernel)$unsqueeze(1)\n\nimageout &lt;- fconv2d(torch_tensor(image)$unsqueeze(1), kernel, 0)\nplot_2d_image(imageout$squeeze())"
  },
  {
    "objectID": "notebooks/5-cnn-exercise-solution.html",
    "href": "notebooks/5-cnn-exercise-solution.html",
    "title": "CNN Exercises",
    "section": "",
    "text": "Question 1: Manual Convolution\nIn this exercise, your task is to implement a function that performs a 2D convolution operation manually on a 3D input image using a given 3D kernel.\nThe input is a tensor with dimensions [channels, height, width] and the kernel is a tensor with dimensions [channels, kH, kW]. Your goal is to produce an output tensor of shape [height - kH + 1, width - kW + 1] by applying the convolution operation. Recall that output element is computed as the sum of the element-wise multiplication of the kernel and the corresponding patch of the input image.\n\nfconv2d &lt;- function(image, kernel, bias) {\n  ...\n}\n\nYou can use the code below to check your implementation against the conv2d module from the torch package.\n\nlibrary(torch)\nconv &lt;- nn_conv2d(3, 1, kernel_size = c(3, 3))\nkernel &lt;- conv$parameters$weight\nbias &lt;- conv$parameters$bias\n\ninput &lt;- torch_randn(1, 3, 28, 28)\ntorch_allclose(\n  fconv2d(input$squeeze(), kernel$squeeze(), bias$squeeze()),\n  conv(input),\n  atol = 1e-5\n)\n\n[1] TRUE\n\n\n\n\nHint\n\n\nAllocate a new empty tensor of the correct size to store the output.\nUsing a nested loop, iterate over each valid spatial location in the input and multiply the corresponding patch of the input with the kernel (torch_sum(patch * kernel)) and add the bias.\n\n\nSolution\n\nfconv2d &lt;- function(image, kernel, bias) {\n  channels &lt;- image$size(1)\n  height &lt;- image$size(2)\n  width &lt;- image$size(3)\n  kH &lt;- kernel$size(2)\n  kW  &lt;- kernel$size(3)\n\n  new_image &lt;- torch_zeros(1, height - kH + 1, width - kW + 1)\n\n  for (i in seq_len(height - kH + 1)) {\n    for (j in seq_len(width - kW + 1)) {\n      patch &lt;- image[.., i:(i + kH - 1), j:(j + kW - 1)]\n      new_image[.., i, j] &lt;- torch_sum(patch * kernel) + bias\n    }\n  }\n  new_image\n}\n\nQuestion 2: Be edgey\nConstruct a convolutional 2x2 kernel that extracts the edges of an image. Apply it using the fconv2d function from the previous exercise.\nAs an input, we use an image from MNIST. You can use the plot_2d_image function from the helper script to plot the image.\n\nlibrary(torchvision)\nsource(here::here(\"scripts/helper.R\"))\nmnist &lt;- mnist_dataset(root = \"data\")\nimage &lt;- mnist$.getitem(13)$x\nplot_2d_image(image)\n\n\n\n\n\n\n\n\nTo get started, use the code below and modify the values of the kernel.\n\nkernel &lt;- matrix(c(0.53, 0.34, 0.22, 0.1), byrow = TRUE, nrow = 2)\nkernel\n\n     [,1] [,2]\n[1,] 0.53 0.34\n[2,] 0.22 0.10\n\nkernel &lt;- torch_tensor(kernel)$unsqueeze(1)\n\nimageout &lt;- fconv2d(torch_tensor(image)$unsqueeze(1), kernel, 0)\nplot_2d_image(imageout$squeeze())\n\n\n\n\n\n\n\n\nSolution\n\nedge_kernel &lt;- torch_tensor(matrix(c(-1, -1, 1, 1), byrow = TRUE, nrow = 3))\n\nWarning in matrix(c(-1, -1, 1, 1), byrow = TRUE, nrow = 3): data length [4] is not a sub-multiple or multiple of the\nnumber of rows [3]\n\nplot_2d_image(edge_kernel$squeeze())"
  },
  {
    "objectID": "notebooks/4-optimizer-exercise.html",
    "href": "notebooks/4-optimizer-exercise.html",
    "title": "Optimizer Exercises",
    "section": "",
    "text": "Question 1\nIn this exercise, the task is to play around with the settings for the optimization of a neural network. We start by generating some (highly non-linear) synthetic data using the mlbench package.\n\nlibrary(torch)\ndata &lt;- mlbench::mlbench.friedman3(n = 3000, sd = 0.1)\nX &lt;- torch_tensor(data$x)\nX[1:2, ]\n\ntorch_tensor\n   40.8977   745.3378     0.3715     3.4246\n   88.3017  1148.7073     0.5288     6.5953\n[ CPUFloatType{2,4} ]\n\nY &lt;- torch_tensor(data$y)$unsqueeze(2)\nY[1:2, ]\n\ntorch_tensor\n 1.5238\n 1.3572\n[ CPUFloatType{2,1} ]\n\n\nThe associated machine learning task is to predict the output Y from the input X.\nNext, we create a dataset for it using the tensor_dataset() function.\n\nds &lt;- tensor_dataset(X, Y)\nds$.getitem(1)\n\n[[1]]\ntorch_tensor\n  40.8977\n 745.3378\n   0.3715\n   3.4246\n[ CPUFloatType{4} ]\n\n[[2]]\ntorch_tensor\n 1.5238\n[ CPUFloatType{1} ]\n\n\nWe can create two sub-datasets – for training and validation – using dataset_subset().\n\nids_train &lt;- sample(1000, 700)\nids_valid &lt;- setdiff(seq_len(1000), ids_train)\nds_train &lt;- dataset_subset(ds, ids_train)\nds_valid &lt;- dataset_subset(ds, ids_valid)\n\nThe network that we will be fitting is a simple MLP:\n\nnn_mlp &lt;- nn_module(\"nn_mlp\",\n  initialize = function() {\n    self$lin1 &lt;- nn_linear(4, 50)\n    self$lin2 &lt;- nn_linear(50, 50)\n    self$lin3 &lt;- nn_linear(50, 1)\n  },\n  forward = function(x) {\n    x |&gt;\n      self$lin1() |&gt;\n      nnf_relu() |&gt;\n      self$lin2() |&gt;\n      nnf_relu() |&gt;\n      self$lin3()\n  }\n)\n\nThe code to compare different optimizer configurations is provided via the compare_configs() function, which you can copy paste.\n\n\nImplementation of compare_configs\n\n\nlibrary(ggplot2)\ncompare_configs &lt;- function(epochs = 30, batch_size = 16, lr = 0.01, weight_decay = 0.01, beta1 = 0.9, beta2 = 0.999) {\n  # Identify which parameter is a list\n  args &lt;- list(batch_size = batch_size, lr = lr, weight_decay = weight_decay, beta1 = beta1, beta2 = beta2)\n  is_list &lt;- sapply(args, is.list)\n\n  if (sum(is_list) != 1) {\n    stop(\"One of the arguments must be a list\")\n  }\n\n  list_arg_name &lt;- names(args)[is_list]\n  list_args &lt;- args[[list_arg_name]]\n  other_args &lt;- args[!is_list]\n\n  # Run train_valid for each value in the list\n  results &lt;- lapply(list_args, function(arg) {\n    network &lt;- with_torch_manual_seed(seed = 123, nn_mlp())\n    other_args[[list_arg_name]] &lt;- arg\n    train_valid(network, ds_train = ds_train, ds_valid = ds_valid, epochs = epochs, batch_size = other_args$batch_size,\n      lr = other_args$lr, betas = c(other_args$beta1, other_args$beta2), weight_decay = other_args$weight_decay)\n  })\n\n  # Combine results into a single data frame\n  combined_results &lt;- do.call(rbind, lapply(seq_along(results), function(i) {\n    df &lt;- results[[i]]\n    df$config &lt;- paste(list_arg_name, \"=\", list_args[[i]])\n    df\n  }))\n\n  upper &lt;- if (max(combined_results$valid_loss) &gt; 10) quantile(combined_results$valid_loss, 0.98) else max(combined_results$valid_loss)\n\n  ggplot(combined_results, aes(x = epoch, y = valid_loss, color = config)) +\n    geom_line() +\n    theme_minimal() +\n    labs(x = \"Epoch\", y = \"Validation RMSE\", color = \"Configuration\") +\n    ylim(min(combined_results$valid_loss), upper)\n}\ntrain_loop &lt;- function(network, dl_train, opt) {\n  network$train()\n  coro::loop(for (batch in dl_train) {\n    opt$zero_grad()\n    Y_pred &lt;- network(batch[[1]])\n    loss &lt;- nnf_mse_loss(Y_pred, batch[[2]])\n    loss$backward()\n    opt$step()\n  })\n}\n\nvalid_loop &lt;- function(network, dl_valid) {\n  network$eval()\n  valid_loss &lt;- c()\n  coro::loop(for (batch in dl_valid) {\n    Y_pred &lt;- with_no_grad(network(batch[[1]]))\n    loss &lt;- sqrt(nnf_mse_loss(Y_pred, batch[[2]]))\n    valid_loss &lt;- c(valid_loss, loss$item())\n  })\n  mean(valid_loss)\n}\n\ntrain_valid &lt;- function(network, ds_train, ds_valid, epochs, batch_size, ...) {\n  opt &lt;- optim_ignite_adamw(network$parameters, ...)\n  train_losses &lt;- numeric(epochs)\n  valid_losses &lt;- numeric(epochs)\n  dl_train &lt;- dataloader(ds_train, batch_size = batch_size)\n  dl_valid &lt;- dataloader(ds_valid, batch_size = batch_size)\n  for (epoch in seq_len(epochs)) {\n    train_loop(network, dl_train, opt)\n    valid_losses[epoch] &lt;- valid_loop(network, dl_valid)\n  }\n  data.frame(epoch = seq_len(epochs), valid_loss = valid_losses)\n}\n\n\nIt takes as arguments:\n\nepochs: The number of epochs to train for. Defaults to 30.\nbatch_size: The batch size to use for training. Defaults to 16.\nlr: The learning rate to use for training. Defaults to 0.01.\nweight_decay: The weight decay to use for training. Defaults to 0.01.\nbeta1: The momentum parameter to use for training. Defaults to 0.9.\nbeta2: The adaptive step size parameter to use for training. Defaults to 0.999.\n\nYou can, e.g., call the function like below:\n\ncompare_configs(epochs = 30, lr = list(0.1, 0.2), weight_decay = 0.02)\n\n\n\n\n\n\n\n\nOne of the arguments (except for epochs) must be a list of values. The function will then run the same training configuration for each of the values in the list and visualize the results.\nExplore a few hyperparameter settings and make some observations as to how they affect the trajectory of the validation loss. The purpose of this exercise is not to derive observations that hold in general.\nQuestion 2: Optimization with Momentum\nIn this exercise, you will build a gradient descent optimizer with momentum. As a use case, we will minimize the Rosenbrock function. The function is defined as:\n\nrosenbrock &lt;- function(x, y) {\n  (1 - x)^2 + 2 * (y - x^2)^2\n}\nrosenbrock(torch_tensor(-1), torch_tensor(-1))\n\ntorch_tensor\n 12\n[ CPUFloatType{1} ]\n\n\nThe ‘parameters’ we will be optimizing is the position of a point (x, y), which will both be updated using gradient descent. The figure below shows the Rosenbrock function, where darker values indicate lower values.\n\n\n\n\n\n\n\n\n\nThe task is to implement the optim_step() function.\n\noptim_step &lt;- function(x, y, lr, x_momentum, y_momentum, beta) {\n  ...\n}\n\nIt will receive as arguments, the current values x and y, the momentum values x_momentum and y_momentum (all scalar tensors) as well as the learning rate lr and the momentum parameter beta. The function should then:\n\nPerform a forward pass.\nCalculate the gradients.\nUpdate the momentum values in-place.\nUpdate the parameters in-place.\n\n\n\nHint\n\nTo perform in-place updates, you can use the $mul_() and $add_() methods.\n\nThe update rule is given exemplarily for x:\n\\[\n\\begin{aligned}\nv_{t + 1} &= \\beta v_t + (1 - \\beta) \\nabla_{x} f(x_t, y_t) \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nx_{t+1} &= x_t - \\eta v_{t + 1}\n\\end{aligned}\n\\]\nTo test your optimizer, you can use the code below. Note that you might have to play around with the number of steps and the learning rate to get a good result.\n\n\nCode to test your optimizer\n\n\noptimize_rosenbrock &lt;- function(steps, lr, beta) {\n  x &lt;- torch_tensor(-1, requires_grad = TRUE)\n  y &lt;- torch_tensor(2, requires_grad = TRUE)\n  momentum_x &lt;- torch_tensor(0)\n  momentum_y &lt;- torch_tensor(0)\n\n  trajectory &lt;- data.frame(\n    x = numeric(steps + 1),\n    y = numeric(steps + 1),\n    value = numeric(steps + 1)\n  )\n  for (step in seq_len(steps)){\n    optim_step(x, y, lr, momentum_x, momentum_y, beta)\n    x$grad$zero_()\n    y$grad$zero_()\n    trajectory$x[step] &lt;- x$item()\n    trajectory$y[step] &lt;- y$item()\n  }\n  trajectory$x[steps + 1] &lt;- x$item()\n  trajectory$y[steps + 1] &lt;- y$item()\n\n  plot_rosenbrock() +\n    geom_path(data = trajectory, aes(x = x, y = y, z = NULL), color = \"red\") +\n    labs(title = \"Optimization Path on Rosenbrock Function\", x = \"x\", y = \"y\")\n}\n\n\nQuestion 3: Weight Decay\nIn exercise 2, we have optimized the Rosenbrock function. Does it make sense to also use weight decay here?"
  },
  {
    "objectID": "notebooks/4-optimizer-exercise-solution.html",
    "href": "notebooks/4-optimizer-exercise-solution.html",
    "title": "Optimizer Exercises",
    "section": "",
    "text": "Question 1\nIn this exercise, the task is to play around with the settings for the optimization of a neural network. We start by generating some (highly non-linear) synthetic data using the mlbench package.\n\nlibrary(torch)\ndata &lt;- mlbench::mlbench.friedman3(n = 3000, sd = 0.1)\nX &lt;- torch_tensor(data$x)\nX[1:2, ]\n\ntorch_tensor\n   40.8977   745.3378     0.3715     3.4246\n   88.3017  1148.7073     0.5288     6.5953\n[ CPUFloatType{2,4} ]\n\nY &lt;- torch_tensor(data$y)$unsqueeze(2)\nY[1:2, ]\n\ntorch_tensor\n 1.5238\n 1.3572\n[ CPUFloatType{2,1} ]\n\n\nThe associated machine learning task is to predict the output Y from the input X.\nNext, we create a dataset for it using the tensor_dataset() function.\n\nds &lt;- tensor_dataset(X, Y)\nds$.getitem(1)\n\n[[1]]\ntorch_tensor\n  40.8977\n 745.3378\n   0.3715\n   3.4246\n[ CPUFloatType{4} ]\n\n[[2]]\ntorch_tensor\n 1.5238\n[ CPUFloatType{1} ]\n\n\nWe can create two sub-datasets – for training and validation – using dataset_subset().\n\nids_train &lt;- sample(1000, 700)\nids_valid &lt;- setdiff(seq_len(1000), ids_train)\nds_train &lt;- dataset_subset(ds, ids_train)\nds_valid &lt;- dataset_subset(ds, ids_valid)\n\nThe network that we will be fitting is a simple MLP:\n\nnn_mlp &lt;- nn_module(\"nn_mlp\",\n  initialize = function() {\n    self$lin1 &lt;- nn_linear(4, 50)\n    self$lin2 &lt;- nn_linear(50, 50)\n    self$lin3 &lt;- nn_linear(50, 1)\n  },\n  forward = function(x) {\n    x |&gt;\n      self$lin1() |&gt;\n      nnf_relu() |&gt;\n      self$lin2() |&gt;\n      nnf_relu() |&gt;\n      self$lin3()\n  }\n)\n\nThe code to compare different optimizer configurations is provided via the compare_configs() function, which you can copy paste.\n\n\nImplementation of compare_configs\n\n\nlibrary(ggplot2)\ncompare_configs &lt;- function(epochs = 30, batch_size = 16, lr = 0.01, weight_decay = 0.01, beta1 = 0.9, beta2 = 0.999) {\n  # Identify which parameter is a list\n  args &lt;- list(batch_size = batch_size, lr = lr, weight_decay = weight_decay, beta1 = beta1, beta2 = beta2)\n  is_list &lt;- sapply(args, is.list)\n\n  if (sum(is_list) != 1) {\n    stop(\"One of the arguments must be a list\")\n  }\n\n  list_arg_name &lt;- names(args)[is_list]\n  list_args &lt;- args[[list_arg_name]]\n  other_args &lt;- args[!is_list]\n\n  # Run train_valid for each value in the list\n  results &lt;- lapply(list_args, function(arg) {\n    network &lt;- with_torch_manual_seed(seed = 123, nn_mlp())\n    other_args[[list_arg_name]] &lt;- arg\n    train_valid(network, ds_train = ds_train, ds_valid = ds_valid, epochs = epochs, batch_size = other_args$batch_size,\n      lr = other_args$lr, betas = c(other_args$beta1, other_args$beta2), weight_decay = other_args$weight_decay)\n  })\n\n  # Combine results into a single data frame\n  combined_results &lt;- do.call(rbind, lapply(seq_along(results), function(i) {\n    df &lt;- results[[i]]\n    df$config &lt;- paste(list_arg_name, \"=\", list_args[[i]])\n    df\n  }))\n\n  upper &lt;- if (max(combined_results$valid_loss) &gt; 10) quantile(combined_results$valid_loss, 0.98) else max(combined_results$valid_loss)\n\n  ggplot(combined_results, aes(x = epoch, y = valid_loss, color = config)) +\n    geom_line() +\n    theme_minimal() +\n    labs(x = \"Epoch\", y = \"Validation RMSE\", color = \"Configuration\") +\n    ylim(min(combined_results$valid_loss), upper)\n}\ntrain_loop &lt;- function(network, dl_train, opt) {\n  network$train()\n  coro::loop(for (batch in dl_train) {\n    opt$zero_grad()\n    Y_pred &lt;- network(batch[[1]])\n    loss &lt;- nnf_mse_loss(Y_pred, batch[[2]])\n    loss$backward()\n    opt$step()\n  })\n}\n\nvalid_loop &lt;- function(network, dl_valid) {\n  network$eval()\n  valid_loss &lt;- c()\n  coro::loop(for (batch in dl_valid) {\n    Y_pred &lt;- with_no_grad(network(batch[[1]]))\n    loss &lt;- sqrt(nnf_mse_loss(Y_pred, batch[[2]]))\n    valid_loss &lt;- c(valid_loss, loss$item())\n  })\n  mean(valid_loss)\n}\n\ntrain_valid &lt;- function(network, ds_train, ds_valid, epochs, batch_size, ...) {\n  opt &lt;- optim_ignite_adamw(network$parameters, ...)\n  train_losses &lt;- numeric(epochs)\n  valid_losses &lt;- numeric(epochs)\n  dl_train &lt;- dataloader(ds_train, batch_size = batch_size)\n  dl_valid &lt;- dataloader(ds_valid, batch_size = batch_size)\n  for (epoch in seq_len(epochs)) {\n    train_loop(network, dl_train, opt)\n    valid_losses[epoch] &lt;- valid_loop(network, dl_valid)\n  }\n  data.frame(epoch = seq_len(epochs), valid_loss = valid_losses)\n}\n\n\nIt takes as arguments:\n\nepochs: The number of epochs to train for. Defaults to 30.\nbatch_size: The batch size to use for training. Defaults to 16.\nlr: The learning rate to use for training. Defaults to 0.01.\nweight_decay: The weight decay to use for training. Defaults to 0.01.\nbeta1: The momentum parameter to use for training. Defaults to 0.9.\nbeta2: The adaptive step size parameter to use for training. Defaults to 0.999.\n\nYou can, e.g., call the function like below:\n\ncompare_configs(epochs = 30, lr = list(0.1, 0.2), weight_decay = 0.02)\n\n\n\n\n\n\n\n\nOne of the arguments (except for epochs) must be a list of values. The function will then run the same training configuration for each of the values in the list and visualize the results.\nExplore a few hyperparameter settings and make some observations as to how they affect the trajectory of the validation loss. The purpose of this exercise is not to derive observations that hold in general.\nSolution\nThere is not really a ‘solution’ to this exercise. We will go through some of the configurations and try to explain what is happening.\nLearning rate\n\ncompare_configs(epochs = 30, lr = list(1,  0.1, 0.01, 0.001))\n\n\n\n\n\n\n\n\nToo large learning rates lead to unstable updates and divergence. It is not exactly clear why the small learning rate is so unstable, possibly it gets stuck in a bad region of the loss landscape.\nBatch Size\nFor this configuration, we can see that smaller batch sizes lead to considerably better results as they allow for more updates for the given number of epochs.\n\ncompare_configs(epochs = 30, batch_size = list(2, 4, 8, 16, 32, 64))\n\n\n\n\n\n\n\n\nNote that this comparison is not entirely fair as the number of epochs is fixed. We would rather be interested in the performance of different batch sizes for a fixed amount of time and not epochs.\nWeight Decay\nFor too large values of the weight decay, we see that the network struggles to get a good validation loss at first.\n\ncompare_configs(epochs = 30, weight_decay = list(0.5, 0.25, 0.125, 0.0625, 0.03125))\n\n\n\n\n\n\n\n\nBeta 1\nFor the momentum parameter, we can observe that too large values for the momentum parameter lead to oscillations, possibly because the local information is not used enough. Too small values lead to worse performance.\n\ncompare_configs(epochs = 30, beta1 = list(0.5, 0.85, 0.9, 0.95, 0.99, 0.999))\n\n\n\n\n\n\n\n\nBeta 2\nFor larger values of \\(\\beta_2\\), the loss trajectory is considerably smoother.\n\ncompare_configs(epochs = 30, beta2 = list(0.99, 0.999, 0.9999))\n\n\n\n\n\n\n\n\nQuestion 2: Optimization with Momentum\nIn this exercise, you will build a gradient descent optimizer with momentum. As a use case, we will minimize the Rosenbrock function. The function is defined as:\n\nrosenbrock &lt;- function(x, y) {\n  (1 - x)^2 + 2 * (y - x^2)^2\n}\nrosenbrock(torch_tensor(-1), torch_tensor(-1))\n\ntorch_tensor\n 12\n[ CPUFloatType{1} ]\n\n\nThe ‘parameters’ we will be optimizing is the position of a point (x, y), which will both be updated using gradient descent. The figure below shows the Rosenbrock function, where darker values indicate lower values.\n\n\n\n\n\n\n\n\n\nThe task is to implement the optim_step() function.\n\noptim_step &lt;- function(x, y, lr, x_momentum, y_momentum, beta) {\n  ...\n}\n\nIt will receive as arguments, the current values x and y, the momentum values x_momentum and y_momentum (all scalar tensors) as well as the learning rate lr and the momentum parameter beta. The function should then:\n\nPerform a forward pass.\nCalculate the gradients.\nUpdate the momentum values in-place.\nUpdate the parameters in-place.\n\n\n\nHint\n\nTo perform in-place updates, you can use the $mul_() and $add_() methods.\n\nThe update rule is given exemplarily for x:\n\\[\n\\begin{aligned}\nv_{t + 1} &= \\beta v_t + (1 - \\beta) \\nabla_{x} f(x_t, y_t) \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nx_{t+1} &= x_t - \\eta v_{t + 1}\n\\end{aligned}\n\\]\nTo test your optimizer, you can use the code below. Note that you might have to play around with the number of steps and the learning rate to get a good result.\n\n\nCode to test your optimizer\n\n\noptimize_rosenbrock &lt;- function(steps, lr, beta) {\n  x &lt;- torch_tensor(-1, requires_grad = TRUE)\n  y &lt;- torch_tensor(2, requires_grad = TRUE)\n  momentum_x &lt;- torch_tensor(0)\n  momentum_y &lt;- torch_tensor(0)\n\n  trajectory &lt;- data.frame(\n    x = numeric(steps + 1),\n    y = numeric(steps + 1),\n    value = numeric(steps + 1)\n  )\n  for (step in seq_len(steps)){\n    optim_step(x, y, lr, momentum_x, momentum_y, beta)\n    x$grad$zero_()\n    y$grad$zero_()\n    trajectory$x[step] &lt;- x$item()\n    trajectory$y[step] &lt;- y$item()\n  }\n  trajectory$x[steps + 1] &lt;- x$item()\n  trajectory$y[steps + 1] &lt;- y$item()\n\n  plot_rosenbrock() +\n    geom_path(data = trajectory, aes(x = x, y = y, z = NULL), color = \"red\") +\n    labs(title = \"Optimization Path on Rosenbrock Function\", x = \"x\", y = \"y\")\n}\n\n\nSolution\n\noptim_step &lt;- function(x, y, lr, x_momentum, y_momentum, beta) {\n  value &lt;- rosenbrock(x, y)\n  value$backward()\n  x_momentum$mul_(beta)\n  x_momentum$add_((1 - beta) * x$grad)\n  y_momentum$mul_(beta)\n  y_momentum$add_((1 - beta) * y$grad)\n  with_no_grad({\n    x$sub_(lr * x_momentum)\n    y$sub_(lr * y_momentum)\n  })\n  value$item()\n}\noptimize_rosenbrock(steps = 500, lr = 0.01, beta = 0.9)\n\n\n\n\n\n\n\n\nQuestion 3: Weight Decay\nIn exercise 2, we have optimized the Rosenbrock function. Does it make sense to also use weight decay here?\nSolution No, it does not make sense to use weight decay for this optimization as it is intended to prevent overfitting which is not a problem in this case. This is, because there is no uncertainty in the function we are optimizing."
  },
  {
    "objectID": "notebooks/3-modules-data-exercise.html",
    "href": "notebooks/3-modules-data-exercise.html",
    "title": "It’s a Sin(us)",
    "section": "",
    "text": "Question 1: Create a torch::dataset class that takes in arguments n, min, and max during initialization where:\n\nn is the total number of samples\nmin is the lower bound of the data\nmax is the upper bound of the data\n\nIn the initialize method, generate and store:\n\na tensor x of shape (n, 1) that contains n values drawn from a uniform distribution between min and max\na tensor y of shape (n, 1) that is defined as \\(\\sin(x) + \\epsilon\\) where \\(\\epsilon\\) is drawn from a normal distribution with mean 0 and standard deviation 0.1\n\n\n\nHint\n\nUse torch_randn() to draw from a standard normal distribution and multiply it with 0.1^2 to get the desired standard deviation.\n\nImplement the $.getbatch() method to return a named list with values x and y. Then, create an instance of the dataset with n = 1000, min = 0, and max = 10.\nMake sure that the dataset is working by calling the $.getbatch() method. Also, check that the shapes of both tensors returned by the dataset are (n_batch, 1).\nQuestion 2: Create a torch::dataloader that takes in the dataset and returns batches of size 10. Then, iterate over the batches of the dataloader and create one tensor X and one tensor Y that contains the concatenated batches of x and y.\n\n\nHint\n\nThe functions coro::loop() and torch_cat() might be helpful.\n\nQuestion 3: Create a custom torch module that allows modeling the sinus data we have created. To test it, apply it to the tensor X we have created above and calculate its mean squared error with the tensor Y. Don’t forget to run it without tracking gradients.\n\n\nHint\n\nYou can either use nn_module to create a custom module generically, or you can use nn_sequential() to create a custom module that is a sequence of layers.\n\nQuestion 4: Train the model on the task for different hyperparameters (lr or epochs) and visualize the results. Play around with the hyperparameters until you get a good fit. You can use the following code for that:\n\nlibrary(ggplot2)\npredict_network &lt;- function(net, dataloader) {\n  local_no_grad()\n  xs &lt;- list(x = numeric(), y = numeric(), pred = numeric())\n  i &lt;- 1\n  net$eval()\n  coro::loop(for (batch in dataloader) {\n    xs$x &lt;- c(xs$x, as.numeric(batch$x))\n    xs$y &lt;- c(xs$y, as.numeric(batch$y))\n    xs$pred &lt;- c(xs$pred, as.numeric(net(batch$x)))\n  })\n  as.data.frame(xs)\n}\ntrain_network &lt;- function(net, dataloader, epochs, lr) {\n  optimizer &lt;- optim_ignite_adamw(net$parameters, lr = lr)\n  net$train()\n  for (i in seq_len(epochs)) {\n    coro::loop(for (batch in dataloader) {\n      optimizer$zero_grad()\n      Y_pred &lt;- net(batch$x)\n      loss &lt;- nnf_mse_loss(Y_pred, batch$y)\n      loss$backward()\n      optimizer$step()\n    })\n  }\n  predict_network(net, dataloader)\n}\nplot_results &lt;- function(df) {\n  ggplot(data = df, aes(x = x)) +\n    geom_point(aes(y = y, color = \"true\")) +\n    geom_point(aes(y = pred, color = \"pred\")) +\n    theme_minimal() +\n    labs(color = \"\")\n}\ntrain_and_plot &lt;- function(net, dataloader, epochs = 10, lr = 0.01) {\n  result &lt;- train_network(net, dataloader, epochs = epochs, lr = lr)\n  plot_results(result)\n}\n\n\n\n\n\n\n\nTip\n\n\n\nBeware of the reference semantics and make sure that you create a new instance of the network for each run.\n\n\nQuestion 5: Create a new instance from the sinus dataset class created earlier. Now, set the min and max values to 10 and 20 respectively and visualize the predictions of the previously trained network on this new dataset. What do you observe? Can you explain why this is happening and can you fix the network architecture to make it work?\n\n\nHint\n\nThe sinus function has a phase of \\(2 \\pi\\)."
  },
  {
    "objectID": "notebooks/3-modules-data-exercise-solution.html",
    "href": "notebooks/3-modules-data-exercise-solution.html",
    "title": "It’s a Sin(us)",
    "section": "",
    "text": "Question 1: Create a torch::dataset class that takes in arguments n, min, and max during initialization where:\n\nn is the total number of samples\nmin is the lower bound of the data\nmax is the upper bound of the data\n\nIn the initialize method, generate and store:\n\na tensor x of shape (n, 1) that contains n values drawn from a uniform distribution between min and max\na tensor y of shape (n, 1) that is defined as \\(\\sin(x) + \\epsilon\\) where \\(\\epsilon\\) is drawn from a normal distribution with mean 0 and standard deviation 0.1\n\n\n\nHint\n\nUse torch_randn() to draw from a standard normal distribution and multiply it with 0.1^2 to get the desired standard deviation.\n\nImplement the $.getbatch() method to return a named list with values x and y. Then, create an instance of the dataset with n = 1000, min = 0, and max = 10.\nMake sure that the dataset is working by calling the $.getbatch() method. Also, check that the shapes of both tensors returned by the dataset are (n_batch, 1).\nSolution\n\nlibrary(torch)\nsin_dataset &lt;- dataset(\n  initialize = function(n, min, max) {\n    self$x &lt;- torch_rand(n, 1) * (max - min) + min\n    self$y &lt;- torch_sin(self$x) + torch_randn(n, 1) * 0.1^1\n  },\n  .getbatch = function(i) {\n    list(x = self$x[i, drop = FALSE], y = self$y[i, drop = FALSE])\n  },\n  .length = function() {\n    length(self$x)\n  }\n)\nds &lt;- sin_dataset(n = 1000, min = 0, max = 10)\nbatch &lt;- ds$.getbatch(1:10)\nbatch$x$shape\n\n[1] 10  1\n\nbatch$y$shape\n\n[1] 10  1\n\n\nQuestion 2: Create a torch::dataloader that takes in the dataset and returns batches of size 10. Then, iterate over the batches of the dataloader and create one tensor X and one tensor Y that contains the concatenated batches of x and y.\n\n\nHint\n\nThe functions coro::loop() and torch_cat() might be helpful.\n\nSolution\n\ndl &lt;- dataloader(ds, batch_size = 10)\nbatches &lt;- list()\ncoro::loop(for (batch in dl) {\n  batches &lt;- c(batches, list(batch))\n})\nX &lt;- torch_cat(lapply(batches, function(batch) batch$x), dim = 1)\nX$shape\n\n[1] 1000    1\n\nY &lt;- torch_cat(lapply(batches, function(batch) batch$y), dim = 1)\nY$shape\n\n[1] 1000    1\n\n\nQuestion 3: Create a custom torch module that allows modeling the sinus data we have created. To test it, apply it to the tensor X we have created above and calculate its mean squared error with the tensor Y. Don’t forget to run it without tracking gradients.\n\n\nHint\n\nYou can either use nn_module to create a custom module generically, or you can use nn_sequential() to create a custom module that is a sequence of layers.\n\nSolution\n\nnn_sin &lt;- nn_module(\"nn_sin\",\n  initialize = function(latent = 200) {\n    self$lin1 &lt;- nn_linear(1, latent)\n    self$lin2 &lt;- nn_linear(latent, latent)\n    self$lin3 &lt;- nn_linear(latent, 1)\n  },\n  forward = function(x) {\n    x |&gt;\n      self$lin1() |&gt;\n      nnf_relu() |&gt;\n      self$lin2() |&gt;\n      nnf_relu() |&gt;\n      self$lin3()\n  }\n)\nnet &lt;- nn_sin(200)\nY_pred &lt;- with_no_grad(net(X))\nnnf_mse_loss(Y_pred, Y)\n\ntorch_tensor\n1.08382\n[ CPUFloatType{} ]\n\n\nQuestion 4: Train the model on the task for different hyperparameters (lr or epochs) and visualize the results. Play around with the hyperparameters until you get a good fit. You can use the following code for that:\n\nlibrary(ggplot2)\npredict_network &lt;- function(net, dataloader) {\n  local_no_grad()\n  xs &lt;- list(x = numeric(), y = numeric(), pred = numeric())\n  i &lt;- 1\n  net$eval()\n  coro::loop(for (batch in dataloader) {\n    xs$x &lt;- c(xs$x, as.numeric(batch$x))\n    xs$y &lt;- c(xs$y, as.numeric(batch$y))\n    xs$pred &lt;- c(xs$pred, as.numeric(net(batch$x)))\n  })\n  as.data.frame(xs)\n}\ntrain_network &lt;- function(net, dataloader, epochs, lr) {\n  optimizer &lt;- optim_ignite_adamw(net$parameters, lr = lr)\n  net$train()\n  for (i in seq_len(epochs)) {\n    coro::loop(for (batch in dataloader) {\n      optimizer$zero_grad()\n      Y_pred &lt;- net(batch$x)\n      loss &lt;- nnf_mse_loss(Y_pred, batch$y)\n      loss$backward()\n      optimizer$step()\n    })\n  }\n  predict_network(net, dataloader)\n}\nplot_results &lt;- function(df) {\n  ggplot(data = df, aes(x = x)) +\n    geom_point(aes(y = y, color = \"true\")) +\n    geom_point(aes(y = pred, color = \"pred\")) +\n    theme_minimal() +\n    labs(color = \"\")\n}\ntrain_and_plot &lt;- function(net, dataloader, epochs = 10, lr = 0.01) {\n  result &lt;- train_network(net, dataloader, epochs = epochs, lr = lr)\n  plot_results(result)\n}\n\n\n\n\n\n\n\nTip\n\n\n\nBeware of the reference semantics and make sure that you create a new instance of the network for each run.\n\n\nSolution\n\nnet &lt;- nn_sin(200)\ntrain_and_plot(net, dl, epochs = 200, lr = 0.01)\n\n\n\n\n\n\n\n\nQuestion 5: Create a new instance from the sinus dataset class created earlier. Now, set the min and max values to 10 and 20 respectively and visualize the predictions of the previously trained network on this new dataset. What do you observe? Can you explain why this is happening and can you fix the network architecture to make it work?\n\n\nHint\n\nThe sinus function has a phase of \\(2 \\pi\\).\n\nSolution\n\ndl_ood &lt;- dataloader(sin_dataset(n = 1000, min = 0, max = 20), batch_size = 10)\nplot_results(predict_network(net, dl_ood))\n\n\n\n\n\n\n\n\nFor values in the range [10, 20], the network fails to generalize. This is because the network only observed values in the range [0, 10] during training.\nWe can fix this by preprocessing the data using the modulo operator, i.e. using the correct inductive bias for the problem.\n\nnn_sin2 &lt;- nn_module(\"nn_sin2\",\n  initialize = function(latent = 200) {\n    self$lin1 &lt;- nn_linear(1, latent)\n    self$lin2 &lt;- nn_linear(latent, latent)\n    self$lin3 &lt;- nn_linear(latent, 1)\n  },\n  forward = function(x) {\n    (x %% (2 * pi)) |&gt;\n      self$lin1() |&gt;\n      nnf_relu() |&gt;\n      self$lin2() |&gt;\n      nnf_relu() |&gt;\n      self$lin3()\n  }\n)\nnet2 &lt;- nn_sin2(200)\ndf &lt;- train_network(net2, dl, epochs = 200, lr = 0.01)\nplot_results(predict_network(net2, dl_ood))"
  },
  {
    "objectID": "notebooks/2-autograd-exercise.html",
    "href": "notebooks/2-autograd-exercise.html",
    "title": "Autograd",
    "section": "",
    "text": "Question 1: Appreciating autograd\nConsider the following function:\n\\[\nf(x) = x^2 + 3x + 2\n\\]\nAs well as the function \\(g(x) = f(f(f(x)))\\)\nCalculate the gradient of both functions at point \\(x = 2\\).\nQuestion 2: Approximating functions with gradients\nThe defining feature of the gradient is that it allows us to approximate the function locally by a linear function.\nI.e., for some value \\(x^*\\), we know for very small \\(\\delta\\), that\n\\[\nf(x^* + \\delta) \\approx f(x^*) + f'(x^*) \\cdot \\delta\n\\]\nPlot the function from earlier as well as the local linear approximation at \\(x = 2\\) using ggplot2.\n\n\nHint\n\nTo do so, follow these steps:\n\nCreate a sequence with 100 equidistant values between -4 to 4 using torch_linspace().\nCreate the true function values at these points using the function from exercise 1.\nApproximate the function using the formula \\(f(x^* + \\delta) \\approx f(x^*) + f'(x^*) \\cdot \\delta\\).\nCreate a data.frame with columns x, y_true, y_approx.\nUse ggplot2 to plot the function and its linear approximation.\n\n\nQuestion 3: Look ma, I made my own autograd function\nIn this exercise, we will build our own, custom autograd function. While you might rarely need this in practice, it still allows you to get a better understanding of how the autograd system works. There is also a tutorial on this on the torch website.\nTo construct our own autograd function, we need to define:\n\nThe forward pass:\n\nHow to calculate outputs from input\nWhat to save for the backward pass\n\nThe backward pass:\n\nHow to calculate the gradient of the output with respect to the input\n\n\nThe task is to re-create the ReLU activation function, which is a common activation function in neural networks and which is defined as:\n\\[\n\\text{ReLU}(x) = \\max(0, x)\n\\]\nNote that strictly speaking, the ReLU function is not differentiable at \\(x = 0\\) (but a subgradient can be used instead). The derivative/subgradient of the ReLU function is:\n\\[\n\\text{ReLU}'(x) = \\begin{cases}\n1 & \\text{if } x &gt; 0 \\\\\n0 & \\text{if } x \\leq 0 \\\\\n\\end{cases}\n\\]\nIn torch, a custom autograd function can be constructed using autograd_function() and it accepts arguments forward and backward which are functions that define the forward and backward pass: They both take as first argument a ctx, which is a communication object that is used to save information during the forward pass to be able to compute the gradient in the backward pass (e.g. for \\(f(x) = x * a\\), to calculate the gradient of \\(f\\) with respect to \\(a\\) we need to know the input value \\(x\\)). The return value of the backward pass should be a list of gradients with respect to the inputs. To check whether a gradient for an input is needed (has requires_grad = TRUE), you can use ctx$needs_input_grad which is a named list with boolean values for each input.\nThe backward function additionally takes a second argument grad_output, which is the gradient of the output: E.g., if our function is \\(f(x)\\) and we calculate the gradient of \\(g(x) = h(f(x))\\), then grad_output is the derivative of \\(g\\) with respect to its input, evaluated at \\(f(x)\\). This is essentially the chain rule: \\(\\frac{\\partial g}{\\partial x} = \\frac{\\partial h}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x}\\).\nFill out the missing parts (...) in the code below.\n\nrelu &lt;- autograd_function(\n  forward = function(ctx, input) {\n    mask &lt;- ...\n    output &lt;- torch_where(mask, ...)\n    ctx$save_for_backward(mask)\n    output\n  },\n  backward = function(ctx, grad_output) {\n    grads &lt;- list(input = NULL)\n    if (ctx$needs_input_grad$input) {\n      mask &lt;- ctx$saved_variables[[1]]\n      grads$input &lt;- ...\n    }\n    grads\n  }\n)\n\nTo check that it’s working, use the code below (with your relu instead of nnf_relu) and check that the results are the same.\n\nx &lt;- torch_tensor(-1, requires_grad = TRUE)\n(nnf_relu(x)^2)$backward()\nx$grad\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\nx$grad$zero_()\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\nx &lt;- torch_tensor(3, requires_grad = TRUE)\n(nnf_relu(x)^2)$backward()\nx$grad\n\ntorch_tensor\n 6\n[ CPUFloatType{1} ]"
  },
  {
    "objectID": "notebooks/2-autograd-exercise-solution.html",
    "href": "notebooks/2-autograd-exercise-solution.html",
    "title": "Autograd",
    "section": "",
    "text": "Question 1: Appreciating autograd\nConsider the following function:\n\\[\nf(x) = x^2 + 3x + 2\n\\]\nAs well as the function \\(g(x) = f(f(f(x)))\\)\nCalculate the gradient of both functions at point \\(x = 2\\).\nSolution\n\nlibrary(torch)\nx &lt;- torch_tensor(2, requires_grad = TRUE)\nf &lt;- function(x) {\n  x^2 + 3 * x + 2\n}\ng &lt;- function(x) {\n  f(f(f(x)))\n}\n\n# Calculate f'(2)\nf(x)$backward()\ngrad &lt;- x$grad$clone()\ngrad\n\ntorch_tensor\n 7\n[ CPUFloatType{1} ]\n\n# For another backward pass, we reset the gradient as they otherwise accumulate\nx$grad$zero_()\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\n# Calculate the gradient of g(x)\ng(x)$backward()\n\n# Create a copy of the gradient\ngrad2 &lt;- x$grad$clone()\ngrad2\n\ntorch_tensor\n 69363\n[ CPUFloatType{1} ]\n\n# Zero gradients for good measure\nx$grad$zero_()\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\n\nQuestion 2: Approximating functions with gradients\nThe defining feature of the gradient is that it allows us to approximate the function locally by a linear function.\nI.e., for some value \\(x^*\\), we know for very small \\(\\delta\\), that\n\\[\nf(x^* + \\delta) \\approx f(x^*) + f'(x^*) \\cdot \\delta\n\\]\nPlot the function from earlier as well as the local linear approximation at \\(x = 2\\) using ggplot2.\n\n\nHint\n\nTo do so, follow these steps:\n\nCreate a sequence with 100 equidistant values between -4 to 4 using torch_linspace().\nCreate the true function values at these points using the function from exercise 1.\nApproximate the function using the formula \\(f(x^* + \\delta) \\approx f(x^*) + f'(x^*) \\cdot \\delta\\).\nCreate a data.frame with columns x, y_true, y_approx.\nUse ggplot2 to plot the function and its linear approximation.\n\n\nSolution\n\nlibrary(ggplot2)\nx &lt;- x$detach() # No need to track gradients anymore\ndeltas &lt;- torch_linspace(-4, 4, 100)\ny_true &lt;- f(x + deltas)\ny_approx &lt;- f(x) + grad * deltas\n\nd &lt;- data.frame(x = as_array(x + deltas), y_true = as_array(y_true), y_approx = as_array(y_approx))\n\nggplot(d, aes(x = x)) +\n  geom_line(aes(y = y_true, color = \"True function\")) +\n  geom_line(aes(y = y_approx, color = \"Linear approximation\")) +\n  theme_minimal() +\n  labs(\n    title = \"Gradient as a local linear approximation\",\n    y = \"f(x)\",\n    x = \"x\",\n    colour = \"\"\n  )\n\n\n\n\n\n\n\n\nQuestion 3: Look ma, I made my own autograd function\nIn this exercise, we will build our own, custom autograd function. While you might rarely need this in practice, it still allows you to get a better understanding of how the autograd system works. There is also a tutorial on this on the torch website.\nTo construct our own autograd function, we need to define:\n\nThe forward pass:\n\nHow to calculate outputs from inputs\nWhat to save for the backward pass\n\nThe backward pass:\n\nHow to calculate the gradient of the output with respect to the inputs, using the information saved during the forward pass\n\n\nThe task is to re-create the ReLU activation function, which is a common activation function in neural networks and which is defined as:\n\\[\n\\text{ReLU}(x) = \\max(0, x)\n\\]\nNote that strictly speaking, the ReLU function is not differentiable at \\(x = 0\\) (but a subgradient can be used instead). The derivative/subgradient of the ReLU function is:\n\\[\n\\text{ReLU}'(x) = \\begin{cases}\n1 & \\text{if } x &gt; 0 \\\\\n0 & \\text{if } x \\leq 0 \\\\\n\\end{cases}\n\\]\nIn torch, a custom autograd function can be constructed using autograd_function() and it accepts arguments forward and backward which are functions that define the forward and backward pass: They both take as first argument a ctx, which is a communication object that is used to save information during the forward pass to be able to compute the gradient in the backward pass (e.g. for \\(f(x) = x * a\\), to calculate the gradient of \\(f\\) with respect to \\(a\\) we need to know the input value \\(x\\)). The return value of the backward pass should be a list of gradients of the final node of the autograd graph with respect to the inputs. To check whether a gradient for an input is needed (has requires_grad = TRUE), you can use ctx$needs_input_grad which is a named list with boolean values for each input.\nThe backward function additionally takes a second argument grad_output, which is the gradient of the output: E.g., if our function is \\(f(x)\\) and we calculate the gradient of \\(g(x) = h(f(x))\\), then grad_output is the derivative of \\(h\\) with respect to its input, evaluated at \\(f(x)\\). This is essentially the chain rule: \\(\\frac{\\partial g}{\\partial x} = \\frac{\\partial h}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x}\\). The $backward() method of the autograd function $fe would in this case therefore not return \\(\\frac{\\partial f}{\\partial x}\\), but \\(\\frac{\\partial g}{\\partial x}\\).\nFill out the missing parts (...) in the code below.\n\nrelu &lt;- autograd_function(\n  forward = function(ctx, input) {\n    mask &lt;- ...\n    output &lt;- torch_where(mask, ...)\n    ctx$save_for_backward(mask)\n    output\n  },\n  backward = function(ctx, grad_output) {\n    grads &lt;- list(input = NULL)\n    if (ctx$needs_input_grad$input) {\n      mask &lt;- ctx$saved_variables[[1]]\n      grads$input &lt;- ...\n    }\n    grads\n  }\n)\n\nTo check that it’s working, use the code below (with your relu instead of nnf_relu) and check that the results are the same.\n\nx &lt;- torch_tensor(-1, requires_grad = TRUE)\n(nnf_relu(x)^2)$backward()\nx$grad\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\nx$grad$zero_()\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\nx &lt;- torch_tensor(3, requires_grad = TRUE)\n(nnf_relu(x)^2)$backward()\nx$grad\n\ntorch_tensor\n 6\n[ CPUFloatType{1} ]\n\n\nSolution\n\nrelu &lt;- autograd_function(\n  forward = function(ctx, input) {\n    mask &lt;- input &gt; 0\n    output &lt;- torch_where(mask, input, torch_tensor(0))\n    ctx$save_for_backward(mask)\n    output\n  },\n  backward = function(ctx, grad_output) {\n    grads &lt;- list(input = NULL)\n    if (ctx$needs_input_grad$input) {\n      mask &lt;- ctx$saved_variables[[1]]\n      grads$input &lt;- grad_output * mask\n    }\n    grads\n  }\n)\n\nx &lt;- torch_tensor(-1, requires_grad = TRUE)\n(relu(x)^2)$backward()\nx$grad\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\nx$grad$zero_()\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\nx &lt;- torch_tensor(3, requires_grad = TRUE)\n(relu(x)^2)$backward()\nx$grad\n\ntorch_tensor\n 6\n[ CPUFloatType{1} ]"
  },
  {
    "objectID": "notebooks/1-tensor-exercise.html",
    "href": "notebooks/1-tensor-exercise.html",
    "title": "Tensors",
    "section": "",
    "text": "Note\n\n\n\nTo solve these exercises, consulting the torch function reference can be helpful.\n\n\nQuestion 1: Tensor creation and manipulation\nRecreate this torch tensor:\n\n\ntorch_tensor\n 1  2  3\n 4  5  6\n[ CPULongType{2,3} ]\n\n\n\n\nHint\n\nFirst create an R matrix and then convert it using torch_tensor().\n\nNext, create a view of the tensor so it looks like this:\n\n\ntorch_tensor\n 1  2\n 3  4\n 5  6\n[ CPULongType{3,2} ]\n\n\n\n\nHint\n\nUse the $view() method and pass the desired shape as a vector.\n\nCheck programmatically that you successfully created a view, and not a copy.\n\n\nHint\n\nSee what happens when you modify one of the tensors.\n\nQuestion 2: More complex reshaping\nConsider the following tensor:\n\nx &lt;- torch_tensor(1:6)\nx\n\ntorch_tensor\n 1\n 2\n 3\n 4\n 5\n 6\n[ CPULongType{6} ]\n\n\nReshape it so it looks like this.\n\n\ntorch_tensor\n 1  3  5\n 2  4  6\n[ CPULongType{2,3} ]\n\n\n\n\nHint\n\nFirst reshape to (2, 3) and then $permute() the two dimensions.\n\nQuestion 3: Broadcasting\nConsider the following vectors:\n\nx1 &lt;- torch_tensor(c(1, 2))\nx1\n\ntorch_tensor\n 1\n 2\n[ CPUFloatType{2} ]\n\nx2 &lt;- torch_tensor(c(3, 7))\nx2\n\ntorch_tensor\n 3\n 7\n[ CPUFloatType{2} ]\n\n\nPredict the result (shape and values) of the following operation by applying the broadcasting rules.\n\nx1 + x2$reshape(c(2, 1))\n\nQuestion 4: Handling Singleton dimensions\nA common operation in deep learning is to add or get rid of singleton dimensions, i.e., dimensions of size 1. As this is so common, torch offers a $squeeze() and $unsqueeze() method to add and remove singleton dimensions.\nUse these two functions to first remove the second dimension and then add one in the first position.\n\nx &lt;- torch_randn(2, 1)\nx\n\ntorch_tensor\n-0.1115\n 0.1204\n[ CPUFloatType{2,1} ]\n\n\nQuestion 5: Matrix multiplication\nGenerate a random matrix \\(A\\) of shape (10, 5) and a random matrix \\(B\\) of shape (10, 5) by sampling from a standard normal distribution.\n\n\nHint\n\nUse torch_randn(nrow, ncol) to generate random matrices.\n\nCan you multiply these two matrices with each other and if so, in which order? If not, generate two random matrices with compatible shapes and multiply them.\nQuestion 6: Uniform sampling\nGenerate 10 random variables from a uniform distribution (using only torch functions) in the interval \\([10, 20]\\). Use torch_rand() for this (which does not allow for min and max parameters).\n\n\nHint\n\nAdd the lower bound and multiply with the width of the interval.\n\nThen, calculate the mean of the values that are larger than 15.\nQuestion 7: Don’t touch this\nConsider the code below:\n\nf &lt;- function(x) {\n  x[1] &lt;- torch_tensor(-99)\n  return(x)\n}\nx &lt;- torch_tensor(1:3)\ny &lt;- f(x)\nx\n\ntorch_tensor\n-99\n  2\n  3\n[ CPULongType{3} ]\n\n\nImplement a new different version of this function that returns the same tensor but does not change the value of the input tensor in-place.\n\n\nHint\n\nThe $clone() method might be helpful."
  },
  {
    "objectID": "notebooks/1-tensor-exercise-solution.html",
    "href": "notebooks/1-tensor-exercise-solution.html",
    "title": "Tensors",
    "section": "",
    "text": "Note\n\n\n\nTo solve these exercises, consulting the torch function reference can be helpful.\n\n\nQuestion 1: Tensor creation and manipulation\nRecreate this torch tensor:\n\n\ntorch_tensor\n 1  2  3\n 4  5  6\n[ CPULongType{2,3} ]\n\n\n\n\nHint\n\nFirst create an R matrix and then convert it using torch_tensor().\n\nNext, create a view of the tensor so it looks like this:\n\n\ntorch_tensor\n 1  2\n 3  4\n 5  6\n[ CPULongType{3,2} ]\n\n\n\n\nHint\n\nUse the $view() method and pass the desired shape as a vector.\n\nCheck programmatically that you successfully created a view, and not a copy.\n\n\nHint\n\nSee what happens when you modify one of the tensors.\n\nSolution\nWe start by creating the tensor:\n\nx &lt;- torch_tensor(matrix(1:6, byrow = TRUE, nrow = 2))\nx\n\ntorch_tensor\n 1  2  3\n 4  5  6\n[ CPULongType{2,3} ]\n\n\nThen, we create a view of the tensor:\n\ny &lt;- x$view(c(3, 2))\n\nTo check that we created a view, we can modify one of the tensors and see if the other one changes:\n\nx[1, 1] &lt;- 100\ny\n\ntorch_tensor\n 100    2\n   3    4\n   5    6\n[ CPULongType{3,2} ]\n\n\nQuestion 2: More complex reshaping\nConsider the following tensor:\n\nx &lt;- torch_tensor(1:6)\nx\n\ntorch_tensor\n 1\n 2\n 3\n 4\n 5\n 6\n[ CPULongType{6} ]\n\n\nReshape it so it looks like this.\n\n\ntorch_tensor\n 1  3  5\n 2  4  6\n[ CPULongType{2,3} ]\n\n\n\n\nHint\n\nFirst reshape to (2, 3) and then $permute() the two dimensions.\n\nSolution We therefore first reshape to (3, 2) and then permute the two dimensions to get the desired shape (2, 3).\n\nx &lt;- x$reshape(c(3, 2))\nx\n\ntorch_tensor\n 1  2\n 3  4\n 5  6\n[ CPULongType{3,2} ]\n\nx$permute(c(2, 1))\n\ntorch_tensor\n 1  3  5\n 2  4  6\n[ CPULongType{2,3} ]\n\n\nQuestion 3: Broadcasting\nConsider the following vectors:\n\nx1 &lt;- torch_tensor(c(1, 2))\nx1\n\ntorch_tensor\n 1\n 2\n[ CPUFloatType{2} ]\n\nx2 &lt;- torch_tensor(c(3, 7))\nx2\n\ntorch_tensor\n 3\n 7\n[ CPUFloatType{2} ]\n\n\nPredict the result (shape and values) of the following operation by applying the broadcasting rules.\n\nx1 + x2$reshape(c(2, 1))\n\nSolution\nThe result is the following tensor:\n\n\ntorch_tensor\n 4  5\n 8  9\n[ CPUFloatType{2,2} ]\n\n\nWe will now show how to arrive at this step by step. According to the broadcasting rules, we start by adding a singleton dimension to the first tensor:\n\nx1 &lt;- x1$reshape(c(1, 2))\n\nNow, we have a tensor of shape (1, 2) and a tensor of shape (2, 1). Next, we extend the first tensor along the first dimension to match the second tensor:\n\nx1 &lt;- x1$expand(c(2, 2))\n\nWe do this analogously for the second (reshaped) tensor:\n\nx2 &lt;- x2$reshape(c(2, 1))$expand(c(2, 2))\n\nNow they both have the same shape (2, 2), so we can add them:\n\nx1 + x2\n\ntorch_tensor\n 4  5\n 8  9\n[ CPUFloatType{2,2} ]\n\n\nQuestion 4: Handling Singleton dimensions\nA common operation in deep learning is to add or get rid of singleton dimensions, i.e., dimensions of size 1. As this is so common, torch offers a $squeeze() and $unsqueeze() method to add and remove singleton dimensions.\nUse these two functions to first remove the second dimension and then add one in the first position.\n\nx &lt;- torch_randn(2, 1)\nx\n\ntorch_tensor\n-0.1115\n 0.1204\n[ CPUFloatType{2,1} ]\n\n\nSolution\n\nx$squeeze(2)$unsqueeze(1)\n\ntorch_tensor\n-0.1115  0.1204\n[ CPUFloatType{1,2} ]\n\n\nQuestion 5: Matrix multiplication\nGenerate a random matrix \\(A\\) of shape (10, 5) and a random matrix \\(B\\) of shape (10, 5) by sampling from a standard normal distribution.\n\n\nHint\n\nUse torch_randn(nrow, ncol) to generate random matrices.\n\nCan you multiply these two matrices with each other and if so, in which order? If not, generate two random matrices with compatible shapes and multiply them.\nSolution\nWe can only multiply a matrix of shape (n, k) with a matrix of shape (k, m), i.e., the the number of columns in the first matrix matches the number of rows in the second matrix.\nWe can therefore not multiply the two matrices with each other in either order. To generate two random matrices with compatible shapes, we can generate two random matrices with shape (10, 5) and (5, 10).\n\nA &lt;- torch_randn(10, 5)\nB &lt;- torch_randn(5, 10)\nA$matmul(B)\n\ntorch_tensor\n-1.4311  0.6090 -1.4795 -0.6977  2.4857 -0.7402  0.4060 -0.4299  2.9035  0.1459\n-4.0841  3.8794 -1.5376 -3.5270  4.8175 -0.7630  0.1188  3.0368  1.0634  0.0011\n-0.3880 -1.4639 -1.3191 -0.0589  3.1754 -3.1779  1.7006  0.0521  5.0765  0.0552\n 1.6030 -2.2295  1.1606  3.3083  3.3677  1.5567 -2.3565 -5.1759 -1.9122  5.1734\n 4.0126 -4.3978  0.5547  1.9958 -3.4347 -2.2880  2.1990  0.2017  2.6702 -1.7145\n 0.8548  3.0118 -2.0971 -3.3564 -8.1899  3.3494  1.5969  4.4134  0.4593 -6.8904\n 0.0597 -0.1650 -2.5737 -1.1190  6.1582 -0.6400  0.8576  0.2152  5.0070  1.6070\n 0.2675  2.4575 -2.6582 -3.1801 -3.0074  2.0887  1.4936  3.5447  2.3877 -4.3110\n-3.7894  1.8938  0.0528 -0.9525  0.3706 -1.8813  0.0365  0.2768  0.2025 -0.8839\n 2.7060 -2.1856  1.0679  2.6758 -6.8991  1.6866 -0.2875 -2.8479 -1.4630 -1.6319\n[ CPUFloatType{10,10} ]\n\n\nQuestion 6: Uniform sampling\nGenerate 10 random variables from a uniform distribution (using only torch functions) in the interval \\([10, 20]\\). Use torch_rand() for this (which does not allow for min and max parameters).\n\n\nHint\n\nAdd the lower bound and multiply with the width of the interval.\n\nThen, calculate the mean of the values that are larger than 15.\nSolution Because the uniform distribution of torch has no min and max parameters like runif(), we instead sample from a standard uniform distribution and then scale and shift it to the desired interval.\n\nn &lt;- 10\na &lt;- 10\nb &lt;- 20\nx &lt;- torch_rand(n) * (b - a) + a\nx\n\ntorch_tensor\n 17.2108\n 15.4495\n 15.4898\n 13.4831\n 15.0240\n 13.4448\n 16.4367\n 19.8558\n 15.7574\n 12.7854\n[ CPUFloatType{10} ]\n\nmean(x[x &gt; 15])\n\ntorch_tensor\n16.4606\n[ CPUFloatType{} ]\n\n\nQuestion 7: Don’t touch this\nConsider the code below:\n\nf &lt;- function(x) {\n  x[1] &lt;- torch_tensor(-99)\n  return(x)\n}\nx &lt;- torch_tensor(1:3)\ny &lt;- f(x)\nx\n\ntorch_tensor\n-99\n  2\n  3\n[ CPULongType{3} ]\n\n\nImplement a new different version of this function that returns the same tensor but does not change the value of the input tensor in-place.\n\n\nHint\n\nThe $clone() method might be helpful.\n\nSolution\nWe need to $clone() the tensor before we modify it.\n\ng &lt;- function(x) {\n  x &lt;- x$clone()\n  x[1] &lt;- torch_tensor(-99)\n  x\n}\nx &lt;- torch_tensor(1:3)\ny &lt;- g(x)\nx\n\ntorch_tensor\n 1\n 2\n 3\n[ CPULongType{3} ]"
  },
  {
    "objectID": "notebooks/0-exercise-intro.html",
    "href": "notebooks/0-exercise-intro.html",
    "title": "Exercise Intro",
    "section": "",
    "text": "Tutorial and Exercise\nMost of the exercises are similar to the examples from the tutorial.\nAI Autocompletion\nIf you have AI autocompletion, we recommend disabling it, as GPT-4 already knows the torch API.\nHints\nHints give additional information that can help you solve the exercise. Don’t hesitate to open them when you are stuck.\nCPU vs. CUDA\nAll exercises can be solved with a CPU only.\nUncertainty\nTo make the exercises run fast, we are often using datasets with few observations and resamplings without too many iterations. This means that there is often uncertainty in the results that we mostly ignore, but which would have to be taken account in a proper analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning with (mlr3)torch in R 🔥",
    "section": "",
    "text": "This is a course, containing seven tutorials and corresponding exercises with solutions on torch and mlr3torch.\nThe seven topics are:\n\nTorch Tensors\nAutograd\nModules and Data\nOptimization & Regularization\nConvolutional Neural Networks\nIntro to mlr3torch (and mlr3 recap)\nTraining Efficiency\n\n\n\n\nAfter editing the content, e.g. in the notebooks folder, run quarto render to render the website. This will render the website into the docs/ folder. Upon pushing the changes to GitHub, the content of the docs/ folder is automatically deployed to GitHub Pages.\n\n\n\nSome of the content is based on the book Deep Learning and Scientific Computing with R torch by Sigrid Keydana. This course has been funded by Essential Data Science Training."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Deep Learning with (mlr3)torch in R 🔥",
    "section": "",
    "text": "This is a course, containing seven tutorials and corresponding exercises with solutions on torch and mlr3torch.\nThe seven topics are:\n\nTorch Tensors\nAutograd\nModules and Data\nOptimization & Regularization\nConvolutional Neural Networks\nIntro to mlr3torch (and mlr3 recap)\nTraining Efficiency"
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Deep Learning with (mlr3)torch in R 🔥",
    "section": "",
    "text": "After editing the content, e.g. in the notebooks folder, run quarto render to render the website. This will render the website into the docs/ folder. Upon pushing the changes to GitHub, the content of the docs/ folder is automatically deployed to GitHub Pages."
  },
  {
    "objectID": "index.html#credit",
    "href": "index.html#credit",
    "title": "Deep Learning with (mlr3)torch in R 🔥",
    "section": "",
    "text": "Some of the content is based on the book Deep Learning and Scientific Computing with R torch by Sigrid Keydana. This course has been funded by Essential Data Science Training."
  },
  {
    "objectID": "notebooks/1-tensor-exercise-task.html",
    "href": "notebooks/1-tensor-exercise-task.html",
    "title": "Tensors",
    "section": "",
    "text": "Note\n\n\n\nTo solve these exercises, consulting the torch function reference can be helpful.\n\n\nQuestion 1: Tensor creation and manipulation\nRecreate this torch tensor:\n\n\ntorch_tensor\n 1  2  3\n 4  5  6\n[ CPULongType{2,3} ]\n\n\n\n\nHint\n\nFirst create an R matrix and then convert it using torch_tensor().\n\nNext, create a view of the tensor so it looks like this:\n\n\ntorch_tensor\n 1  2\n 3  4\n 5  6\n[ CPULongType{3,2} ]\n\n\n\n\nHint\n\nUse the $view() method and pass the desired shape as a vector.\n\nCheck programmatically that you successfully created a view, and not a copy.\n\n\nHint\n\nSee what happens when you modify one of the tensors.\n\nQuestion 2: More complex reshaping\nConsider the following tensor:\n\nx &lt;- torch_tensor(1:6)\nx\n\ntorch_tensor\n 1\n 2\n 3\n 4\n 5\n 6\n[ CPULongType{6} ]\n\n\nReshape it so it looks like this.\n\n\ntorch_tensor\n 1  3  5\n 2  4  6\n[ CPULongType{2,3} ]\n\n\n\n\nHint\n\nFirst reshape to (2, 3) and then $permute() the two dimensions.\n\nQuestion 3: Broadcasting\nConsider the following vectors:\n\nx1 &lt;- torch_tensor(c(1, 2))\nx1\n\ntorch_tensor\n 1\n 2\n[ CPUFloatType{2} ]\n\nx2 &lt;- torch_tensor(c(3, 7))\nx2\n\ntorch_tensor\n 3\n 7\n[ CPUFloatType{2} ]\n\n\nPredict the result (shape and values) of the following operation by applying the broadcasting rules.\n\nx1 + x2$reshape(c(2, 1))\n\nQuestion 4: Handling Singleton dimensions\nA common operation in deep learning is to add or get rid of singleton dimensions, i.e., dimensions of size 1. As this is so common, torch offers a $squeeze() and $unsqueeze() method to add and remove singleton dimensions.\nUse these two functions to first remove the second dimension and then add one in the first position.\n\nx &lt;- torch_randn(2, 1)\nx\n\ntorch_tensor\n-0.1115\n 0.1204\n[ CPUFloatType{2,1} ]\n\n\nQuestion 5: Matrix multiplication\nGenerate a random matrix \\(A\\) of shape (10, 5) and a random matrix \\(B\\) of shape (10, 5) by sampling from a standard normal distribution.\n\n\nHint\n\nUse torch_randn(nrow, ncol) to generate random matrices.\n\nCan you multiply these two matrices with each other and if so, in which order? If not, generate two random matrices with compatible shapes and multiply them.\nQuestion 6: Uniform sampling\nGenerate 10 random variables from a uniform distribution (using only torch functions) in the interval \\([10, 20]\\). Use torch_rand() for this (which does not allow for min and max parameters).\n\n\nHint\n\nAdd the lower bound and multiply with the width of the interval.\n\nThen, calculate the mean of the values that are larger than 15.\nQuestion 7: Don’t touch this\nConsider the code below:\n\nf &lt;- function(x) {\n  x[1] &lt;- torch_tensor(-99)\n  return(x)\n}\nx &lt;- torch_tensor(1:3)\ny &lt;- f(x)\nx\n\ntorch_tensor\n-99\n  2\n  3\n[ CPULongType{3} ]\n\n\nImplement a new different version of this function that returns the same tensor but does not change the value of the input tensor in-place.\n\n\nHint\n\nThe $clone() method might be helpful."
  },
  {
    "objectID": "notebooks/1-tensor.html",
    "href": "notebooks/1-tensor.html",
    "title": "Torch Tensors",
    "section": "",
    "text": "Tensors are the fundamental data structure in torch, serving as the backbone for both deep learning and scientific computing operations. While similar to R arrays, tensors offer enhanced capabilities that make them particularly suited for modern computational tasks, namely GPU acceleration and automatic differentiation (autograd)."
  },
  {
    "objectID": "notebooks/1-tensor.html#creating-tensors",
    "href": "notebooks/1-tensor.html#creating-tensors",
    "title": "Torch Tensors",
    "section": "Creating Tensors",
    "text": "Creating Tensors\nOne way to create tensors is to convert R matrices (or analogously arrays or vectors) to torch tensors using torch_tensor():\n\nlibrary(torch)\n# From R matrices\nx_matrix &lt;- matrix(1:6, nrow = 2, ncol = 3)\nx_matrix\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nx_tensor &lt;- torch_tensor(x_matrix)\nx_tensor\n\ntorch_tensor\n 1  3  5\n 2  4  6\n[ CPULongType{2,3} ]\n\n\nFor specific types of tensors, there are also dedicated functions:\n\nzeros_tensor &lt;- torch_zeros(2, 3)\nzeros_tensor\n\ntorch_tensor\n 0  0  0\n 0  0  0\n[ CPUFloatType{2,3} ]\n\nones_tensor &lt;- torch_ones(2, 3)\nones_tensor\n\ntorch_tensor\n 1  1  1\n 1  1  1\n[ CPUFloatType{2,3} ]\n\nlike_tensor &lt;- torch_zeros_like(ones_tensor)\nlike_tensor\n\ntorch_tensor\n 0  0  0\n 0  0  0\n[ CPUFloatType{2,3} ]\n\n\n\nRandom Sampling\nYou can also randomly sample torch tensors:\n\nnormal_tensor &lt;- torch_randn(2, 3)    # Samples from N(0,1)\nuniform_tensor &lt;- torch_rand(2, 3)    # Samples from U(0,1)\n\n\n\n\n\n\n\nRandom Seeds in torch\n\n\n\ntorch maintains its own random number generator, separate from R’s. Setting R’s random seed with set.seed() does not affect torch’s random operations. Instead, use torch_manual_seed() to control the reproducibility of torch operations.\n\n\n\n\nMissing Values\n\n\n\n\n\n\nQuiz: NaN vs NA\n\n\n\nQuestion: What is the difference between NaN and NA in R?\n\n\nClick for answer\n\nNaN is a floating-point value that represents an undefined or unrepresentable value (such as 0 / 0).\nNA is a missing value indicator used in vectors, matrices, and data frames to represent unknown or missing data.\n\n\n\nTorch tensors do not have a corresponding representation for R’s NA values. When converting R vectors containing NAs to torch tensors, you need to be cautious:\n\nDouble: NA_real_ becomes NaN\n\ntorch_tensor(NA_real_)\n\ntorch_tensor\nnan\n[ CPUFloatType{1} ]\n\n\nInteger: NA_integer_ becomes the smallest negative value:\n\ntorch_tensor(NA_integer_)\n\ntorch_tensor\n-2.1475e+09\n[ CPULongType{1} ]\n\n\nLogical: NA becomes TRUE:\n\ntorch_tensor(NA)\n\ntorch_tensor\n 1\n[ CPUBoolType{1} ]\n\n\n\nYou should handle missing values carefully before converting them to torch tensors.\n\n\n\n\n\n\nQuiz: Conversion\n\n\n\nQuestion: Can you guess why the behavior is as it is?\n\n\nClick for answer\n\nWhen converting an R array to a torch tensors, the underlying data is simply copied over. Because R uses special values for NAs that are not standardized by the industry, torch will interprete them differently. E.g. in R, NA integers are internally represented as the smallest negative value.\nIn principle, torch could scan R objects for these values, but this would make conversion slower."
  },
  {
    "objectID": "notebooks/1-tensor.html#tensor-properties",
    "href": "notebooks/1-tensor.html#tensor-properties",
    "title": "Torch Tensors",
    "section": "Tensor Properties",
    "text": "Tensor Properties\n\nShape\nLike R arrays, each tensor has a shape and a dimension:\n\nprint(x_tensor$shape)\n\n[1] 2 3\n\nprint(x_tensor$dim())\n\n[1] 2\n\n\n\n\nData Type\nFurthermore, each tensor has a datatype. Unlike base R, where typically there is one integer type (32 bits) and one floating-point type (double, 64 bits), torch differentiates between different precisions:\n\nFloating point:\n\nfloat32_tensor &lt;- torch_ones(2, 3, dtype = torch_float32())  # Default float\nfloat64_tensor &lt;- torch_ones(2, 3, dtype = torch_float64())  # Double precision\nfloat16_tensor &lt;- torch_ones(2, 3, dtype = torch_float16())  # Half precision\n\nUsually, you work with 32-bit floats.\nInteger:\n\nint32_tensor &lt;- torch_ones(2, 3, dtype = torch_int32())\nint64_tensor &lt;- torch_ones(2, 3, dtype = torch_int64())  # Long\nint16_tensor &lt;- torch_ones(2, 3, dtype = torch_int16())  # Short\nint8_tensor  &lt;- torch_ones(2, 3, dtype = torch_int8())    # Byte\nuint8_tensor &lt;- torch_ones(2, 3, dtype = torch_uint8())  # Unsigned byte\n\nBoolean:\n\nbool_tensor &lt;- torch_ones(2, 3, dtype = torch_bool())\n\n\nYou can convert between datatypes using the $to() method:\n\n# Converting between datatypes\nx &lt;- torch_ones(2, 3)  # Default float32\nx_int &lt;- x$to(dtype = torch_int32())\n\nNote that floats are converted to integers by truncating, not by rounding.\n\ntorch_tensor(2.999)$to(dtype = torch_int())\n\ntorch_tensor\n 2\n[ CPUIntType{1} ]\n\ntorch_tensor(-2.999)$to(dtype = torch_int())\n\ntorch_tensor\n-2\n[ CPUIntType{1} ]\n\n\n\n\n\n\n\n\nQuiz: Data Types\n\n\n\nQuestion: What is the advantage of 64-bit floats over 32-bit floats, what the disadvantage?\n\n\nClick for answer\n\n64-bit floats are more precise, but also require more memory. Furhermore, operations on 64-bit floats are slower than on 32-bit floats. One way to increase tensor operations is to use lower precision, a process called quantization.\n\n\n\n\n\nDevice\nEach tensor lives on a “device”, where common options are:\n\ncpu for CPU, which is available everywhere\ncuda for NVIDIA GPUs\nmps for Apple Silicon (M1/M2/M3) GPUs on macOS\n\n\n# Create a tensor and move it to CUDA if available\nx &lt;- torch_randn(2, 3)\nif (cuda_is_available()) {\n  x &lt;- x$to(device = torch_device(\"cuda\"))\n  # x &lt;- x$cuda() also works\n} else {\n  print(\"CUDA not available; tensor remains on CPU\")\n}\n\n[1] \"CUDA not available; tensor remains on CPU\"\n\nprint(x$device)\n\ntorch_device(type='cpu') \n\nx &lt;- x$to(device = \"cpu\")\n# x &lt;- x$cpu() also works\nprint(x$device)\n\ntorch_device(type='cpu') \n\n\nGPU acceleration enables massive parallelization of tensor operations, often providing 10-100x speedups compared to CPU processing for large-scale computations.\n\n\n\n\n\n\nDevice Compatibility\n\n\n\nTensors must reside on the same device to perform operations between them."
  },
  {
    "objectID": "notebooks/1-tensor.html#converting-tensors-back-to-r",
    "href": "notebooks/1-tensor.html#converting-tensors-back-to-r",
    "title": "Torch Tensors",
    "section": "Converting Tensors Back to R",
    "text": "Converting Tensors Back to R\nYou can easily convert torch tensors back to R using as_array(), as.matrix(), or $item():\n\n0-dimensional tensors (scalars) are converted to R vectors with length 1:\n\n{.3 .cell-code} torch_scalar_tensor(1)$item() # as_array() also works\n\n1-dimensional tensors are converted to R vectors:\n\nas_array(torch_randn(3))\n\n[1] -1.4168206  0.8429176 -0.6306752\n\n\n\\(&gt;1\\)-dimensional tensors are converted to R arrays:\n\nas_array(torch_randn(2, 2))\n\n          [,1]       [,2]\n[1,] 1.2340047  0.3126765\n[2,] 0.6971866 -0.9950489"
  },
  {
    "objectID": "notebooks/1-tensor.html#basic-tensor-operations",
    "href": "notebooks/1-tensor.html#basic-tensor-operations",
    "title": "Torch Tensors",
    "section": "Basic Tensor Operations",
    "text": "Basic Tensor Operations\nTorch provides two main syntaxes for tensor operations: function-style (torch_*()) and method-style (using $).\nHere’s an example with matrix multiplication:\n\na &lt;- torch_tensor(matrix(1:6, nrow=2, ncol=3))\na\n\ntorch_tensor\n 1  3  5\n 2  4  6\n[ CPULongType{2,3} ]\n\nb &lt;- torch_tensor(matrix(7:12, nrow=3, ncol=2))\nb\n\ntorch_tensor\n  7  10\n  8  11\n  9  12\n[ CPULongType{3,2} ]\n\n# Matrix multiplication - two equivalent ways\nc1 &lt;- torch_matmul(a, b)  # Function style\nc2 &lt;- a$matmul(b)         # Method style\n\ntorch_equal(c1, c2)\n\n[1] TRUE\n\n\nBelow, there is another example using addition:\n\n# Addition - two equivalent ways\nx &lt;- torch_ones(2, 2)\ny &lt;- torch_ones(2, 2)\nz1 &lt;- torch_add(x, y)  # Function style\nz2 &lt;- x$add(y)         # Method style\n\n\n\n\n\n\n\nIn-place Operations\n\n\n\nOperations that modify the tensor directly are marked with an underscore suffix (_). These operations are more memory efficient as they do not allocate a new tensor:\n\nx &lt;- torch_ones(2, 2)\nx\n\ntorch_tensor\n 1  1\n 1  1\n[ CPUFloatType{2,2} ]\n\nx$add_(1)  # Adds 1 to all elements in place\n\ntorch_tensor\n 2  2\n 2  2\n[ CPUFloatType{2,2} ]\n\nx\n\ntorch_tensor\n 2  2\n 2  2\n[ CPUFloatType{2,2} ]\n\n\n\n\nYou can also apply common summary functions to torch tensors:\n\nx = torch_randn(1000)\nmean(x)\n\ntorch_tensor\n0.0391642\n[ CPUFloatType{} ]\n\nmax(x)\n\ntorch_tensor\n3.31465\n[ CPUFloatType{} ]\n\nsd(x)\n\n[1] 0.9848639\n\n\nAccessing elements from a tensor is also similar to R arrays and matrices, i.e., it is 1-based.\n\nx &lt;- matrix(1:6, nrow = 3)\nx\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\nxt &lt;- torch_tensor(x)\nxt\n\ntorch_tensor\n 1  4\n 2  5\n 3  6\n[ CPULongType{3,2} ]\n\nx[1:2, 1]\n\n[1] 1 2\n\nxt[1:2, 1]\n\ntorch_tensor\n 1\n 2\n[ CPULongType{2} ]\n\n\nOne difference between indexing torch vectors and standard R vectors is the behavior regarding negative indices. While R vectors remove the element at the specified index, torch vectors return elements from the beginning.\n\n(1:3)[-1]\n\n[1] 2 3\n\ntorch_tensor(1:3)[-1]\n\ntorch_tensor\n3\n[ CPULongType{} ]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhile (R) torch is 1-based, PyTorch is 0-based. When translating PyTorch code to R, you need to be careful with this difference.\n\n\nAnother convenient feature in torch is the .. syntax for indexing:\n\narr &lt;- array(1:24, dim = c(4, 3, 2))\narr[1:2, , ] # works\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   13   17   21\n[2,]   14   18   22\n\narr[1:2, ]    # does not work\n\nError in arr[1:2, ]: incorrect number of dimensions\n\n\nIn torch, you can achieve the same result as follows:\n\ntensor &lt;- torch_tensor(arr)\ntensor[1:2, ..]\n\ntorch_tensor\n(1,.,.) = \n   1  13\n   5  17\n   9  21\n\n(2,.,.) = \n   2  14\n   6  18\n  10  22\n[ CPULongType{2,3,2} ]\n\n\nYou can also specify indices after the .. operator:\n\ntensor[.., 1]\n\ntorch_tensor\n  1   5   9\n  2   6  10\n  3   7  11\n  4   8  12\n[ CPULongType{4,3} ]\n\n\nNote that when you select a single element from a dimension, the dimension is removed:\n\ndim(tensor[.., 1])\n\n[1] 4 3\n\n\nJust like in R, you can prevent this behavior by setting drop = FALSE:\n\ndim(tensor[.., 1, drop = FALSE])\n\n[1] 4 3 1\n\n\nTensors also support indexing by boolean masks, which will result in a 1-dimensional tensor:\n\ntensor[tensor &gt; 15]\n\ntorch_tensor\n 17\n 21\n 18\n 22\n 19\n 23\n 16\n 20\n 24\n[ CPULongType{9} ]\n\n\nWe can also extract the first two rows and columns of the tensor from the first index of the third dimension:\n\ntensor[1:2, 1:2, 1]\n\ntorch_tensor\n 1  5\n 2  6\n[ CPULongType{2,2} ]"
  },
  {
    "objectID": "notebooks/1-tensor.html#broadcasting-rules",
    "href": "notebooks/1-tensor.html#broadcasting-rules",
    "title": "Torch Tensors",
    "section": "Broadcasting Rules",
    "text": "Broadcasting Rules\nAnother difference between R arrays and torch tensors is how operations on tensors with different shapes are handled. For example, in R, we cannot add a matrix with shape (1, 2) to a matrix with shape (2, 3):\n\nm1 &lt;- matrix(1:4, nrow = 2)\nm2 &lt;- matrix(1:2, nrow = 2)\nm1 + m2\n\nError in m1 + m2: non-conformable arrays\n\n\nBroadcasting (similar to “recycling” in R) allows torch to perform operations between tensors of different shapes.\n\nt1 &lt;- torch_tensor(m1)\nt2 &lt;- torch_tensor(m2)\nt1 + t2\n\ntorch_tensor\n 2  4\n 4  6\n[ CPULongType{2,2} ]\n\n\nThere are strict rules that define when two shapes are compatible:\n\nIf tensors have a different number of dimensions, prepend 1’s to the shape of the lower-dimensional tensor until they match.\nTwo dimensions are compatible when:\n\nThey are equal, or\nOne of them is 1 (which will be stretched to match the other)\n\nIf any dimension pair is incompatible, broadcasting fails.\n\n\n\n\n\n\n\nQuiz: Broadcasting Rules\n\n\n\nQuestion 1: Does broadcasting work to add a tensor of shape (2, 1, 3)to a tensor of shape (4, 3)? What would be the resulting shape?\n\n\nClick for answer\n\nThe resulting shape would be (2, 4, 3). Here’s why:\n\nPrepend one to the dimensions of the second tensor to get (1, 4, 3).\nGoing dimension by dimension:\n\nFirst: 2 vs 1 -&gt; Compatible, expand to 2\nSecond: 1 vs 4 -&gt; Compatible, expand to 4\nThird: 3 vs 3 -&gt; Compatible, remains 3\n\nAll pairs are compatible, so broadcasting succeeds.\n\n\nQuestion 2: Anser the same for tensors of shape (2, 3) and (3, 2)?\n\n\nClick for answer\n\nNo, broadcasting would fail in this case. Here’s why:\n\nBoth tensors have the same number of dimensions (2), so no prepending is needed.\nGoing dimension by dimension:\n\nFirst: 2 vs 3 -&gt; Incompatible (neither is 1)\nSecond: 3 vs 2 -&gt; Incompatible (neither is 1)\n\nSince both dimension pairs are incompatible, broadcasting fails."
  },
  {
    "objectID": "notebooks/1-tensor.html#reshaping-tensors",
    "href": "notebooks/1-tensor.html#reshaping-tensors",
    "title": "Torch Tensors",
    "section": "Reshaping Tensors",
    "text": "Reshaping Tensors\nTorch provides several ways to reshape tensors while preserving their data:\n\n# Create an example tensor\nx &lt;- torch_tensor(0:15)\nprint(x)\n\ntorch_tensor\n  0\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n[ CPULongType{16} ]\n\n\nWe can reshape this tensor with shape (16) to a tensor with shape (4, 4).\n\ny &lt;- x$reshape(c(4, 4))\ny\n\ntorch_tensor\n  0   1   2   3\n  4   5   6   7\n  8   9  10  11\n 12  13  14  15\n[ CPULongType{4,4} ]\n\n\nWhen x is reshaped to y, we can imagine it as initializing a new tensor of the desired shape and then filling up the rows and columns of the new tensor by iterating over the rows and columns of the old tensor:\n\ny2 &lt;- torch_zeros(4, 4)\nfor (j in 1:4) { # columns\n  for (i in 1:4) { # rows\n    y2[i, j] &lt;- y[i, j]\n  }\n}\nsum(abs(y - y2))\n\ntorch_tensor\n0\n[ CPUFloatType{} ]\n\n\nInternally, this type of reshaping is (in many cases) implemented by changing the stride of the tensor without altering the underlying data.\n\nx$stride()\n\n[1] 1\n\ny$stride()\n\n[1] 4 1\n\n\nThe value of the stride indicates how many elements to skip to get to the next element along each dimension: If we move from element x[1] (1) to element x[2] (2), we move one index along the columns of y. If we move from x[1] to x[5] (5), i.e., 4 steps, we move one index along the rows of y.\nThis means, for example, that reshaping of torch tensors can be considerably more efficient than reshaping of R arrays, as the latter will always allocate a new, reordered vector, while the former just changes the strides.\nThe functionality of strides is illustrated in the image below.\n\n\n\n\n\nSource\n\n\n\n\n\n\nQuiz: Strides\n\n\n\nQuestion 1: How do you need to change the strides from a matrix with strides (4, 1) (like the one above) to transpose it?\n\n\nClick for answer\n\nThe matrix can be transposed by changing the strides from (4, 1) to (1, 4).\n\ny$t()$stride()\n\n[1] 1 4\n\n\n\n\n\nWhen reshaping tensors, you can also infer a dimension by setting it to -1:\n\nx$reshape(c(-1, 4))$shape\n\n[1] 4 4\n\n\nOf course, not all reshaping operations are valid. The number of elements in the original tensor and the reshaped tensor must be the same:\n\nx$reshape(6)\n\nError in (function (self, shape) : shape '[6]' is invalid for input of size 16"
  },
  {
    "objectID": "notebooks/1-tensor.html#reference-semantics",
    "href": "notebooks/1-tensor.html#reference-semantics",
    "title": "Torch Tensors",
    "section": "Reference Semantics",
    "text": "Reference Semantics\nOne key property of torch tensors is that they have reference semantics.\n\nx &lt;- torch_ones(2)\ny &lt;- x\ny[1] &lt;- 5\nx # was modified\n\ntorch_tensor\n 5\n 1\n[ CPUFloatType{2} ]\n\n\nThis differs from R, where objects typically have value semantics:\n\nx &lt;- c(1, 1)\ny &lt;- x\ny[1] &lt;- 5\nx # was not modified\n\n[1] 1 1\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnother notable exception to value semantics are environments and R6 classes, which are used in the mlr3 ecosystem.\n\n\nWhen one tensor (y) shares underlying data with another tensor (x), this is called a view. It is also possible to obtain a view on a subset of a tensor, e.g., via slicing as shown below. There, torch_arange() creates a tensor with elements from 1 to 10.\n\nx &lt;- torch_arange(start = 1, end = 10, step = 1)\nx\n\ntorch_tensor\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n[ CPUFloatType{10} ]\n\ny &lt;- x[1:3]\ny\n\ntorch_tensor\n 1\n 2\n 3\n[ CPUFloatType{3} ]\n\ny[1] &lt;- 100\nx[1]\n\ntorch_tensor\n100\n[ CPUFloatType{} ]\n\n\nUnfortunately, similar operations might sometimes create a view and sometimes allocate a new tensor. In the example below, we create a subset that is a non-contiguous sequence, and hence a new tensor is allocated:\n\nx &lt;- torch_arange(1, 10)\ny &lt;- x[c(1, 3, 5)]\ny[1] &lt;- 100\nx[1]\n\ntorch_tensor\n1\n[ CPUFloatType{} ]\n\n\nIf it is important to create a copy of a vector, you can call the $clone() method:\n\nx &lt;- torch_arange(1, 3)\ny &lt;- x$clone()\ny[1] &lt;- 10\nx[1] # is still 1\n\ntorch_tensor\n1\n[ CPUFloatType{} ]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is also the case for the $reshape() methods from the last section, which will in some cases create a view and in other cases allocate a new tensor with the desired shape. If you want to ensure that you create a view on a tensor, you can use the $view() method, which will fail if the required view is not possible.\n\n\n\n\n\n\n\n\nQuiz: Contiguous Data\n\n\n\nQuestion 1: Reshaping a 2D tensor\nConsider the tensor below:\n\nx1 &lt;- torch_tensor(matrix(1:6, nrow = 2, byrow = FALSE))\nx1\n\ntorch_tensor\n 1  3  5\n 2  4  6\n[ CPULongType{2,3} ]\n\n\nWhat is the result of x1$reshape(6), i.e., what are the first, second, …, sixth elements?\n\n\nClick for answer\n\nThis will result in (1, 3, 5, 2, 4, 6) because we (imagine that) first iterate over the rows and then the columns when “creating” the new tensor."
  },
  {
    "objectID": "notebooks/2-autograd-exercise-task.html",
    "href": "notebooks/2-autograd-exercise-task.html",
    "title": "Autograd",
    "section": "",
    "text": "Question 1: Appreciating autograd\nConsider the following function:\n\\[\nf(x) = x^2 + 3x + 2\n\\]\nAs well as the function \\(g(x) = f(f(f(x)))\\)\nCalculate the gradient of both functions at point \\(x = 2\\).\nQuestion 2: Approximating functions with gradients\nThe defining feature of the gradient is that it allows us to approximate the function locally by a linear function.\nI.e., for some value \\(x^*\\), we know for very small \\(\\delta\\), that\n\\[\nf(x^* + \\delta) \\approx f(x^*) + f'(x^*) \\cdot \\delta\n\\]\nPlot the function from earlier as well as the local linear approximation at \\(x = 2\\) using ggplot2.\n\n\nHint\n\nTo do so, follow these steps:\n\nCreate a sequence with 100 equidistant values between -4 to 4 using torch_linspace().\nCreate the true function values at these points using the function from exercise 1.\nApproximate the function using the formula \\(f(x^* + \\delta) \\approx f(x^*) + f'(x^*) \\cdot \\delta\\).\nCreate a data.frame with columns x, y_true, y_approx.\nUse ggplot2 to plot the function and its linear approximation.\n\n\nQuestion 3: Look ma, I made my own autograd function\nIn this exercise, we will build our own, custom autograd function. While you might rarely need this in practice, it still allows you to get a better understanding of how the autograd system works. There is also a tutorial on this on the torch website.\nTo construct our own autograd function, we need to define:\n\nThe forward pass:\n\nHow to calculate outputs from inputs\nWhat to save for the backward pass\n\nThe backward pass:\n\nHow to calculate the gradient of the output with respect to the inputs, using the information saved during the forward pass\n\n\nThe task is to re-create the ReLU activation function, which is a common activation function in neural networks and which is defined as:\n\\[\n\\text{ReLU}(x) = \\max(0, x)\n\\]\nNote that strictly speaking, the ReLU function is not differentiable at \\(x = 0\\) (but a subgradient can be used instead). The derivative/subgradient of the ReLU function is:\n\\[\n\\text{ReLU}'(x) = \\begin{cases}\n1 & \\text{if } x &gt; 0 \\\\\n0 & \\text{if } x \\leq 0 \\\\\n\\end{cases}\n\\]\nIn torch, a custom autograd function can be constructed using autograd_function() and it accepts arguments forward and backward which are functions that define the forward and backward pass: They both take as first argument a ctx, which is a communication object that is used to save information during the forward pass to be able to compute the gradient in the backward pass (e.g. for \\(f(x) = x * a\\), to calculate the gradient of \\(f\\) with respect to \\(a\\) we need to know the input value \\(x\\)). The return value of the backward pass should be a list of gradients of the final node of the autograd graph with respect to the inputs. To check whether a gradient for an input is needed (has requires_grad = TRUE), you can use ctx$needs_input_grad which is a named list with boolean values for each input.\nThe backward function additionally takes a second argument grad_output, which is the gradient of the output: E.g., if our function is \\(f(x)\\) and we calculate the gradient of \\(g(x) = h(f(x))\\), then grad_output is the derivative of \\(h\\) with respect to its input, evaluated at \\(f(x)\\). This is essentially the chain rule: \\(\\frac{\\partial g}{\\partial x} = \\frac{\\partial h}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x}\\). The $backward() method of the autograd function $fe would in this case therefore not return \\(\\frac{\\partial f}{\\partial x}\\), but \\(\\frac{\\partial g}{\\partial x}\\).\nFill out the missing parts (...) in the code below.\n\nrelu &lt;- autograd_function(\n  forward = function(ctx, input) {\n    mask &lt;- ...\n    output &lt;- torch_where(mask, ...)\n    ctx$save_for_backward(mask)\n    output\n  },\n  backward = function(ctx, grad_output) {\n    grads &lt;- list(input = NULL)\n    if (ctx$needs_input_grad$input) {\n      mask &lt;- ctx$saved_variables[[1]]\n      grads$input &lt;- ...\n    }\n    grads\n  }\n)\n\nTo check that it’s working, use the code below (with your relu instead of nnf_relu) and check that the results are the same.\n\nx &lt;- torch_tensor(-1, requires_grad = TRUE)\n(nnf_relu(x)^2)$backward()\nx$grad\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\nx$grad$zero_()\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\nx &lt;- torch_tensor(3, requires_grad = TRUE)\n(nnf_relu(x)^2)$backward()\nx$grad\n\ntorch_tensor\n 6\n[ CPUFloatType{1} ]"
  },
  {
    "objectID": "notebooks/2-autograd.html",
    "href": "notebooks/2-autograd.html",
    "title": "Autograd",
    "section": "",
    "text": "Automatic differentiation (autograd) is one of torch’s key features, enabling the automatic computation of gradients for optimization tasks like training neural networks. Unlike numerical differentiation, which approximates gradients using finite differences, autograd computes exact gradients by tracking operations as they are performed and automatically applying the chain rule of calculus. This makes it possible to efficiently compute gradients of complex functions with respect to many parameters—a critical requirement for training modern neural networks.\nAutograd works by building a dynamic computational graph of operations, where each node represents a tensor and each edge represents a mathematical operation.\nWhy do we need automatic differentiation?\nIn deep learning, training a model requires iteratively updating parameters to minimize a loss function, which measures the difference between predictions and actual data. These updates depend on calculating gradients of the loss with respect to model parameters, information used by optimization algorithms like stochastic gradient descent (SGD). Automatic Differentiation eliminates the need to manually derive these gradients."
  },
  {
    "objectID": "notebooks/2-autograd.html#enabling-gradient-tracking",
    "href": "notebooks/2-autograd.html#enabling-gradient-tracking",
    "title": "Autograd",
    "section": "Enabling Gradient Tracking",
    "text": "Enabling Gradient Tracking\nTo use autograd, tensors must have their requires_grad field set to TRUE. This can either be set during tensor construction or changed afterward using the in-place modifier $requires_grad_(TRUE). In the context of deep learning, we track the gradients of the weights of a neural network. The simplest “neural network” is a linear model with slope \\(a\\) and bias \\(b\\) and a single input \\(x\\).\nThe forward pass is defined as:\n\\[\\hat{y} = u + b = a \\times x + b\\]\nWe might be interested in how the prediction \\(\\hat{y}\\) changes for the given \\(x\\) when we change the weight \\(a\\) or the bias \\(b\\). We will later use this to adjust the weights \\(a\\) and \\(b\\) to improve predictions, i.e., to perform gradient-based optimization. To write down the gradients, let \\(u = a \\times x\\) denote the intermediate tensor from the linear predictor.\nWe can now derive the gradients for:\n\nWeight \\(a\\):\nThis is expressed by the gradient \\(\\frac{\\partial \\hat{y}}{\\partial a}\\). We can compute the derivative using the chain rule as:\n\\[\\frac{\\partial \\hat{y}}{\\partial a} = \\frac{\\partial \\hat{y}}{\\partial u} \\cdot \\frac{\\partial u}{\\partial a} = 1 \\cdot x = x\\]\nBias \\(b\\):\n\\[\\frac{\\partial \\hat{y}}{\\partial b} = 1\\]\n\n\nlibrary(torch)\n\na &lt;- torch_tensor(2, requires_grad = TRUE)\na$requires_grad\n\n[1] TRUE\n\nb &lt;- torch_tensor(1, requires_grad = TRUE)\nx &lt;- torch_tensor(3)\n\nWe can use the weights and input to perform a forward pass:\n\nu &lt;- a * x\ny &lt;- u + b\n\nWhen you perform operations on tensors with gradient tracking, torch builds a computational graph on the fly. In the figure below:\n\nBlue tensors are those for which we want to calculate gradients.\nThe violet node is an intermediate tensor.\nThe yellow boxes are differentiable functions.\nThe green node is the final tensor with respect to which we want to calculate gradients.\n\n\n\n\n\n\ngraph TD\n    a[a] --&gt; mul[Multiply]\n    x[x] --&gt; mul\n    mul --&gt; u[u]\n    u --&gt; add[Add]\n    b[b] --&gt; add\n    add --&gt; y[y]\n\n    %% Styling\n    classDef input fill:#a8d5ff,stroke:#333\n    classDef op fill:#ffe5a8,stroke:#333\n    classDef output fill:#a8ffb6,stroke:#333\n    classDef grad fill:#ffa8a8,stroke:#333,stroke-dasharray:5,5\n    classDef intermediate fill:#d5a8ff,stroke:#333\n    classDef nograd fill:#e8e8e8,stroke:#333\n\n    class a,b input\n    class mul,add op\n    class y output\n    class u intermediate\n    class x nograd\n\n\n\n\n\n\nEach intermediate tensor knows how to calculate gradients with respect to its inputs.\n\ny$grad_fn\n\nAddBackward0\n\nu$grad_fn\n\nMulBackward0\n\n\nTo calculate the gradients \\(\\frac{\\partial y}{\\partial a}\\) and \\(\\frac{\\partial y}{\\partial b}\\), we can traverse this computational graph backwards, invoke the differentiation functions, and multiply the individual derivatives according to the chain rule.\nIn torch, this is done by calling $backward() on y. Note that $backward() can only be called on scalar tensors. Afterwards, the gradients are accessible in the $grad field of the tensors a and b:\n\n# Compute gradients\ny$backward()\n\n# Access gradients\nprint(a$grad)  # dy/da = x = 3\n\ntorch_tensor\n 3\n[ CPUFloatType{1} ]\n\nprint(b$grad)  # dy/db = 1\n\ntorch_tensor\n 1\n[ CPUFloatType{1} ]\n\n\nNote that for the intermediate value u, no gradient is stored.\n\n\n\n\n\n\nTip\n\n\n\nWhen you want to perform an operation on tensors that require gradients without tracking this specific operation, you can use with_no_grad(...).\n\n\nIn the next section, we will show how we can use gradients to train our simple linear model."
  },
  {
    "objectID": "notebooks/2-autograd.html#a-simple-linear-model",
    "href": "notebooks/2-autograd.html#a-simple-linear-model",
    "title": "Autograd",
    "section": "A Simple Linear Model",
    "text": "A Simple Linear Model\nWe can use autograd to fit a simple linear regression model. Let’s first generate some synthetic observations \\(\\{(x^{(i)}, y^{(i)})\\}_{i = 1}^n\\).\n\nlibrary(ggplot2)\n\n# Set random seed for reproducibility\ntorch_manual_seed(42)\n\n# Generate synthetic data\nn &lt;- 100\na_true &lt;- 2.5\nb_true &lt;- 1.0\n\n# Create input X and add noise to output Y\nX &lt;- torch_randn(n)\nnoise &lt;- torch_randn(n) * 0.5\nY &lt;- X * a_true + b_true + noise\n\n\n\n\n\n\n\n\n\n\nTo start the optimization, we need to randomly initialize our parameters a and b.\n\na &lt;- torch_randn(1, requires_grad = TRUE)\nb &lt;- torch_randn(1, requires_grad = TRUE)\n\nTo optimize the parameters \\(a\\) and \\(b\\), we further need to define the loss function that quantifies the discrepancy between our predictions \\(\\hat{y}\\) and the observed values \\(y\\). The standard loss for linear regression is the L2 loss:\n\\[ \\mathcal{L}y, \\hat{y}) = (y - \\hat{y})^2\\]\nThis allows us to define the empirical risk function that assigns the average loss over all observations to a given pair of parameters \\(a\\) and \\(b\\):\n\\[ \\hat{\\mathcal{R}}(a, b) = \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} - (a \\times x^{(i)} + b))^2\\]\nThis risk function is visualized in the contour plot below.\n\n\n\n\n\n\n\n\n\nWe can optimize the parameters \\(a\\) and \\(b\\) to converge to the minimum by using gradient descent. Gradient descent is a fundamental optimization algorithm that helps us find the minimum of a function by iteratively moving in the direction of steepest descent."
  },
  {
    "objectID": "notebooks/2-autograd.html#understanding-gradient-descent",
    "href": "notebooks/2-autograd.html#understanding-gradient-descent",
    "title": "Autograd",
    "section": "Understanding Gradient Descent",
    "text": "Understanding Gradient Descent\nThe gradient of a function points in the direction of the steepest increase—like pointing uphill on mountainous terrain. Therefore, the negative gradient points in the direction of the steepest decrease—like pointing downhill.\nGradient descent uses this property to iteratively:\n\nCalculate the gradient at the current position.\nTake a small step in the opposite direction of the gradient, so the negative gradient.\nRepeat until we reach a minimum.\n\nNote that the gradient only tells us in which direction we have to go, not how far. The length of the step should not be:\n\nToo large because the gradient approximation – and with it the notion of steepest descent – only holds in a small neighborhood.\nToo small as otherwise the convergence will be slow.\n\nThe general update formula for the weights \\(a\\) and \\(b\\) is:\n\\[a_{t+1} = a_t - \\eta \\frac{\\partial R}{\\partial a_t}\\] \\[b_{t+1} = b_t - \\eta \\frac{\\partial R}{\\partial b_t}\\]\nwhere \\(\\eta\\) is the learning rate / step size, and \\(R\\) is the empirical risk function.\nIn practice, when dealing with large datasets, computing the gradient over the entire dataset can be computationally expensive. Instead, we often use Stochastic Gradient Descent (SGD), where the gradient is estimated using only a few observations (a so called ‘batch’), but more on that later.\nWe start by implementing a single gradient step. Note that if we repeatedly call loss$backward(), the gradients in a and b would accumulate, so we set them to 0 before performing the update. The return value of the update will be the parameter values and the loss so we can plot them later. Also, note that we mutate the parameters a and b in-place (suffix _).\n\nupdate_params &lt;- function(X_batch, Y_batch, lr, a, b) {\n  # Perform forward pass, calculate average loss\n  Y_hat &lt;- X_batch * a + b\n  loss &lt;- mean((Y_hat - Y_batch)^2)\n\n  # Calculate gradients\n  loss$backward()\n\n  # We don't want to track gradients when we update the parameters.\n  with_no_grad({\n    a$sub_(lr * a$grad)\n    b$sub_(lr * b$grad)\n  })\n\n  # Ensure gradients are zero\n  a$grad$zero_()\n  b$grad$zero_()\n\n  # $item() converts scalar tensors to R vectors of length 1\n  list(\n    a = a$item(),\n    b = b$item(),\n    loss = loss$item()\n  )\n}\n\nNext, we use this function to learn the parameters \\(a\\) and \\(b\\) by repeatedly applying the update function.\n\nlibrary(data.table)\n\n# Hyperparameters\nlr &lt;- 0.02\nepochs &lt;- 10\nbatch_size &lt;- 10\n\n# Split data into 10 batches of size 10\nbatches &lt;- split(sample(1:100), rep(seq_len(batch_size), length.out = 100))\nhistory &lt;- list()\nfor (epoch in seq_len(epochs)) {\n  for (step in 1:10) {\n    result &lt;- update_params(X[batches[[step]]], Y[batches[[step]]], lr, a, b)\n    history &lt;- append(history, list(as.data.table(result)))\n  }\n}\n\nhistory = rbindlist(history)\n\nThis example demonstrates how we can use torch’s autograd to implement gradient descent for fitting a simple linear regression model. The dashed red lines show the progression of the model during training, with increasing opacity for later steps. The blue line represents the true relationship.\n\n\n\n\n\n\n\n\n\nWe can also visualize the parameter updates over time:\n\n\n\n\n\n\n\n\n\nOf course, better solutions exist for estimating a simple linear model. Because the risk is simply a quadratic function, we can analytically solve this optimization problem by solving the normal equations. The motivation behind the example was merely to demonstrate how we can utilize an autograd system to estimate the parameters of a model."
  },
  {
    "objectID": "notebooks/3-modules-data-exercise-task.html",
    "href": "notebooks/3-modules-data-exercise-task.html",
    "title": "It’s a Sin(us)",
    "section": "",
    "text": "Question 1: Create a torch::dataset class that takes in arguments n, min, and max during initialization where:\n\nn is the total number of samples\nmin is the lower bound of the data\nmax is the upper bound of the data\n\nIn the initialize method, generate and store:\n\na tensor x of shape (n, 1) that contains n values drawn from a uniform distribution between min and max\na tensor y of shape (n, 1) that is defined as \\(\\sin(x) + \\epsilon\\) where \\(\\epsilon\\) is drawn from a normal distribution with mean 0 and standard deviation 0.1\n\n\n\nHint\n\nUse torch_randn() to draw from a standard normal distribution and multiply it with 0.1^2 to get the desired standard deviation.\n\nImplement the $.getbatch() method to return a named list with values x and y. Then, create an instance of the dataset with n = 1000, min = 0, and max = 10.\nMake sure that the dataset is working by calling the $.getbatch() method. Also, check that the shapes of both tensors returned by the dataset are (n_batch, 1).\nSolution\n\nlibrary(torch)\nsin_dataset &lt;- dataset(\n  initialize = function(n, min, max) {\n    self$x &lt;- torch_rand(n, 1) * (max - min) + min\n    self$y &lt;- torch_sin(self$x) + torch_randn(n, 1) * 0.1^2\n  },\n  .getbatch = function(i) {\n    list(x = self$x[i, drop = FALSE], y = self$y[i, drop = FALSE])\n  },\n  .length = function() {\n    length(self$x)\n  }\n)\nds &lt;- sin_dataset(n = 1000, min = 0, max = 10)\nbatch &lt;- ds$.getbatch(1:10)\nbatch$x$shape\n\n[1] 10  1\n\nbatch$y$shape\n\n[1] 10  1\n\n\nQuestion 2: Create a torch::dataloader that takes in the dataset and returns batches of size 10. Then, iterate over the batches of the dataloader and create one tensor X and one tensor Y that contains the concatenated batches of x and y.\n\n\nHint\n\nThe functions coro::loop() and torch_cat() might be helpful.\n\nSolution\n\ndl &lt;- dataloader(ds, batch_size = 10)\nbatches &lt;- list()\ncoro::loop(for (batch in dl) {\n  batches &lt;- c(batches, list(batch))\n})\nX &lt;- torch_cat(lapply(batches, function(batch) batch$x), dim = 1)\nX$shape\n\n[1] 1000    1\n\nY &lt;- torch_cat(lapply(batches, function(batch) batch$y), dim = 1)\nY$shape\n\n[1] 1000    1\n\n\nQuestion 3: Create a custom torch module that allows modeling the sinus data we have created. To test it, apply it to the tensor X we have created above and calculate its mean squared error with the tensor Y. Disable gradient tracking when doing so.\n\n\nHint\n\nYou can either use nn_module to create a custom module generically, or you can use nn_sequential() to create a custom module that is a sequence of layers.\n\nSolution\n\nnn_sin &lt;- nn_module(\"nn_sin\",\n  initialize = function(latent = 200) {\n    self$lin1 &lt;- nn_linear(1, latent)\n    self$lin2 &lt;- nn_linear(latent, latent)\n    self$lin3 &lt;- nn_linear(latent, 1)\n  },\n  forward = function(x) {\n    x |&gt;\n      self$lin1() |&gt;\n      nnf_relu() |&gt;\n      self$lin2() |&gt;\n      nnf_relu() |&gt;\n      self$lin3()\n  }\n)\nnet &lt;- nn_sin(200)\nY_pred &lt;- with_no_grad(net(X))\nnnf_mse_loss(Y_pred, Y)\n\ntorch_tensor\n1.0703\n[ CPUFloatType{} ]\n\n\nQuestion 4: Train the model on the task for different hyperparameters (lr or epochs) and visualize the results. Play around with the hyperparameters until you get a good fit. You can use the helper functions below for that.\n\nlibrary(ggplot2)\npredict_network &lt;- function(net, dataloader) {\n  local_no_grad()\n  xs &lt;- list(x = numeric(), y = numeric(), pred = numeric())\n  i &lt;- 1\n  net$eval()\n  coro::loop(for (batch in dataloader) {\n    xs$x &lt;- c(xs$x, as.numeric(batch$x))\n    xs$y &lt;- c(xs$y, as.numeric(batch$y))\n    xs$pred &lt;- c(xs$pred, as.numeric(net(batch$x)))\n  })\n  as.data.frame(xs)\n}\ntrain_network &lt;- function(net, dataloader, epochs, lr) {\n  optimizer &lt;- optim_ignite_adamw(net$parameters, lr = lr)\n  net$train()\n  for (i in seq_len(epochs)) {\n    coro::loop(for (batch in dataloader) {\n      optimizer$zero_grad()\n      Y_pred &lt;- net(batch$x)\n      loss &lt;- nnf_mse_loss(Y_pred, batch$y)\n      loss$backward()\n      optimizer$step()\n    })\n  }\n  predict_network(net, dataloader)\n}\nplot_results &lt;- function(df) {\n  ggplot(data = df, aes(x = x)) +\n    geom_point(aes(y = y, color = \"true\")) +\n    geom_point(aes(y = pred, color = \"pred\")) +\n    theme_minimal() +\n    labs(color = \"\")\n}\ntrain_and_plot &lt;- function(net, dataloader, epochs = 10, lr = 0.01) {\n  result &lt;- train_network(net, dataloader, epochs = epochs, lr = lr)\n  plot_results(result)\n}\n\n\n\n\n\n\n\nTip\n\n\n\nBeware the reference semantics and make sure that you create a new instance of the network for each run.\n\n\nSolution\n\nnet &lt;- nn_sin(200)\ntrain_and_plot(net, dl, epochs = 200, lr = 0.01)\n\n\n\n\n\n\n\n\nQuestion 5: Create a new instance from the sinus dataset class created earlier. Now, set the min and max values to 0 and 20 respectively and visualize the predictions of the previously trained network on this new dataset. What do you observe? Can you explain why this is happening and can you fix the network architecture to make it work?\n\n\nHint\n\nThe sinus function has a phase of \\(2 \\pi\\).\n\nSolution\n\ndl_ood &lt;- dataloader(sin_dataset(n = 1000, min = 0, max = 20), batch_size = 10)\nplot_results(predict_network(net, dl_ood))\n\n\n\n\n\n\n\n\nFor values in the range [10, 20], the network fails to generalize. This is because the network only observed values in the range [0, 10] during training.\nWe can fix this by preprocessing the data using the modulo operator, i.e. using the correct inductive bias for the problem.\n\nnn_sin2 &lt;- nn_module(\"nn_sin2\",\n  initialize = function(latent = 200) {\n    self$lin1 &lt;- nn_linear(1, latent)\n    self$lin2 &lt;- nn_linear(latent, latent)\n    self$lin3 &lt;- nn_linear(latent, 1)\n  },\n  forward = function(x) {\n    (x %% (2 * pi)) |&gt;\n      self$lin1() |&gt;\n      nnf_relu() |&gt;\n      self$lin2() |&gt;\n      nnf_relu() |&gt;\n      self$lin3()\n  }\n)\nnet2 &lt;- nn_sin2(200)\ndf &lt;- train_network(net2, dl, epochs = 200, lr = 0.01)\nplot_results(predict_network(net2, dl_ood))"
  },
  {
    "objectID": "notebooks/3-modules-data.html",
    "href": "notebooks/3-modules-data.html",
    "title": "Modules and Data",
    "section": "",
    "text": "In the previous notebook, we explored how to use torch’s autograd system to fit simple linear models. We manually:\n\nManaged the weights.\nDefined the forward path for the model.\nComputed gradients and updated parameters using a simple update rule: a$sub_(lr * a$grad).\n\nFor more complex models, this approach becomes cumbersome. torch offers several high-level abstractions that simplify building and training neural networks:\n\nnn_module: A class to organize model parameters and define the forward pass, i.e. the neural network architecture.\ndataset and dataloader: Classes to handle data loading and batching, replacing our manual data handling.\noptim: Classes that implement various optimization algorithms, replacing our simple gradient updates.\n\nLet’s explore how these components work together by building a neural network to classify spirals. Note that we only briefly touch on optimizers here and dedicate an additional notebook to them."
  },
  {
    "objectID": "notebooks/3-modules-data.html#from-linear-models-to-deep-neural-networks",
    "href": "notebooks/3-modules-data.html#from-linear-models-to-deep-neural-networks",
    "title": "Modules and Data",
    "section": "",
    "text": "In the previous notebook, we explored how to use torch’s autograd system to fit simple linear models. We manually:\n\nManaged the weights.\nDefined the forward path for the model.\nComputed gradients and updated parameters using a simple update rule: a$sub_(lr * a$grad).\n\nFor more complex models, this approach becomes cumbersome. torch offers several high-level abstractions that simplify building and training neural networks:\n\nnn_module: A class to organize model parameters and define the forward pass, i.e. the neural network architecture.\ndataset and dataloader: Classes to handle data loading and batching, replacing our manual data handling.\noptim: Classes that implement various optimization algorithms, replacing our simple gradient updates.\n\nLet’s explore how these components work together by building a neural network to classify spirals. Note that we only briefly touch on optimizers here and dedicate an additional notebook to them."
  },
  {
    "objectID": "notebooks/3-modules-data.html#neural-network-architecture-with-nn_module",
    "href": "notebooks/3-modules-data.html#neural-network-architecture-with-nn_module",
    "title": "Modules and Data",
    "section": "Neural Network Architecture with nn_module",
    "text": "Neural Network Architecture with nn_module\nThe nn_module class serves several purposes, it\n\nacts as a container for learnable parameters.\nprovides train/eval modes, which are essential for layers like dropout and batch normalization.\ndefines the forward pass of the model.\n\nTorch offers many common neural network modules out of the box. For example, the simple linear model we created earlier (\\(\\hat{y} = a \\times x + b\\)) can be constructed using the built-in nn_linear module:\n\nlibrary(torch)\nlinear_model &lt;- nn_linear(in_features = 1, out_features = 1, bias = TRUE)\nlinear_model$parameters\n\n$weight\ntorch_tensor\n-0.4078\n[ CPUFloatType{1,1} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n0.01 *\n 3.3125\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n\nWe can perform a forward pass by simply calling the function on some inputs.\n\nlinear_model(torch_randn(1))\n\ntorch_tensor\n-0.5108\n[ CPUFloatType{1} ][ grad_fn = &lt;ViewBackward0&gt; ]\n\n\nNote that while nn_modules behave like functions, they also maintain a state, primarily their parameter weights.\nImplementing a custom nn_module is straightforward and requires defining two key methods:\n\ninitialize: This constructor runs when the module is created. It defines the layers and their dimensions. It can take arguments that allow to configure the network layer (such as the number of neurons in a layer).\nforward: This method defines how data flows through your network: it specifies the actual computation path from inputs to outputs.\n\nLet’s implement a simple linear regression module ourselves.\n\nnn_simple_linear &lt;- nn_module(\"nn_simple_linear\",\n  initialize = function() {\n    # `self` refers to the object itself\n    self$a = nn_parameter(torch_randn(1), requires_grad = TRUE)\n    self$b = nn_parameter(torch_randn(1), requires_grad = TRUE)\n  },\n  forward = function(x) {\n    self$a * x + self$b\n  }\n)\n\nNote that nn_simple_linear is not an nn_module itself but an nn_module_generator. To create the nn_module, we call it, which invokes the $initialize() method defined above:\n\nsimple_linear &lt;- nn_simple_linear()\nsimple_linear\n\nAn `nn_module` containing 2 parameters.\n\n── Parameters ──────────────────────────────────────────────────────────────────────────────────────────────────────────\n• a: Float [1:1]\n• b: Float [1:1]\n\nsimple_linear$parameters\n\n$a\ntorch_tensor\n 0.1123\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n$b\ntorch_tensor\n 0.9447\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n\nFurthermore, note that we wrapped the trainable tensors in nn_parameter(), ensuring they are included in the $parameters. Only those weights that are part of the network’s parameters (and have $requires_grad set to TRUE) will later be updated by the optimizer.\n\nsimple_linear$parameters\n\n$a\ntorch_tensor\n 0.1123\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n$b\ntorch_tensor\n 0.9447\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n\nBesides parameters, neural networks can also have buffers (nn_buffer). Buffers are tensors that are part of the model’s state but don’t receive gradients during backpropagation.\nAdditionally, an nn_module operates in either a train or eval state that are activated by $train() and $eval() respectively.\n\nsimple_linear$train()\nsimple_linear$training\n\n[1] TRUE\n\nsimple_linear$eval()\nsimple_linear$training\n\n[1] FALSE\n\n\nSome nn_modules (such as batch normalization) behave differently depending on this mode, so it’s essential to ensure that the network is in the correct mode.\nAnother important method of a network is $state_dict(), which returns the network’s parameters and buffers.\n\nsimple_linear$state_dict()\n\n$a\ntorch_tensor\n 0.1123\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n$b\ntorch_tensor\n 0.9447\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n\nYou can load new parameters into a network using $load_state_dict():\n\nsimple_linear$load_state_dict(list(\n  a = nn_parameter(torch_tensor(1)),\n  b = nn_parameter(torch_tensor(0))\n))\nsimple_linear$state_dict()\n\n$a\ntorch_tensor\n 1\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n$b\ntorch_tensor\n 0\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n\nThe state dict can, for example, be used to save the network’s weights for later use. Note that, in general, you cannot simply save and load torch objects using saveRDS and readRDS:\n\npth &lt;- tempfile()\nsaveRDS(simple_linear$state_dict(), pth)\nreadRDS(pth)\n\n$a\ntorch_tensor\n\n\nError in (function (self) : external pointer is not valid\n\n\nInstead, you need to use torch_save and torch_load:\n\ntorch_save(simple_linear$state_dict(), pth)\ntorch_load(pth)\n\n$a\ntorch_tensor\n 1\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n$b\ntorch_tensor\n 0\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n\nIt is also possible to save the entire nn_module.\n\ntorch_save(simple_linear, pth)\ntorch_load(pth)\n\nAn `nn_module` containing 2 parameters.\n\n── Parameters ──────────────────────────────────────────────────────────────────────────────────────────────────────────\n• a: Float [1:1]\n• b: Float [1:1]\n\n\nBesides adding parameters and buffers to the network’s state dict by registering nn_parameters and nn_buffers in the module’s $initialize() method, you can also register other nn_modules, which we will do in the next section.\n\nThe World is Not Linear\nWhile we have so far explained much of torch’s functionality using simple linear networks, the main idea of deep learning is to model complex, non-linear relationships. Below, we generate some non-linear synthetic spiral data for binary classification:\n\nlibrary(torch)\nlibrary(ggplot2)\nlibrary(mlbench)\n\n# Generate spiral data\nset.seed(123)\nn &lt;- 500\nspiral &lt;- mlbench.spirals(n, sd = 0.1)\n\n# Convert to data frame\nspiral_data &lt;- data.frame(\n  x1 = spiral$x[,1],\n  x2 = spiral$x[,2],\n  label = as.factor(spiral$classes)\n)\n\nThe data looks like this:\n\n\n\n\n\n\n\n\n\nWhile linear models are often useful and have helped us explain the torch API, they are limited in capturing the complex, non-linear patterns commonly present in real-world data, especially unstructured types like images, text, audio, and video. Deep neural networks typically consist of many different layers (hence the name “deep”) and combine linear and non-linear layers with various other components, allowing them to represent highly complex functions. Traditional machine learning and statistics rely on manual feature engineering to transform raw inputs, whereas deep neural networks have revolutionized this process by automatically learning hierarchical features directly from the data.\nOne challenging problem is defining a neural network architecture for a given task. This is where architectural choices and their associated inductive biases come into play. An inductive bias represents a set of structural assumptions of how our predictive function looks like and behaves. These biases help the model generalize beyond its training data by favoring certain solutions over others.\nSome examples of inductive biases in different neural network architectures are convolutional neural networks, transformers, and multi-layer perceptrons (MLPs). Here, we will focus on MLPs, which are the most basic type of neural network (but are an integral part of basically every other neural network).\nThe different layers in a Multi-Layer Perceptron (MLP) consist of an affine-linear transformation followed by a non-linear function, such as a ReLU activation function:\n\nSource\nOur simple multi-layer perceptron has minimal inductive biases:\n\nContinuity: Similar inputs should produce similar outputs.\nHierarchical Feature Learning: Each layer builds increasingly abstract representations.\n\nThis flexibility makes MLPs general-purpose learners, but they may require more data or parameters to learn patterns that specialized architectures can discover more efficiently.\nFor our spirals classification problem, we will use a simple MLP with three hidden layers:\n\nnn_spiral_net &lt;- nn_module(\"nn_spiral_net\",\n  initialize = function(input_size, hidden_size, output_size) {\n    self$fc1 &lt;- nn_linear(input_size, hidden_size)\n    self$fc2 &lt;- nn_linear(hidden_size, hidden_size)\n    self$fc3 &lt;- nn_linear(hidden_size, hidden_size)\n    self$fc4 &lt;- nn_linear(hidden_size, output_size)\n    self$relu = nn_relu()\n  },\n\n  forward = function(x) {\n    x |&gt;\n      self$fc1() |&gt;\n      self$relu() |&gt;\n      self$fc2() |&gt;\n      self$relu() |&gt;\n      self$fc3() |&gt;\n      self$relu() |&gt;\n      self$fc4()\n  }\n)\n\n\n\n\n\n\n\nTip\n\n\n\nInstead of creating an nn_relu() during network initialization, we could have used the nnf_relu function directly in the forward pass. This is possible for activation functions as they have no trainable weights.\nIn general, nn_ functions create module instances that can maintain state (like trainable weights or running statistics), while nnf_ functions provide the same operations as pure functions without any state.\nFurthermore, for simple sequential networks, we could have used nn_sequential to define the network instead of nn_module. This allows you to chain layers together in a linear fashion without explicitly defining the forward pass.\n\n\nThe architecture of such an MLP layer is visualized below:\n\nSource\nWe can create a concrete network instance by calling the resulting nn_module_generator and specifying the required parameters.\n\n# Create model instance\nmodel &lt;- nn_spiral_net(\n  input_size = 2,\n  hidden_size = 64,\n  output_size = 2\n)\n\nprint(model)\n\nAn `nn_module` containing 8,642 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────────────────────────────────────────────\n• fc1: &lt;nn_linear&gt; #192 parameters\n• fc2: &lt;nn_linear&gt; #4,160 parameters\n• fc3: &lt;nn_linear&gt; #4,160 parameters\n• fc4: &lt;nn_linear&gt; #130 parameters\n• relu: &lt;nn_relu&gt; #0 parameters\n\n\nAt this point, let’s briefly discuss the ‘head’ of the network, as well as loss functions.\nClassification\nThe output dimension of a classification network is usually the number of classes, which is 2 in our case. However, the output is not probabilities but logit scores. To convert a vector of scores to probabilities, we apply the softmax function:\n\\[ \\text{softmax}(x) = \\frac{\\exp(x)}{\\sum_i \\exp(x_i)} \\]\nIn torch, we can apply the softmax function using nnf_softmax(), specifying the dimension along which to apply softmax.\n\nlogits &lt;- model(torch_randn(2, 2))\nprint(logits)\n\ntorch_tensor\n0.01 *\n-1.4163 -4.6232\n  4.6392 -6.1470\n[ CPUFloatType{2,2} ][ grad_fn = &lt;AddmmBackward0&gt; ]\n\n# dim = 2 applies softmax along the class dimension (columns)\nnnf_softmax(logits, dim = 2)\n\ntorch_tensor\n 0.5080  0.4920\n 0.5269  0.4731\n[ CPUFloatType{2,2} ][ grad_fn = &lt;SoftmaxBackward0&gt; ]\n\n\nThe most commonly used loss function is cross-entropy. For a true probability vector \\(p\\) and a predicted probability vector \\(q\\), the cross-entropy is defined as:\n\\[ \\text{CE}(p, q) = - \\sum_i p_i \\log(q_i) \\]\nNote that when the true probability \\(p\\) is 1 for the true class and 0 for all other classes, the cross-entropy simplifies to:\n\\[ \\text{CE}(p, q) = - \\log(q_{y}) \\]\nwhere \\(y\\) is the true class and \\(q_y\\) is its predicted probability.\nTo calculate the cross-entropy loss, we need to pass the predicted scores and the true class indices to the loss function. The classes should be labeled from 1 to C for a total of C classes.\n\ny_true &lt;- torch_tensor(c(1, 2), dtype = torch_long())\ny_true\n\ntorch_tensor\n 1\n 2\n[ CPULongType{2} ]\n\nlogits\n\ntorch_tensor\n0.01 *\n-1.4163 -4.6232\n  4.6392 -6.1470\n[ CPUFloatType{2,2} ][ grad_fn = &lt;AddmmBackward0&gt; ]\n\nnnf_cross_entropy(input = logits, target = y_true)\n\ntorch_tensor\n0.712886\n[ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward0&gt; ]\n\n\nRegression\nFor regression tasks, the final layer is almost always a simple linear layer with a single output. We can construct a version of the spiral network for regression by changing the final layer to a linear layer with a single output:\n\nmodel_regr &lt;- nn_spiral_net(input_size = 2, hidden_size = 64, output_size = 1)\nx &lt;- torch_randn(1, 2)\ny_hat &lt;- model_regr(x)\ny &lt;- torch_randn(1, 1)\n\nThe loss function typically used is the mean squared error, defined as:\n\\[ \\text{MSE}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} - \\hat{y}^{(i)})^2 \\]\nIn torch, we can apply the mean squared error loss using nnf_mse_loss(), or construct an MSE module:\n\nmse &lt;- nn_mse_loss()\nmse(y_hat, y)\n\ntorch_tensor\n0.581592\n[ CPUFloatType{} ][ grad_fn = &lt;MseLossBackward0&gt; ]\n\nnnf_mse_loss(y_hat, y)\n\ntorch_tensor\n0.581592\n[ CPUFloatType{} ][ grad_fn = &lt;MseLossBackward0&gt; ]\n\n\n\n\n\n\n\n\nNote\n\n\n\nFinally, it’s important to note that there is nothing inherently ‘magical’ about nn_modules. We could have equally implemented the same network manually ourselves, i.e. without using the nn_module class."
  },
  {
    "objectID": "notebooks/3-modules-data.html#dataset-and-dataloader",
    "href": "notebooks/3-modules-data.html#dataset-and-dataloader",
    "title": "Modules and Data",
    "section": "Dataset and DataLoader",
    "text": "Dataset and DataLoader\nBesides the network architecture, another essential component of deep learning is the dataset. The two central classes are dataset and dataloader, which address separate concerns:\n\ndataset: Handles data storage and access to individual samples. The methods are:\n\n.getitem(): Returns a single sample, regardless of the retrieval method (e.g., reading from disk or fetching from a database).\n.getbatch() (optional): Returns a full batch.\n.length(): Returns the dataset size.\n\ndataloader: Given a dataset, handles batching, shuffling, and parallel loading.\n\nWe will start by creating a custom dataset class for the spiral problem. In its $initialize() method, it expects a data.frame with columns \"x1\", \"x2\", and \"label\". We then convert these to tensors and store them in the object.\nBelow, we implement .getitem(), but we could also implement .getbatch(), which retrieves a vector of indices. Note that implementing .getbatch() can sometimes offer performance benefits.\n\nspiral_dataset &lt;- dataset(\n  name = \"spiral_dataset\",\n  initialize = function(data) {\n    self$x &lt;- torch_tensor(as.matrix(data[, c(\"x1\", \"x2\")]))\n    self$y &lt;- torch_tensor(as.integer(data$label))\n  },\n  .getitem = function(i) {\n    list(\n      x = self$x[i,],\n      y = self$y[i]\n    )\n  },\n  .length = function() {\n    self$y$size()[[1]]\n  }\n)\n\nNow that we have defined the dataset class generator, let’s create training and validation datasets:\nTraining and validation datasets serve different purposes:\n\nTraining data is used to update the model’s parameters and learn patterns.\nValidation data helps evaluate how well the model generalizes to unseen data.\n\nValidation in deep learning is crucial for:\n\nDetecting Overfitting: If training loss decreases but validation loss increases, the model is likely overfitting to the training data.\nModel Selection: We can use validation performance to choose the best model architecture and hyperparameters.\nEarly Stopping: We can halt training when validation performance stops improving to prevent overfitting.\n\n\n# Split data into train and validation sets\ntrain_ids &lt;- sample(1:500, 400)\ntrain_data &lt;- spiral_data[train_ids,]\nvalid_data &lt;- spiral_data[-train_ids,]\n\n# Create datasets\ntrain_dataset &lt;- spiral_dataset(train_data)\nvalid_dataset &lt;- spiral_dataset(valid_data)\n\nWe can access individual elements via the $.getitem() method:\n\ntrain_dataset$.getitem(1)\n\n$x\ntorch_tensor\n-0.2233\n-0.8093\n[ CPUFloatType{2} ]\n\n$y\ntorch_tensor\n1\n[ CPULongType{} ]\n\n\nConstructing a dataloader is straightforward:\n\ntrain_loader &lt;- dataloader(\n  train_dataset,\n  batch_size = 64,\n  # shuffling is important when your data is ordered\n  shuffle = TRUE,\n  drop_last = FALSE\n)\n\nvalid_loader &lt;- dataloader(\n  valid_dataset,\n  batch_size = 64,\n  shuffle = FALSE,\n  drop_last = FALSE\n)\n\nThe most common way to iterate over the batches of a dataloader is to use the coro::loop function, which resembles a for loop:\n\nn_batches &lt;- 0\ncoro::loop(for (batch in train_loader) {\n  # do something with the batch\n  n_batches &lt;- n_batches + 1\n})\nprint(n_batches)\n\n[1] 7\n\n\nIt is also possible to manually iterate over the batches by first creating an iterator using torch::dataloader_make_iter() and then calling dataloader_next() until NULL is returned, indicating that the iterator is exhausted.\n\niter &lt;- dataloader_make_iter(train_loader)\nn_batches &lt;- 0\nwhile (!is.null(batch &lt;&lt;- dataloader_next(iter))) {\n  n_batches &lt;- n_batches + 1\n}\nprint(n_batches)\n\n[1] 7\n\n\nThe torch::dataloader class also has other parameters that e.g. allow to parallelize the loading. This will be covered in the Training Efficiency notebook."
  },
  {
    "objectID": "notebooks/3-modules-data.html#training-loop",
    "href": "notebooks/3-modules-data.html#training-loop",
    "title": "Modules and Data",
    "section": "Training Loop",
    "text": "Training Loop\nTo train our MLP on the data, we need to specify how the gradients will update the network parameters, which is the role of the optimizer. While we’ll cover more complex optimizers in the next section, we’ll use a vanilla SGD optimizer with a learning rate of 0.3 and pass it the parameters of the model we wish to optimize. Note that it is important to move the model to the correct device before passing it to the optimizer.\n\n# Move model to device\ndevice &lt;- if (cuda_is_available()) \"cuda\" else \"cpu\"\nmodel$to(device = device)\n\noptimizer &lt;- optim_sgd(model$parameters, lr = 0.3)\n\nFor the training loop, we only need methods from the optimizer class:\n\nThe $step() method updates the weights based on the gradients and the optimizer configuration (e.g., the learning rate).\nThe $zero_grad() method sets the gradients of all parameters handled by the optimizer to 0.\n\nNow, let’s put everything together:\n\n# Training settings\nn_epochs &lt;- 50\n\n# Training loop\nhistory &lt;- list(loss = numeric(), train_acc = numeric(), valid_acc = numeric())\n\nfor(epoch in seq_len(n_epochs)) {\n  model$train()  # Set to training mode\n\n  # Training loop\n  train_losses &lt;- numeric()\n  train_accs &lt;- numeric()\n  coro::loop(for(batch in train_loader) {\n    # Move batch to device\n    x &lt;- batch$x$to(device = device)\n    y &lt;- batch$y$to(device = device)\n\n    # Forward pass and average loss computation\n    output &lt;- model(x)\n    loss &lt;- nnf_cross_entropy(output, y)\n\n    # Backward pass\n    optimizer$zero_grad()\n    loss$backward()\n\n    # Update parameters\n    optimizer$step()\n\n    # Store training losses\n    train_losses &lt;- c(train_losses, loss$item())\n    train_accs &lt;- c(train_accs, mean(as_array(output$argmax(dim = 2) == y)))\n  })\n\n  history$loss &lt;- c(history$loss, mean(train_losses))\n  history$train_acc &lt;- c(history$train_acc, mean(train_accs))\n\n  # Validation loop\n\n  # Set model to evaluation mode\n  model$eval()\n\n  valid_accs &lt;- numeric()\n  coro::loop(for(batch in valid_loader) {\n    x &lt;- batch$x$to(device = device)\n    y &lt;- batch$y$to(device = device)\n    # IMPORTANT: Disable gradient tracking\n    output &lt;- with_no_grad(model(x))\n    valid_acc &lt;- as_array(output$argmax(dim = 2) == y)\n    valid_accs = c(valid_accs, mean(valid_acc))\n  })\n\n  history$valid_acc &lt;- c(history$valid_acc, mean(valid_accs))\n}\n\nThe decision boundary plot shows how our neural network learned to separate the spiral classes, demonstrating its ability to learn non-linear patterns that a simple linear model couldn’t capture.\n\n\n\n\n\n\n\n\n\nWe can also visualize the predictions of our final network:\n\n\n\n\n\n\n\n\n\nThis example demonstrates how torch’s high-level components work together to build and train neural networks:\n\nnn_module manages our parameters and network architecture.\nThe optimizer handles parameter updates.\nThe dataset and dataloader classes work in tandem for data loading.\nThe training loop integrates everything seamlessly."
  },
  {
    "objectID": "notebooks/4-optimizer-exercise-task.html",
    "href": "notebooks/4-optimizer-exercise-task.html",
    "title": "Optimizer Exercises",
    "section": "",
    "text": "Question 1\nIn this exercise, the task is to play around with the settings for the optimization of a neural network. We start by generating some (highly non-linear) synthetic data using the mlbench package.\n\nlibrary(torch)\ndata &lt;- mlbench::mlbench.friedman3(n = 3000, sd = 0.1)\nX &lt;- torch_tensor(data$x)\nX[1:2, ]\n\ntorch_tensor\n   40.8977   745.3378     0.3715     3.4246\n   88.3017  1148.7073     0.5288     6.5953\n[ CPUFloatType{2,4} ]\n\nY &lt;- torch_tensor(data$y)$unsqueeze(2)\nY[1:2, ]\n\ntorch_tensor\n 1.5238\n 1.3572\n[ CPUFloatType{2,1} ]\n\n\nThe associated machine learning task is to predict the output Y from the input X.\nNext, we create a dataset for it using the tensor_dataset() function.\n\nds &lt;- tensor_dataset(X, Y)\nds$.getitem(1)\n\n[[1]]\ntorch_tensor\n  40.8977\n 745.3378\n   0.3715\n   3.4246\n[ CPUFloatType{4} ]\n\n[[2]]\ntorch_tensor\n 1.5238\n[ CPUFloatType{1} ]\n\n\nWe can create two sub-datasets – for training and validation – using dataset_subset().\n\nids_train &lt;- sample(1000, 700)\nids_valid &lt;- setdiff(seq_len(1000), ids_train)\nds_train &lt;- dataset_subset(ds, ids_train)\nds_valid &lt;- dataset_subset(ds, ids_valid)\n\nThe network that we will be fitting is a simple MLP:\n\nnn_mlp &lt;- nn_module(\"nn_mlp\",\n  initialize = function() {\n    self$lin1 &lt;- nn_linear(4, 50)\n    self$lin2 &lt;- nn_linear(50, 50)\n    self$lin3 &lt;- nn_linear(50, 1)\n  },\n  forward = function(x) {\n    x |&gt;\n      self$lin1() |&gt;\n      nnf_relu() |&gt;\n      self$lin2() |&gt;\n      nnf_relu() |&gt;\n      self$lin3()\n  }\n)\n\nThe code to compare different optimizer configurations is provided via the compare_configs() function, which you can copy paste.\n\n\nImplementation of compare_configs\n\n\nlibrary(ggplot2)\ncompare_configs &lt;- function(epochs = 30, batch_size = 16, lr = 0.01, weight_decay = 0.01, beta1 = 0.9, beta2 = 0.999) {\n  # Identify which parameter is a list\n  args &lt;- list(batch_size = batch_size, lr = lr, weight_decay = weight_decay, beta1 = beta1, beta2 = beta2)\n  is_list &lt;- sapply(args, is.list)\n\n  if (sum(is_list) != 1) {\n    stop(\"One of the arguments must be a list\")\n  }\n\n  list_arg_name &lt;- names(args)[is_list]\n  list_args &lt;- args[[list_arg_name]]\n  other_args &lt;- args[!is_list]\n\n  # Run train_valid for each value in the list\n  results &lt;- lapply(list_args, function(arg) {\n    network &lt;- with_torch_manual_seed(seed = 123, nn_mlp())\n    other_args[[list_arg_name]] &lt;- arg\n    train_valid(network, ds_train = ds_train, ds_valid = ds_valid, epochs = epochs, batch_size = other_args$batch_size,\n      lr = other_args$lr, betas = c(other_args$beta1, other_args$beta2), weight_decay = other_args$weight_decay)\n  })\n\n  # Combine results into a single data frame\n  combined_results &lt;- do.call(rbind, lapply(seq_along(results), function(i) {\n    df &lt;- results[[i]]\n    df$config &lt;- paste(list_arg_name, \"=\", list_args[[i]])\n    df\n  }))\n\n  upper &lt;- if (max(combined_results$valid_loss) &gt; 10) quantile(combined_results$valid_loss, 0.98) else max(combined_results$valid_loss)\n\n  ggplot(combined_results, aes(x = epoch, y = valid_loss, color = config)) +\n    geom_line() +\n    theme_minimal() +\n    labs(x = \"Epoch\", y = \"Validation RMSE\", color = \"Configuration\") +\n    ylim(min(combined_results$valid_loss), upper)\n}\ntrain_loop &lt;- function(network, dl_train, opt) {\n  network$train()\n  coro::loop(for (batch in dl_train) {\n    opt$zero_grad()\n    Y_pred &lt;- network(batch[[1]])\n    loss &lt;- nnf_mse_loss(Y_pred, batch[[2]])\n    loss$backward()\n    opt$step()\n  })\n}\n\nvalid_loop &lt;- function(network, dl_valid) {\n  network$eval()\n  valid_loss &lt;- c()\n  coro::loop(for (batch in dl_valid) {\n    Y_pred &lt;- with_no_grad(network(batch[[1]]))\n    loss &lt;- sqrt(nnf_mse_loss(Y_pred, batch[[2]]))\n    valid_loss &lt;- c(valid_loss, loss$item())\n  })\n  mean(valid_loss)\n}\n\ntrain_valid &lt;- function(network, ds_train, ds_valid, epochs, batch_size, ...) {\n  opt &lt;- optim_ignite_adamw(network$parameters, ...)\n  train_losses &lt;- numeric(epochs)\n  valid_losses &lt;- numeric(epochs)\n  dl_train &lt;- dataloader(ds_train, batch_size = batch_size)\n  dl_valid &lt;- dataloader(ds_valid, batch_size = batch_size)\n  for (epoch in seq_len(epochs)) {\n    train_loop(network, dl_train, opt)\n    valid_losses[epoch] &lt;- valid_loop(network, dl_valid)\n  }\n  data.frame(epoch = seq_len(epochs), valid_loss = valid_losses)\n}\n\n\nIt takes as arguments:\n\nepochs: The number of epochs to train for. Defaults to 30.\nbatch_size: The batch size to use for training. Defaults to 16.\nlr: The learning rate to use for training. Defaults to 0.01.\nweight_decay: The weight decay to use for training. Defaults to 0.01.\nbeta1: The momentum parameter to use for training. Defaults to 0.9.\nbeta2: The adaptive step size parameter to use for training. Defaults to 0.999.\n\nYou can, e.g., call the function like below:\n\ncompare_configs(epochs = 30, lr = list(0.1, 0.2), weight_decay = 0.02)\n\n\n\n\n\n\n\n\nOne of the arguments (except for epochs) must be a list of values. The function will then run the same training configuration for each of the values in the list and visualize the results.\nExplore a few hyperparameter settings and make some observations as to how they affect the trajectory of the validation loss. The purpose of this exercise is not to derive observations that hold in general.\nQuestion 2: Optimization with Momentum\nIn this exercise, you will build a gradient descent optimizer with momentum. As a use case, we will minimize the Rosenbrock function. The function is defined as:\n\nrosenbrock &lt;- function(x, y) {\n  (1 - x)^2 + 2 * (y - x^2)^2\n}\nrosenbrock(torch_tensor(-1), torch_tensor(-1))\n\ntorch_tensor\n 12\n[ CPUFloatType{1} ]\n\n\nThe ‘parameters’ we will be optimizing is the position of a point (x, y), which will both be updated using gradient descent. The figure below shows the Rosenbrock function, where darker values indicate lower values.\n\n\n\n\n\n\n\n\n\nThe task is to implement the optim_step() function.\n\noptim_step &lt;- function(x, y, lr, x_momentum, y_momentum, beta) {\n  ...\n}\n\nIt will receive as arguments, the current values x and y, the momentum values x_momentum and y_momentum (all scalar tensors) as well as the learning rate lr and the momentum parameter beta. The function should then:\n\nPerform a forward pass.\nCalculate the gradients.\nUpdate the momentum values in-place.\nUpdate the parameters in-place.\n\n\n\nHint\n\nTo perform in-place updates, you can use the $mul_() and $add_() methods.\n\nThe update rule is given exemplarily for x:\n\\[\n\\begin{aligned}\nv_{t + 1} &= \\beta v_t + (1 - \\beta) \\nabla_{x} f(x_t, y_t) \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nx_{t+1} &= x_t - \\eta v_{t + 1}\n\\end{aligned}\n\\]\nTo test your optimizer, you can use the code below. Note that you might have to play around with the number of steps and the learning rate to get a good result.\n\n\nCode to test your optimizer\n\n\noptimize_rosenbrock &lt;- function(steps, lr, beta) {\n  x &lt;- torch_tensor(-1, requires_grad = TRUE)\n  y &lt;- torch_tensor(2, requires_grad = TRUE)\n  momentum_x &lt;- torch_tensor(0)\n  momentum_y &lt;- torch_tensor(0)\n\n  trajectory &lt;- data.frame(\n    x = numeric(steps + 1),\n    y = numeric(steps + 1),\n    value = numeric(steps + 1)\n  )\n  for (step in seq_len(steps)){\n    optim_step(x, y, lr, momentum_x, momentum_y, beta)\n    x$grad$zero_()\n    y$grad$zero_()\n    trajectory$x[step] &lt;- x$item()\n    trajectory$y[step] &lt;- y$item()\n  }\n  trajectory$x[steps + 1] &lt;- x$item()\n  trajectory$y[steps + 1] &lt;- y$item()\n\n  plot_rosenbrock() +\n    geom_path(data = trajectory, aes(x = x, y = y, z = NULL), color = \"red\") +\n    labs(title = \"Optimization Path on Rosenbrock Function\", x = \"x\", y = \"y\")\n}\n\n\nQuestion 3: Weight Decay\nIn exercise 2, we have optimized the Rosenbrock function. Does it make sense to also use weight decay here?"
  },
  {
    "objectID": "notebooks/4-optimizer.html",
    "href": "notebooks/4-optimizer.html",
    "title": "Optimization & Regularization",
    "section": "",
    "text": "In this notebook, we will focus on the optimization and regularization aspects of deep learning.\nOptimizers are algorithms that iteratively adjust the parameters of a neural network to minimize the loss function during training. They define how a network learns from data.\nLet’s denote \\(\\hat{\\mathcal{R}}(\\theta)\\) as the empirical risk function, which assigns the empirical risk given data \\(\\{(x^{(i)}, y^{(i)})\\}_{i = 1}^n\\) to a parameter vector \\(\\theta\\). Here, \\(f_\\theta\\) is the model’s prediction function, \\(x^{(i)}\\) is the \\(i\\)-th sample in the training data, and \\(y^{(i)}\\) is the corresponding target value. \\[\\hat{\\mathcal{R}}(\\theta) = \\frac{1}{n} \\sum_{i=1}^n L(f_\\theta(x^{(i)}), y^{(i)}) \\]\nOften, the empirical risk function is extended with a regularization term. Regularization in machine learning and statistics is used to prevent overfitting by adding a penalty term to the risk function, which discourages overly complex models that might fit noise in the training data. It helps improve generalization to unseen data. One common regularizer is the L2 norm of the parameter vector, which penalizes large coefficients by adding the squared magnitude of the coefficients to the loss function:\n\\[\n\\hat{\\mathcal{R}}_{\\text{reg}}(\\theta) = \\hat{\\mathcal{R}}(\\theta) + \\lambda \\sum_{j=1}^p \\theta_j^2\n\\]\nHere, \\(\\lambda\\) controls the strength of the regularization, i.e., the trade-off between fitting the training data and keeping the parameters small. This encourages the model to prefer less complex solutions, where complexity is measured by the L2 norm of the coefficients. As a result, parameter vectors will have entries closer to the zero vector, a concept known as parameter shrinkage.\nWhile the goal of the risk function is to define what we want, it’s the optimizer’s job to find the parameter vector \\(\\theta^*\\) that minimizes the empirical risk function. For simplicity, we will now refer to both the regularized and unregularized risk function as \\(\\hat{\\mathcal{R}}\\).\n\\[\\theta^* = \\arg \\min_\\theta \\hat{\\mathcal{R}}(\\theta)\\]\nThis is done by iteratively updating the parameter vector \\(\\theta\\) using the gradient of the risk function with respect to the parameter vector. The simplified update formula for a parameter \\(\\theta\\) at time step \\(t\\) is given by:\n\\[\\theta_{t+1} = \\theta_t - \\eta \\frac{\\partial \\hat{\\mathcal{R}}}{\\partial \\theta_t}\\]\nWhere:\nThe optimizers used in practice differ from the above formula, as:\nBefore we cover these more advanced approaches (specifically their implementation in AdamW), we will first focus on the vanilla version of Stochastic Gradient Descent (SGD)."
  },
  {
    "objectID": "notebooks/4-optimizer.html#mini-batch-effects-in-sgd",
    "href": "notebooks/4-optimizer.html#mini-batch-effects-in-sgd",
    "title": "Optimization & Regularization",
    "section": "Mini-Batch Effects in SGD",
    "text": "Mini-Batch Effects in SGD\nWhen using mini-batches, the gradient becomes a noisy estimate of the gradient over the full dataset. With \\(\\nabla_{\\theta} \\hat{\\mathcal{R}}^{(i)}_t := \\frac{\\partial \\hat{\\mathcal{R}}^{(i)}}{\\partial \\theta}\\) being the gradient of the risk function with respect to the entire parameter vector estimated using \\((x^{(i)}, y^{(i)})\\), the mini-batch gradient is given by:\n\\[\\nabla_{\\theta} \\hat{\\mathcal{R}}^B_t = \\frac{1}{|B|} \\sum_{i \\in B} \\nabla_{\\theta} \\hat{\\mathcal{R}}^{(i)}_t\\]\nwhere \\(B\\) is the batch of samples and \\(|B|\\) is the batch size.\nThe update formula for SGD is then given by:\n\\[\\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\theta} \\hat{\\mathcal{R}}^B_t\\]\nThe difference between the full gradient and the mini-batch gradient is visualized in the image below:\n\n\n\n\n\n\n\nQuiz: Vanilla SGD\n\n\n\nQuestion 1: What happens when the batch size is too small or too large?\n\n\nClick for answer\n\nTrade-offs with Batch Size:\n\nLarger batches provide more accurate gradient estimates.\nSmaller batches introduce more noise but allow more frequent parameter updates.\n\n\nQuestion 2: The mini-batch gradient is an approximation of the gradient over the full dataset. Does the latter also approximate something? If so, what?\n\n\nClick for answer\n\nIn machine learning, we assume that the data is drawn from a distribution \\(\\mathbb{E}\\). The gradient over the full dataset approximates the expectation over this distribution:\n\\[\\nabla_\\theta \\mathcal{R} = \\mathbb{E} [\\nabla_\\theta L(f_\\theta(x), y)]\\]\n\n\n\nBecause deep learning models can have many parameters and computing gradients is expensive, understanding the effects of different batch sizes and convergence is important. The computational cost (which we define as the time it takes to perform one optimization step) of a gradient update using a batch size \\(B\\) consists of:\n\nLoading the batch into memory (if the data does not fit into RAM).\nThe forward pass of the model.\nThe backward pass of the model.\nThe update of the parameters.\n\nWe will discuss point 1 later, and point 4 does not depend on the batch size, so we can ignore it.\n\n\n\n\n\n\nQuiz: Bang for Your Buck\n\n\n\nQuestion 1: True or false: The cost (duration) of performing a gradient update using a batch size of \\(2\\) is twice the cost of a batch size of \\(1\\).\n\n\nClick for answer\n\nFalse. Because GPUs can perform many operations simultaneously, the cost of performing a gradient update using a batch size of \\(2\\) is not twice the cost of a batch size of \\(1\\). The cost depends on many factors, but if the model is small, the cost of a batch with 2 observations might be almost the same as one with one observation.\n\nQuestion 2: The standard error of the mini-batch gradient estimate (which characterizes the precision of the gradient estimate) can be written as:\n\\[\\text{SE}_{\\nabla \\hat{\\mathcal{R}}^B_t} = \\frac{\\sigma_{\\nabla \\hat{\\mathcal{R}}_t}}{\\sqrt{|B|}}\\]\nwhere \\(\\sigma_{\\nabla \\hat{\\mathcal{R}}_t}\\) is the standard deviation of the gradient estimate using a single observation.\nDescribe the dynamics of the standard error when increasing the batch size: How do you need to increase a batch size from \\(1\\) to achieve half the standard error? What about increasing a batch size from \\(100\\)?\n\n\nClick for answer\n\nThe standard error decreases as the batch size increases, but with diminishing returns. To halve the standard error:\n\nIncrease the batch size from \\(1\\) to \\(4\\).\nIncrease the batch size from \\(100\\) to \\(400\\).\n\nThis is because the standard error is inversely proportional to the square root of the batch size."
  },
  {
    "objectID": "notebooks/4-optimizer.html#mini-batch-gradient-descent-its-not-all-about-runtime",
    "href": "notebooks/4-optimizer.html#mini-batch-gradient-descent-its-not-all-about-runtime",
    "title": "Optimization & Regularization",
    "section": "Mini-Batch Gradient Descent: It’s not all about runtime",
    "text": "Mini-Batch Gradient Descent: It’s not all about runtime\nAs we have now covered some of the dynamics of a simple gradient-based optimizer, we can examine the final parameter vector \\(\\theta^*\\) that the optimizer converges to. When using a gradient-based optimizer, the updates will stop once the gradient is close to zero. We will now discuss the type of solutions where this is true and their properties.\nWe need to distinguish saddle points from local minima from global minima:\n\nIn deep learning, where high-dimensional parameter spaces are common, saddle points are more likely to occur than local minima. However, due to the stochastic nature of SGD, optimizers will find local minima instead of saddle points.\n\n\n\n\n\n\nQuiz: Local vs. Global Minima, Generalization\n\n\n\nQuestion 1: Do you believe SGD will find local or global minima? Explain your reasoning.\n\n\nClick for answer\n\nBecause the gradient only has local information about the loss function, SGD finds local minima.\n\nQuestion 2: Assuming we have found a \\(\\theta^*\\) that has low training loss, does this ensure that we have found a good model?\n\n\nClick for answer\n\nNo, because we only know that the model has low training loss, but not necessarily low test loss.\n\n\n\nSGD has been empirically shown to find solutions that generalize well to unseen data. This phenomenon is attributed to the implicit regularization effects of SGD, where the noise introduced by mini-batch sampling helps guide the optimizer towards broader minima with smaller L2 norms. These broader minima are typically associated with better generalization performance compared to sharp minima.\n\nSource\nThese properties are also known as implicit regularization of SGD. Regularization generally refers to techniques that prevent overfitting and improve generalization. There are also explicit regularization techniques, which we will cover next.\n\nWeight Decay\nOne modification to the SGD update formula is the so-called weight decay, which is equivalent to adding a regularization penalty term to the loss function as we have seen earlier.\n\n\n\n\n\n\nNote\n\n\n\nFor more complex optimizers such as Adam, weight decay is not equivalent to adding a regularization penalty term to the loss function (Loshchilov 2017). However, the main idea of both approaches is still to shrink the weights to \\(0\\) during training.\n\n\nIf we integrate weight decay into the gradient update formula, we get the following:\n\\[\\theta_{t+1} = \\theta_t - \\eta \\big(\\nabla_{\\theta} \\hat{\\mathcal{R}}^B_t- \\lambda \\theta_t\\big)\\]\nThis formula shows that the weight decay term (\\(- \\lambda \\theta_t\\)) effectively shrinks the weights during each update, helping to prevent overfitting.\n\nSource\n\n\nMomentum\nMomentum is a technique that helps accelerate gradient descent by using an exponential moving average of past gradients. Like a ball rolling down a hill, momentum helps the optimizer:\n\nMove faster through areas of consistent gradient direction.\nPush through sharp local minima and saddle points.\nDampen oscillations in areas where the gradient frequently changes direction.\n\nThe exponential moving momentum update can be expressed mathematically as:\n\\[\n(1 - \\beta) \\sum_{\\tau=1}^{t} \\beta^{t-\\tau} \\nabla_{\\theta} \\hat{\\mathcal{R}}^B_{\\tau-1}\n\\]\nIn order to avoid having to keep track of all the gradients, we can calculate the update in two steps as follows:\n\\[\nv_t = \\beta_1 v_{t-1} + (1 - \\beta_1) \\nabla_\\theta \\hat{\\mathcal{R}}^B_t\n\\]\n\\[\n\\theta_{t+1} = \\theta_t - \\eta \\frac{v_t}{1 - \\beta_1^t}\n\\]\nThe hyperparameter \\(\\beta_1\\) is the momentum decay rate (typically 0.9), \\(v_t\\) is the exponential moving average of gradients, and \\(\\eta\\) is the learning rate as before. Note that dividing by \\(1 - \\beta_1^t\\) counteracts a bias because \\(v_0\\) is initialized to \\(0\\).\n\nSource\n\n\nAdaptive Learning Rates\nAdaptive learning rate methods automatically adjust the learning rate for each parameter during training. This is particularly useful because:\n\nDifferent parameters may require different learning rates.\nThe optimal learning rate often changes during training.\n\nBefore, we had one global learning rate \\(\\eta\\) for all parameters. However, learning rates are now allowed to:\n\nChange over time.\nBe different for different parameters.\n\nOur vanilla SGD update formula is now generalized to handle adaptive learning rates:\n\\[\\theta_{t+1} = \\theta_t - \\eta_t \\cdot \\nabla_\\theta \\frac{v_t}{1 - \\beta_1^t}\\]\nHere, \\(\\eta_t\\) is now not a scalar learning rate, but a vector of learning rates for each parameter, and ‘\\(\\cdot\\)’ denotes the element-wise multiplication. Further, \\(\\epsilon\\) is a small constant for numerical stability.\nIn AdamW, the adaptive learning rate is controlled by the second moment estimate (squared gradients):\n\\[g_t = \\beta_2 g_{t-1} + (1-\\beta_2)(\\nabla_\\theta \\hat{\\mathcal{R}}^B_t)^2\\] \\[\\hat{\\eta}_t = \\eta \\frac{1}{\\sqrt{g_t} + \\epsilon}\\]\nIn words, this means: In steep directions where the gradient is large, the learning rate is small and vice versa. The parameters \\(\\beta_2\\) and \\(\\epsilon\\) are hyperparameters that control the decay rate and numerical stability of the second moment estimate.\n\nWhen combining weight decay, adaptive learning rates, and momentum, we get the AdamW optimizer. It therefore has parameters:\n\nlr: The learning rate.\nweight_decay: The weight decay parameter.\nbetas: The momentum parameters (\\(\\beta_1\\) and \\(\\beta_2\\)).\neps: The numerical stability parameter.\n\nNote that AdamW also has another configuration parameter amsgrad, which is disabled by default in torch, but which can help with convergence."
  },
  {
    "objectID": "notebooks/4-optimizer.html#learning-rate-schedules",
    "href": "notebooks/4-optimizer.html#learning-rate-schedules",
    "title": "Optimization & Regularization",
    "section": "Learning Rate Schedules",
    "text": "Learning Rate Schedules\nWhile we have already covered dynamic learning rates, it can still be beneficial to use a learning rate scheduler to further improve convergence. Like for adaptive learning rates, the learning rate is then not a constant scalar, but a function of the current epoch or iteration. Note that the learning rate schedulers discussed here can also be combined with adaptive learning rates such as in AdamW and are not mutually exclusive.\nDecaying learning rates:\nThis includes gradient decay, cosine annealing, and cyclical learning rates. The general idea is to start with a high learning rate and then gradually decrease it over time.\nWarmup:\nWarmup is a technique that gradually increases the learning rate from a small value to a larger value over a specified number of epochs. This ensures that in the beginning, where the weights are randomly initialized, the learning rate is not too high.\nCyclical Learning Rates:\nCyclical learning rates are a technique that involves periodically increasing and decreasing the learning rate. This can help the optimizer to traverse saddle points faster and find better solutions.\nThe different schedules are visualized below:\n\n\n\n\n\n\n\n\n\nIn torch, learning rate schedulers are prefixed by lr_, such as the simple lr_step, where the learning rate is multiplied by a factor of gamma every step_size epochs. In order to use them, we need to pass the optimizer to the scheduler and specify additional arguments.\n\nscheduler = lr_step(opt, step_size = 2, gamma = 0.1)\n\nThe main API of a learning rate scheduler is the $step() method, which updates the learning rate. For some schedulers, this needs to be called after each optimization step, for others after each epoch. You can find this out by consulting the documentation of the specific scheduler.\n\nopt$param_groups[[1L]]$lr\n\n[1] 0.2\n\nscheduler$step()\nopt$param_groups[[1L]]$lr\n\n[1] 0.2\n\nscheduler$step()\nopt$param_groups[[1L]]$lr\n\n[1] 0.02"
  },
  {
    "objectID": "notebooks/4-optimizer.html#saving-an-optimizer",
    "href": "notebooks/4-optimizer.html#saving-an-optimizer",
    "title": "Optimization & Regularization",
    "section": "Saving an Optimizer",
    "text": "Saving an Optimizer\nIn order to resume training at a later stage, we can save the optimizer’s state which is accessible via $state_dict().\n\nstate_dict = opt$state_dict()\n\nThis state dictionary contains:\n\nThe $param_groups which contains the hyperparameters for each parameter group.\nThe $state which contains the optimizer’s internal state, such as the momentum and second moment estimates.\n\n\nstate_dict$param_groups[[1L]]\n\n$params\n[1] 1 2\n\n$lr\n[1] 0.02\n\n$betas\n[1] 0.900 0.999\n\n$eps\n[1] 1e-08\n\n$weight_decay\n[1] 0.01\n\n$amsgrad\n[1] FALSE\n\n$initial_lr\n[1] 0.2\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is possible to set different parameters (such as learning rate) for different parameter groups.\n\no2 = optim_adamw(list(\n  list(params = torch_tensor(1), lr = 1),\n  list(params = torch_tensor(2), lr = 2)\n))\no2$param_groups[[1L]]$lr\n\n[1] 1\n\no2$param_groups[[2L]]$lr\n\n[1] 2\n\n\n\n\nThe $state field contains the state for each parameter:\n\nstate_dict$state\n\n$`1`\n$`1`$step\ntorch_tensor\n1\n[ CPUFloatType{} ]\n\n$`1`$exp_avg\ntorch_tensor\n0.01 *\n 5.9926\n[ CPUFloatType{1,1} ]\n\n$`1`$exp_avg_sq\ntorch_tensor\n0.0001 *\n 3.5912\n[ CPUFloatType{1,1} ]\n\n\n$`2`\n$`2`$step\ntorch_tensor\n1\n[ CPUFloatType{} ]\n\n$`2`$exp_avg\ntorch_tensor\n0.01 *\n 2.1779\n[ CPUFloatType{1} ]\n\n$`2`$exp_avg_sq\ntorch_tensor\n1e-05 *\n 4.7433\n[ CPUFloatType{1} ]\n\n\nJust like for the nn_module, we can save the optimizer state using torch_save().\n\npth = tempfile(fileext = \".pth\")\ntorch_save(state_dict, pth)\n\n\n\n\n\n\n\nWarning\n\n\n\nGenerally, we don’t want to save the whole optimizer, as this also contains the weight tensors of the model that one usually wants to save separately.\n\n\nWe can load the optimizer state again using torch_load().\n\nstate_dict2 = torch_load(pth)\nopt2 &lt;- optim_adamw(model$parameters, lr = 0.2)\nopt2$load_state_dict(state_dict2)"
  },
  {
    "objectID": "notebooks/4-optimizer.html#embedded-regularization-techniques",
    "href": "notebooks/4-optimizer.html#embedded-regularization-techniques",
    "title": "Optimization & Regularization",
    "section": "Embedded Regularization Techniques",
    "text": "Embedded Regularization Techniques\nBesides the explicit regularization effects of weight decay and the implicit regularization effects of mini-batch gradient descent, there are also other regularization techniques that improve generalization of deep neural networks. Here, we focus on dropout and layer normalization, which are both embedded in neural network architectures.\n\nDropout\nDropout is a regularization technique used to prevent overfitting in neural networks. During each training iteration, dropout randomly “drops” a subset of neurons by setting their activations to zero with a specified probability. This forces the network to distribute the learned representations more evenly across neurons. Dropout is most commonly used in the context of fully connected layers.\n\n\n\n\n\nSource\nNote that neurons are only dropped when the module is in train mode, not in eval mode.\n\nx = torch_randn(10, 5)\ndropout = nn_dropout(p = 0.5)\ndropout(x)\n\ntorch_tensor\n-0.0000  3.7545  4.3769  0.0000  0.0000\n 0.4759  0.7142  1.8560 -1.8215  0.0000\n 0.0000  0.0000  0.8545 -1.1291 -0.0000\n-0.0383  2.3208 -0.3911  2.5771  0.0000\n 0.0000 -0.0000  0.0000  0.0000  0.0000\n 0.6956  0.0000 -0.0000  2.9860  0.2500\n-0.9978 -2.0387  0.0000  0.5153 -0.0000\n-0.0000  0.0000 -0.9363 -0.0000 -0.7703\n-0.9378 -0.2968  3.6971  1.6708 -0.0000\n-0.0000  1.2085  0.0000  0.0000 -2.8396\n[ CPUFloatType{10,5} ]\n\ndropout$eval()\ndropout(x)\n\ntorch_tensor\n-0.4076  1.8772  2.1884  0.3678  0.0293\n 0.2379  0.3571  0.9280 -0.9107  0.5313\n 0.1559  0.1716  0.4273 -0.5645 -0.1329\n-0.0192  1.1604 -0.1956  1.2885  0.3373\n 0.3247 -1.6603  1.3701  1.0716  0.2485\n 0.3478  2.6682 -0.4942  1.4930  0.1250\n-0.4989 -1.0193  0.3470  0.2577 -0.4072\n-0.4410  0.8347 -0.4681 -0.0240 -0.3851\n-0.4689 -0.1484  1.8486  0.8354 -1.1521\n-0.2673  0.6042  0.4407  0.5085 -1.4198\n[ CPUFloatType{10,5} ]\n\n\n\n\n\n\n\n\nQuiz: Dropout\n\n\n\nQuestion 1: Worse Training Loss: You are training a neural network with and without dropout. The training loss is higher with dropout, is this a bug?\n\n\nClick for answer\n\nNot necessarily, as dropout is a regularization technique that prevents overfitting. Its goal is to reduce the generalization performance of the model and not to improve training performance.\n\n\n\n\n\nBatch Normalization\nBatch Normalization is an important technique in deep learning that contributed significantly to speeding up the training process, especially in convolutional neural networks that are covered in the next chapter. During training, batch normalization introduces noise into the network by normalizing each mini-batch independently. Besides faster congerence, batch normalization also acts as a regularizer, where the model learns to be less sensitive to the specific details of the training data, thus reducing overfitting.\nThe formula for batch normalization (during training) is given by:\n\\[\n\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n\\]\nwhere:\n\n\\(\\hat{x}\\) is the normalized output,\n\\(x\\) is the input,\n\\(\\mu_B\\) is the mean of the batch,\n\\(\\sigma_B^2\\) is the variance of the batch,\n\\(\\epsilon\\) is a small constant added for numerical stability.\n\nDuring inference, the module uses the running mean and variance of the training data to normalize the input.\nIn torch, different versions of batch normalization exist for different dimensions of the input tensor. Below, we illustrate the batch normalization module using a 1D input tensor (the batch dimension does not count here):\n\nx = torch_randn(10, 5)\nbn = nn_batch_norm1d(num_features = 5)\nbn(x)\n\ntorch_tensor\n 1.2203  1.1179  1.1652 -1.3292  0.5830\n 0.6674 -1.1404 -0.1058  0.1933 -0.4474\n 0.2395  0.6881  1.0513 -1.2640 -0.2211\n-1.9849  0.2075 -0.3904  0.5686  1.6812\n-0.5494 -0.9593 -1.6051 -1.3635  0.3326\n 0.4905  1.1264  0.4455  0.8893 -0.3957\n-0.9948 -0.0520 -0.2290  0.1912 -1.1758\n-0.5657 -1.3184 -1.8173  1.1717 -1.8676\n-0.0116  1.3793  0.4786  1.4124  1.1464\n 1.4886 -1.0491  1.0071 -0.4698  0.3644\n[ CPUFloatType{10,5} ][ grad_fn = &lt;NativeBatchNormBackward0&gt; ]\n\n\n\n\n\n\n\n\nQuiz: Batch Normalization\n\n\n\nQuestion 1: Earlier we have learned that nn_modules have buffers and parameters, where only the latter are learned with gradient descent. Do you think the mean and variance are parameters or buffers?\n\n\nClick for answer\n\nThey are both buffers as they only store the variance and running mean of all training samples seen, i.e., they are not updated using gradient information.\n\nQuestion 2: Training vs. Evaluation Mode: While many nn_modules behave the same way irrespective of their mode, batch normalization is an example of a module that behaves differently during training and evaluation. During training, the module uses the mean and variance of the current batch, while during evaluation, it uses the running mean and variance of all training samples seen.\n\nbn(x[1:10, ])\n\ntorch_tensor\n 1.2203  1.1179  1.1652 -1.3292  0.5830\n 0.6674 -1.1404 -0.1058  0.1933 -0.4474\n 0.2395  0.6881  1.0513 -1.2640 -0.2211\n-1.9849  0.2075 -0.3904  0.5686  1.6812\n-0.5494 -0.9593 -1.6051 -1.3635  0.3326\n 0.4905  1.1264  0.4455  0.8893 -0.3957\n-0.9948 -0.0520 -0.2290  0.1912 -1.1758\n-0.5657 -1.3184 -1.8173  1.1717 -1.8676\n-0.0116  1.3793  0.4786  1.4124  1.1464\n 1.4886 -1.0491  1.0071 -0.4698  0.3644\n[ CPUFloatType{10,5} ][ grad_fn = &lt;NativeBatchNormBackward0&gt; ]\n\n\nWhich of the following statements is true and why?\n\nbn$eval()\nequal1 = torch_equal(\n  torch_cat(list(bn(x[1:2, ]), bn(x[3:4, ]))),\n  bn(x[1:4, ])\n)\nbn$train()\nequal2 = torch_equal(\n  torch_cat(list(bn(x[1:2, ]), bn(x[3:4, ]))),\n  bn(x[1:4, ])\n)\n\n\n\nClick for answer\n\n\nc(equal1, equal2)\n\n[1]  TRUE FALSE\n\n\nThe first statement is true because, in evaluation mode, the module uses the running mean and variance of all training samples seen. The second statement is false because the first tensor uses different means and variances for rows 1-2 and 3-4, while the second tensor uses the same mean and variance for all rows.\n\n\n\nTo demonstrate these dropout, we apply them to a simple spam classification task. The data has one binary target variable type (spam or no spam) and 57 numerical features.\n\nc(nrow(spam), ncol(spam))\n\n[1] 4601   58\n\ntable(spam$type)\n\n\n   spam nonspam \n   1813    2788 \n\n\nBelow, we create a simple neural network with two hidden layers of dimension 100, ReLU activation and optionally dropout and batch normalization.\n\nnn_reg &lt;- nn_module(\"nn_reg\",\n  initialize = function(dropout, batch_norm) {\n    self$net &lt;- nn_sequential(\n      nn_linear(in_features = 57, out_features = 100),\n      if (batch_norm) nn_batch_norm1d(num_features = 100) else nn_identity(),\n      if (dropout) nn_dropout(p = 0.5) else nn_identity(),\n      nn_relu(),\n      nn_linear(in_features = 100, out_features = 100),\n      if (batch_norm) nn_batch_norm1d(num_features = 100) else nn_identity(),\n      if (dropout) nn_dropout(p = 0.5) else nn_identity(),\n      nn_relu(),\n      nn_linear(in_features = 100, out_features = 2)\n    )\n  },\n  forward = function(x) {\n    self$net(x)\n  }\n)\nnn_drop &lt;- nn_reg(dropout = TRUE, batch_norm = FALSE)\nnn_batch &lt;- nn_reg(dropout = FALSE, batch_norm = TRUE)\nnn_both &lt;- nn_reg(dropout = TRUE, batch_norm = TRUE)\nnn_vanilla &lt;- nn_reg(dropout = FALSE, batch_norm = FALSE)\n\nWe evaluate the performance of the four neural networks created above using subsampling with 10 repetitions and an 80/20 train/test split. We don’t show the specific training code here, but only the resulting confidence intervals for the accuracy. While the intervals are too wide to be able to draw any final conclusions, both regularizaion techniques tend to lead to better results. Also, both normalization techniques improve stability, so that the confidence intervals are narrower."
  },
  {
    "objectID": "notebooks/5-cnn-exercise-task.html",
    "href": "notebooks/5-cnn-exercise-task.html",
    "title": "CNN Exercises",
    "section": "",
    "text": "Question 1: Manual Convolution\nIn this exercise, your task is to implement a function that performs a 2D convolution operation manually on a 3D input image using a given 3D kernel.\nThe input is a tensor with dimensions [channels, height, width] and the kernel is a tensor with dimensions [channels, kH, kW]. Your goal is to produce an output tensor of shape [height - kH + 1, width - kW + 1] by applying the convolution operation. Recall that output element is computed as the sum of the element-wise multiplication of the kernel and the corresponding patch of the input image.\n\nfconv2d &lt;- function(image, kernel, bias) {\n  ...\n}\n\nYou can use the code below to check your implementation against the conv2d module from the torch package.\n\nlibrary(torch)\nconv &lt;- nn_conv2d(3, 1, kernel_size = c(3, 3))\nkernel &lt;- conv$parameters$weight\nbias &lt;- conv$parameters$bias\n\ninput &lt;- torch_randn(1, 3, 28, 28)\ntorch_allclose(\n  fconv2d(input$squeeze(), kernel$squeeze(), bias$squeeze()),\n  conv(input),\n  atol = 1e-5\n)\n\n[1] TRUE\n\n\n\n\nHint\n\n\nAllocate a new empty tensor of the correct size to store the output.\nUsing a nested loop, iterate over each valid spatial location in the input and multiply the corresponding patch of the input with the kernel (torch_sum(patch * kernel)) and add the bias.\n\n\nQuestion 2: Be edgey\nConstruct a convolutional 2x2 kernel that extracts the edges of an image. Apply it using the fconv2d function from the previous exercise.\nAs an input, we use an image from MNIST. You can use the plot_2d_image function from the helper script to plot the image.\n\nlibrary(torchvision)\nsource(here::here(\"scripts/helper.R\"))\nmnist &lt;- mnist_dataset(root = \"data\")\nimage &lt;- mnist$.getitem(13)$x\nplot_2d_image(image)\n\n\n\n\n\n\n\n\nTo get started, use the code below and modify the values of the kernel.\n\nkernel &lt;- matrix(c(0.53, 0.34, 0.22, 0.1), byrow = TRUE, nrow = 2)\nkernel\n\n     [,1] [,2]\n[1,] 0.53 0.34\n[2,] 0.22 0.10\n\nkernel &lt;- torch_tensor(kernel)$unsqueeze(1)\n\nimageout &lt;- fconv2d(torch_tensor(image)$unsqueeze(1), kernel, 0)\nplot_2d_image(imageout$squeeze())"
  },
  {
    "objectID": "notebooks/5-cnn.html",
    "href": "notebooks/5-cnn.html",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "In this notebook, we explore convolutional neural networks (CNNs) used for image classification tasks. Image classification is fundamentally different from working with tabular data because images are highly structured and high-dimensional. For example, in an image, nearby pixels have strong spatial dependencies, whereas tabular data typically consists of independent or loosely related features, with each column representing a distinct attribute."
  },
  {
    "objectID": "notebooks/5-cnn.html#convolutional-layers",
    "href": "notebooks/5-cnn.html#convolutional-layers",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\nThe central component of a CNN is the convolutional layer. It functions by sliding a kernel over the input image, performing element-wise multiplication with the overlapping pixel values, and summing the results to produce a single output value for each kernel position.\n\nSource\nCNNs incorporate several strong inductive biases about visual data:\n\nLocality: Nearby pixels are more likely to be related than distant ones.\nTranslation Invariance: Features should be detected regardless of their position.\n\nThese biases make CNNs particularly effective for image-related tasks, as they align with our understanding of how visual information is structured.\nAs a first example, we will apply a convolutional layer to an image from MNIST—a benchmark dataset widely used in the machine learning community. MNIST comprises 28×28 grayscale images of handwritten digits (ranging from 0 to 9). The classification task is to assign the correct digit to each image.\n\n\n\n\n\n\n\n\n\nWhen working with these images as tensors, each is represented a 3D tensor with dimensions [1, 28, 28], where the first dimension are the number of channels (would be 3 for RGB images, but MNIST is grayscale), and the other two dimensions are the spatial dimensions (width and height) of the image.\n\nstr(image)\n\nFloat [1:1, 1:28, 1:28]\n\n\nSuch a convolutional layer has the following parameters:\n\nin_channels: The number of channels in the input image (e.g., 1 for grayscale images and 3 for RGB images).\nout_channels: The number of filters (or kernels) used by the layer. This determines the number of channels in the output feature map.\nkernel_size: The size of the filter that moves over the image. For instance, a kernel size of 3 means a 3×3 filter.\npadding: The number of pixels added to the borders of the input image. Padding can help control the spatial dimensions of the output.\nstride: The step size with which the filter moves across the input image. A larger stride results in a smaller output feature map.\n\nThe padding and the strides are visualized below:\n\nSource\nFor the input and output channels, we have the following:\n\nSource\nTo create a convolutional layer for a 2D image, we can use the torch::nn_conv2d function.\n\nconv_layer &lt;- nn_conv2d(in_channels = 1, out_channels = 1, kernel_size = 3, padding = 1, stride = 1)\nstr(conv_layer(image))\n\nFloat [1:1, 1:28, 1:28]\n\n\n\n\n\n\n\n\nParameters of a Convolutional Layer\n\n\n\nQuestion 1: Can you set the number of input channels of a convolutional layer freely?\n\n\nClick for answer\n\nNo, the number of input channels is determined by the number of channels of the input tensor.\n\nQuestion 2: Can you come up with a formula for the number of parameters of a convolutional layer? You can assume a symmetric kernel. Note that each kernel also has a bias term which is a scalar.\n\n\nClick for answer\n\nThe formula is out_channels * (kernel_size^2 * in_channels + 1).\n\nQuestion 3: We have an input image of shape (1, 28, 28) and we want to apply a fully connected layer and a convolutional layer that produces an output tensor with the same number of elements.\n\nA convolutional layer with 1 input channel and 1 ouput channel and a kernel of size 3x3 and padding of 1. (The output shape will therefore be (1, 28, 28).)\nA fully connected layer (that treats the input as a vector of dimension 28 * 28 = 784) that produces an output tensor with the same number of elements.\n\nHow many parameters does each layer have? (Recall that the linear layer also has a bias term.)\n\n\nClick for answer\n\n\n\\(1 * (3 * 3 * 1 + 1) = 10\\)\n\\(784 * (784 + 1) = 615440\\)\n\n\n\n\nBecause we have encoded more information about the structural relationship between the input tensor and the output tensor (the same filter is applied to the entire image), the convolutional layer has far fewer parameters than a fully connected layer.\n\nconv_layer$parameters\n\n$weight\ntorch_tensor\n(1,1,.,.) = \n -0.1359  0.0110 -0.1656\n  0.1257 -0.2840  0.2443\n -0.2423 -0.2650 -0.2106\n[ CPUFloatType{1,1,3,3} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n 0.1510\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n\nBelow, we show the output of the first convolutional layer from a (trained) ResNet18 model applied to an image from MNIST."
  },
  {
    "objectID": "notebooks/5-cnn.html#max-pooling",
    "href": "notebooks/5-cnn.html#max-pooling",
    "title": "Convolutional Neural Networks",
    "section": "Max Pooling",
    "text": "Max Pooling\nWhile convolutional layers extract local features from an image by applying a kernel over the input, max pooling is used to downsample the feature maps. Instead of applying a filter, max pooling simply partitions the input into non-overlapping (or sometimes overlapping) regions and selects the maximum value from each region.\n Source\nBelow, we demonstrate it in action. We start by applying a convolutional layer to an MNIST image to obtain feature maps and then apply a 2×2 max pooling layer (with stride 2) on the convolution output. The two results are visualized side-by-side.\n\n# Create a max pooling layer with a 2x2 kernel and stride 2\npool_layer &lt;- nn_max_pool2d(kernel_size = 2, stride = 2)\n\n# Now apply the max pooling layer to the result from the convolution.\npooled_output &lt;- pool_layer(image_rgb2[1, drop = FALSE]$unsqueeze(1))\n\n\n\n\n\n\n\nMax Pooling\n\n\n\nQuestion 1: How many parameters does a max pooling layer have?\n\n\nClick for answer\n\nA max pooling layer has no parameters.\n\nQuestion 2: When applying a max pooling layer to an image of shape (1, 28, 28) and a kernel size of 10x10, a stride of 1 and a padding of 0, what is the shape of the output?\n\n\nClick for answer\n\nThe output shape is (1, 19, 19)."
  },
  {
    "objectID": "notebooks/5-cnn.html#architecture-transfer-learning",
    "href": "notebooks/5-cnn.html#architecture-transfer-learning",
    "title": "Convolutional Neural Networks",
    "section": "Architecture & Transfer Learning",
    "text": "Architecture & Transfer Learning\nWhile we have no covered individual components of CNNs, the question of how to configure and compose them is a challenging task, but essential for building efficient neural networks. However, for many problems, there are predefined architectures that perform well and can be used. Unless there is a specific reason to design a new architecture, it is recommended to use such an architecture.\n\n\n\n\n\n\nNote\n\n\n\nBecause the Python deep learning ecosystem is so large, many more architectures are implemented in Python than in R. One way to use them in R is to simply translate the PyTorch code to (R-)torch. While PyTorch and (R-)torch are quite similar, there are some differences, e.g., 1-based and 0-based indexing. The torch website contains a brief tutorial on this topic.\n\n\nBeyond just using a predefined architecture, it is also possible to use transfer learning, which is a powerful technique in machine learning where a pre-trained model developed for a specific task is reused as the starting point for a model on a second, related task. Instead of training a model from scratch, which can be time-consuming and computationally expensive, transfer learning leverages the knowledge gained from a previously learned task to improve learning efficiency and performance on a new task.\nThe advantages of transfer learning are:\n\nReduced Training Time: Leveraging a pre-trained model can significantly decrease the time required to train a new model, as the foundational feature extraction layers are already optimized.\nImproved Performance: Transfer learning can enhance model performance, especially when the new task has limited training data. The pre-trained model’s knowledge helps in achieving better generalization.\nResource Efficiency: Utilizing pre-trained models reduces the computational resources needed, making it feasible to develop sophisticated models without extensive hardware.\n\nWhen the model is then trained on a new task, only the last layer is replaced with a new output layer to adjust for the new task.\nThis is visualized below:\n\nSource\nThe torchvision package offers various pretrained image networks that are available through the torchvision package. The ResNet-18 model is a well-known pre-trained model that was pretrained on ImageNet. It’s (complicated) architecture is shown below:\n\nBecause it’s architecture is quite complex, we only visualize it below, but don’t define it from scratch.\nSource\nWe can use the pretrained weights by setting the pretrained parameter to TRUE and specify the number of classes of our new task via the num_classes parameter (10 for MNIST).\n\nlibrary(torchvision)\nresnet &lt;- model_resnet18(pretrained = FALSE, num_classes = 10)\nresnet_pretrained &lt;- model_resnet18(pretrained = TRUE)\nresnet_pretrained$fc &lt;- nn_linear(512, 10)\n\nresnet_pretrained\n\nAn `nn_module` containing 11,181,642 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────────────────────────────────────────────\n• conv1: &lt;nn_conv2d&gt; #9,408 parameters\n• bn1: &lt;nn_batch_norm2d&gt; #128 parameters\n• relu: &lt;nn_relu&gt; #0 parameters\n• maxpool: &lt;nn_max_pool2d&gt; #0 parameters\n• layer1: &lt;nn_sequential&gt; #147,968 parameters\n• layer2: &lt;nn_sequential&gt; #525,568 parameters\n• layer3: &lt;nn_sequential&gt; #2,099,712 parameters\n• layer4: &lt;nn_sequential&gt; #8,393,728 parameters\n• avgpool: &lt;nn_adaptive_avg_pool2d&gt; #0 parameters\n• fc: &lt;nn_linear&gt; #5,130 parameters\n\n\nWhen fine-tuning a pretrained model like ResNet-18, it’s common to observe instabilities in gradients, which can manifest as fluctuating validation performance.\nTo fine-tune this model on MNIST, we need to also have a dataloader. In the case of MNIST, a predefined dataloader is available in the torchvision package. We transform both the input and the target to tensors (instead of R arrays).\n\nmnist_ds &lt;- mnist_dataset(root = \"data\", download = TRUE,\n  transform = torch_tensor, target_transform = torch_tensor)\nmnist_ds\n\n&lt;mnist&gt;\n  Inherits from: &lt;dataset&gt;\n  Public:\n    .getitem: function (index) \n    .length: function () \n    check_exists: function () \n    classes: 0 - zero 1 - one 2 - two 3 - three 4 - four 5 - five 6 - ...\n    clone: function (deep = FALSE) \n    data: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  ...\n    download: function () \n    initialize: function (root, train = TRUE, transform = NULL, target_transform = NULL, \n    load_state_dict: function (x, ..., .refer_to_state_dict = FALSE) \n    processed_folder: active binding\n    raw_folder: active binding\n    resources: list\n    root_path: data\n    state_dict: function () \n    target_transform: function (data, dtype = NULL, device = NULL, requires_grad = FALSE, \n    targets: 6 1 5 2 10 3 2 4 2 5 4 6 4 7 2 8 3 9 7 10 5 1 10 2 2 3 5 ...\n    test_file: test.rds\n    train: TRUE\n    training_file: training.rds\n    transform: function (data, dtype = NULL, device = NULL, requires_grad = FALSE, \n\n\nWe can inspect the first element of the dataset.\n\nbatch &lt;- mnist_ds[1:2]\nstr(batch)\n\nList of 2\n $ x:Long [1:2, 1:28, 1:28]\n $ y:Long [1:2]\n\n\nIn order to be able to fine-tune the pretrained model on MNIST, we need to make sure that the format of the input data is compatible with the pretrained model:\n\nThe size of the training images of ResNet-18 were 224x224, while MNIST images are 28x28. We therefore need to resize them.\nResNet-18 was pretrained on ImageNet, which uses RGB images (3 input channels), while MNIST is grayscale (1 input channel).\nThe training images of ResNet-18 were first transformed to be in the range of \\([0, 1]\\) and then normalized to have a mean of \\([0.485, 0.456, 0.406]\\) and a standard deviation of \\([0.229, 0.224, 0.225]\\). Our MNIST images are integer values in the range of \\([0, 255]\\) so we need to apply both transformations.\n\nWe can address this my modifying the transformation from earlier. If we were to implement our own dataset, this would simply be part of the $.getitem() or $.getbatch() method.\n\ntransform_mnist &lt;- function(x) {\n  x &lt;- torch_tensor(x) / 255\n  x &lt;- x$unsqueeze(1)\n  x &lt;- x$expand(c(3, 28, 28))\n  x &lt;- transform_normalize(x, mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225))\n  x &lt;- transform_resize(x, c(224, 224))\n  x\n}\nmnist_train &lt;- mnist_dataset(root = \"data\", download = TRUE, train = TRUE,\n  transform = transform_mnist, target_transform = torch_tensor\n)\nmnist_test &lt;- mnist_dataset(root = \"data\", download = TRUE, train = FALSE,\n  transform = transform_mnist, target_transform = torch_tensor)\n\nWe can now compare the pretrained and untrained resnet on the training task and evaluate it on the validation data. The results are shown below:\nWhen fine-tuning a pretrained model like ResNet-18, it’s common to observe instabilities in gradients, which can manifest as fluctuating validation performance.\nTo address this, one can for example freeze the pretrained layers (for some epochs) and only train the new output head, a process known as freezing layers.\n\n\n\n\n\n\nIn-Context Learning\n\n\n\nLarge foundation models (such as GPT-4) even allow performing tasks on which they were not pretrained on without any finetuning. This is referred to as in-context learning or zero-shot learning. There, the task is fed into the model during inference: “Hey ChatGPT, is What is the sentiment of this sentence. Return -1 for sad, 0 for neutral, 1 for happy: ”"
  },
  {
    "objectID": "notebooks/6-mlr3torch-exercise-task.html",
    "href": "notebooks/6-mlr3torch-exercise-task.html",
    "title": "Training Neural Networks with mlr3torch",
    "section": "",
    "text": "Question 1: Hello World!\nIn this exercise, you will train your first neural network with mlr3torch.\nAs a task, we will use the ‘Indian Liver Patient’ dataset where the goal is to predict whether a patient has liver disease or not.\n\nlibrary(mlr3verse)\nlibrary(mlr3torch)\nilpd &lt;- tsk(\"ilpd\")\nilpd\n\n&lt;TaskClassif:ilpd&gt; (583 x 11): Indian Liver Patient Data\n* Target: diseased\n* Properties: twoclass\n* Features (10):\n  - dbl (5): albumin, albumin_globulin_ratio, direct_bilirubin, total_bilirubin, total_protein\n  - int (4): age, alanine_transaminase, alkaline_phosphatase, aspartate_transaminase\n  - fct (1): gender\n\nautoplot(ilpd)\n\n\n\n\n\n\n\n\nWe remove the gender column from the task, so we need to only deal with numeric features for now.\n\nilpd_num = ilpd$clone(deep = TRUE)\nilpd_num$select(setdiff(ilpd_num$feature_names, \"gender\"))\n\nYour task is to train a simple multi layer perceptron (lrn(\"classif.mlp\")) with 2 hidden layers with 100 neurons each. Set the batch size to 32, the learning rate to 0.001 and the number of epochs to 20. Then, resample the learner on the task with a cross-validation with 5 folds and evaluate the results using classification error and false positive rate (FPR). Is the result good?\n\n\nHint\n\nThe parameter for the learning rate is opt.lr\n\nQuestion 2: Preprocessing\nIn the previous question, we have operated on the ilpd_num task where we excluded the categorical gender column. This was done because the MLP learner operates on numeric features only. You will now create a more complex GraphLearner that also incudes one-hot encoding of the gender column before applying the MLP. Resample this learner on the original ilpd task and evaluate the results using the same measures as before.\n\n\nHint\n\nConcatenate po(\"encode\") with a lrn(\"classif.mlp\") using %&gt;&gt;% to create the GraphLearner. For available options on the encoding, see po(\"encode\")$help().\n\nQuestion 3: Benchmarking\nInstead of resampling a single learner, the goal is now to compare the performance of the MLP with a simple classification tree. Create a benchmark design and compare the performance of the two learners.\n\n\nHint\n\nCreate a classification tree via lrn(\"classif.rpart\"). A benchmark design can be created via benchmark_grid(). To run a benchmark, pass the design to benchmark().\n\nQuestion 4: Iris as a Lazy Tensor\nCreate a version of the iris task where the 4 features are represented as a single lazy tensor column.\nQuestion 5: Custom Architecture\nCreate a network with one hidden layer with 100 neurons and a sigmoid activation function by assempling PipeOps in a Graph. Convert the Graph to a Learner and train the network for 10 epochs using Adam with a learning rate of 0.001 and a batch size of 32."
  },
  {
    "objectID": "notebooks/6-mlr3torch.html",
    "href": "notebooks/6-mlr3torch.html",
    "title": "Training Neural Networks with mlr3torch",
    "section": "",
    "text": "Why Use mlr3torch?\nmlr3torch is a package that extends the mlr3 framework with deep learning capabilities, allowing the application of deep learning techniques to both tabular and non-tabular data. The package implements many routines common in deep learning and allows users to focus on the actual problem at hand. Some advantages of using mlr3torch over ‘only’ torch are:\nHowever, as mlr3torch is a framework, it is less flexible than torch itself, so knowing both is recommended. Another helpful R package that provides many useful functions to train neural networks is luz."
  },
  {
    "objectID": "notebooks/6-mlr3torch.html#mlr3-recap",
    "href": "notebooks/6-mlr3torch.html#mlr3-recap",
    "title": "Training Neural Networks with mlr3torch",
    "section": "mlr3 Recap",
    "text": "mlr3 Recap\nBefore diving into mlr3torch, we will briefly review the core building blocks of the mlr3 machine learning framework. For reference, we recommend the mlr3 book that explains the mlr3 framework in more detail. Additionally, the mlr3 website contains more tutorials and overviews.\n\nTask\nA task is a machine learning problem on a dataset. It consists of the data itself and some metadata such as the features or the target variable. To create an example task that comes with mlr3, we can use the tsk() function:\n\nlibrary(mlr3)\ntsk(\"iris\")\n\n&lt;TaskClassif:iris&gt; (150 x 5): Iris Flowers\n* Target: Species\n* Properties: multiclass\n* Features (4):\n  - dbl (4): Petal.Length, Petal.Width, Sepal.Length, Sepal.Width\n\n\nTo create a custom Task from a data.frame, we can use the as_task_&lt;type&gt; converters:\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ntsk_iris &lt;- as_task_classif(iris, id = \"iris\", target = \"Species\")\ntsk_iris\n\n&lt;TaskClassif:iris&gt; (150 x 5)\n* Target: Species\n* Properties: multiclass\n* Features (4):\n  - dbl (4): Petal.Length, Petal.Width, Sepal.Length, Sepal.Width\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo get the help page for an mlr3 object, you can call tsk_iris$help().\n\n\nYou can access the data of a task using the $data() method, which accepts arguments rows and cols to select specific rows and columns.\n\ntsk_iris$data(rows = 1:5, cols = c(\"Sepal.Length\", \"Sepal.Width\"))\n\n   Sepal.Length Sepal.Width\n          &lt;num&gt;       &lt;num&gt;\n1:          5.1         3.5\n2:          4.9         3.0\n3:          4.7         3.2\n4:          4.6         3.1\n5:          5.0         3.6\n\n\nUsing the mlr3viz extension, we can get an overview of the target distribution:\n\nlibrary(mlr3viz)\nautoplot(tsk_iris)\n\n\n\n\n\n\n\n\n\n\nLearner\nA learner is a machine learning algorithm that can be $train()ed on a Task and be used to make $predict()ions on it. An overview of all learners is shown on our website. We can construct one by passing the identifier of the learner to the lrn() function.\n\nlrn_tree &lt;- lrn(\"classif.rpart\")\n\nNext, we need to split the data into a training and test set and apply the learner on the former.\n\nsplit &lt;- partition(tsk_iris, ratio = 0.8)\nlrn_tree$train(tsk_iris, row_ids = split$train)\n\nThe trained model can be accessed via the $model slot of the learner:\n\nprint(lrn_tree$model)\n\nn= 120 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 120 75 virginica (0.33333333 0.29166667 0.37500000)  \n  2) Petal.Length&lt; 2.45 40  0 setosa (1.00000000 0.00000000 0.00000000) *\n  3) Petal.Length&gt;=2.45 80 35 virginica (0.00000000 0.43750000 0.56250000)  \n    6) Petal.Length&lt; 4.75 32  1 versicolor (0.00000000 0.96875000 0.03125000) *\n    7) Petal.Length&gt;=4.75 48  4 virginica (0.00000000 0.08333333 0.91666667) *\n\n\nTo make predictions on the test set, we can use the $predict() method of the learner:\n\npredictions &lt;- lrn_tree$predict(tsk_iris, row_ids = split$test)\n\nTo make predictions on data.frames, we can use the $predict_newdata() method of the learner:\n\nnew_data &lt;- iris[1:2, ]\nlrn_tree$predict_newdata(new_data)\n\n&lt;PredictionClassif&gt; for 2 observations:\n row_ids  truth response\n       1 setosa   setosa\n       2 setosa   setosa\n\n\n\n\nPerformance Evaluation\nTo assess the quality of the predictions, we can use a Measure. mlr3 comes with many predefined measures, and we can construct them by passing the name of the measure to the msr() function. Below, we construct the mean classification accuracy measure – which can only be applied to classification tasks – and use it to evaluate the predictions.\n\nacc &lt;- msr(\"classif.acc\")\npredictions$score(acc)\n\nclassif.acc \n  0.9333333 \n\n\nFor more elaborate evaluation strategies, we can use rsmp() to define a Resampling strategy that can be executed using resample().\n\nrsmp_cv &lt;- rsmp(\"cv\", folds = 3)\n\nrr &lt;- resample(\n  task       = tsk_iris,\n  learner    = lrn_tree,\n  resampling = rsmp_cv\n)\n\n# Aggregate the results\nrr$aggregate(msr(\"classif.acc\"))\n\nclassif.acc \n  0.9466667 \n\n\n\n\nHyperparameter Tuning\nHyperparameter tuning is an essential process in machine learning to optimize the performance of models by selecting the best combination of hyperparameters. In the mlr3 framework, hyperparameter tuning is facilitated by the mlr3tuning extension.\nWe will now demonstrate how to tune the hyperparameters of the classif.rpart learner.\n\nDefine the Search Space: Specify the range and distribution of hyperparameters to explore.\n\nlibrary(mlr3tuning)\nlrn_tree$configure(\n  cp = to_tune(lower = 0.001, upper = 0.1),\n  maxdepth = to_tune(lower = 1, upper = 30)\n)\n\nChoose a Resampling Strategy: Determine how to evaluate each hyperparameter configuration’s performance.\n\nrsmp_tune &lt;- rsmp(\"cv\", folds = 3)\n\nSelect a Tuner: Decide on the algorithm that will search through the hyperparameter space.\n\ntuner &lt;- tnr(\"random_search\")\n\nSelect a Measure: Define the metric to optimize during tuning.\n\nmsr_tune &lt;- msr(\"classif.acc\")\n\nExecute Tuning: Run the tuning process to find the optimal hyperparameters. Here we also specify our budget of 10 evaluations.\n\ntune_result &lt;- tune(\n  task = tsk_iris,\n  learner = lrn_tree,\n  resampling = rsmp_tune,\n  measure = msr_tune,\n  tuner = tuner,\n  term_evals = 10L\n)\n\nApply the Best Hyperparameters: Update the learner with the best-found hyperparameters and retrain the model.\n\nlrn_tree$param_set$values &lt;- tune_result$result_learner_param_vals\nlrn_tree$train(tsk_iris)\n\n\n\n\n\n\n\n\nQuiz: Tuning Performance\n\n\n\nQuestion 1: Estimating the performance of a tuned model:\nThrough the tuning archive, we can access the performance of the best-found hyperparameter configuration.\n\ntune_result$archive$data[order(classif.acc, decreasing = TRUE), ][1, classif.acc]\n\n[1] 0.94\n\n\nDo you think this is a good estimate for the performance of the final model? Explain your answer.\n\n\nClick for answer\n\nOne reason why we would expect the performance of the final model to be worse than the performance of the best-found hyperparameter configuration is due to optimization bias: We choose the model configuration with the highest validation performance. This selection process biases the result since the chosen model is the best among several trials that are all affected by randomness.\n\n\n\nThese two steps can also be encapsulated in the AutoTuner class, which first finds the best hyperparameters and then trains the model with them.\n\nat &lt;- auto_tuner(\n  learner = lrn_tree,\n  resampling = rsmp_tune,\n  measure = msr_tune,\n  term_evals = 10L,\n  tuner = tuner\n)\n\nThe AutoTuner can be used just like any other Learner. To get a valid performance estimate of the tuning process, we can resample() it on the task. This is called nested resampling: the outer resampling is for evaluation and the inner resampling is for tuning.\n\nrr &lt;- resample(tsk_iris, at, rsmp_tune)\nrr$aggregate(acc)\n\nclassif.acc \n       0.94 \n\n\n\n\nLearning Pipelines\nIn many cases, we don’t only fit a single learner but a whole learning pipeline. Common use cases include the preprocessing of the data, e.g., for imputing missing values, scaling the data, or encoding categorical features, but many other operations are possible. The mlr3 extension mlr3pipelines is a toolbox for defining such learning pipelines. Its core building block is the PipeOp that can be constructed using the po() function.\n\nlibrary(mlr3pipelines)\npca &lt;- po(\"pca\")\n\nJust like a learner, it has a $train() and $predict() method, and we can apply it to a Task using these methods.\n\npca$train(list(tsk_iris))\n\n$output\n&lt;TaskClassif:iris&gt; (150 x 5)\n* Target: Species\n* Properties: multiclass\n* Features (4):\n  - dbl (4): PC1, PC2, PC3, PC4\n\npca$predict(list(tsk_iris))[[1L]]\n\n&lt;TaskClassif:iris&gt; (150 x 5)\n* Target: Species\n* Properties: multiclass\n* Features (4):\n  - dbl (4): PC1, PC2, PC3, PC4\n\n\nUsually, such PipeOps are combined with a Learner into a full learning Graph. This is possible using the %&gt;&gt;% chain operator.\n\nlibrary(mlr3pipelines)\ngraph &lt;- po(\"pca\") %&gt;&gt;% lrn(\"classif.rpart\")\nprint(graph)\n\nGraph with 2 PipeOps:\n            ID         State      sccssors prdcssors\n        &lt;char&gt;        &lt;char&gt;        &lt;char&gt;    &lt;char&gt;\n           pca &lt;&lt;UNTRAINED&gt;&gt; classif.rpart          \n classif.rpart &lt;&lt;UNTRAINED&gt;&gt;                     pca\n\ngraph$plot(horizontal = TRUE)\n\n\n\n\n\n\n\n\nThe resulting Graph can be converted back into a Learner using the as_learner() function and used just like any other Learner.\n\nglrn &lt;- as_learner(graph)\nglrn$train(tsk_iris)"
  },
  {
    "objectID": "notebooks/6-mlr3torch.html#brief-introduction-to-mlr3torch",
    "href": "notebooks/6-mlr3torch.html#brief-introduction-to-mlr3torch",
    "title": "Training Neural Networks with mlr3torch",
    "section": "Brief Introduction to mlr3torch",
    "text": "Brief Introduction to mlr3torch\nmlr3torch builds upon the same components as mlr3, only that we use Deep Learners, and can also work on non-tabular data. A simple example learner is the lrn(\"classif.mlp\") learner, which is a Multi-Layer Perceptron (MLP) for classification tasks.\n\nUsing a Predefined Torch Learner\n\nlibrary(mlr3torch)\nlrn_mlp &lt;- lrn(\"classif.mlp\",\n  neurons = c(50, 50),\n  batch_size = 256,\n  epochs = 30, # Number of training epochs\n  device = \"auto\", # Uses GPU if available, otherwise CPU\n  shuffle = TRUE, # because iris is sorted\n  optimizer = t_opt(\"adamw\"),\n  loss = t_loss(\"cross_entropy\")\n)\n\nThis MLP can be used just like the classification tree above.\n\nlrn_mlp$train(tsk_iris, row_ids = split$train)\n\nThe trained nn_module can be accessed via the $model slot of the learner:\n\nlrn_mlp$model$network\n\nAn `nn_module` containing 2,953 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────────────────────────────────────────────\n• 0: &lt;nn_linear&gt; #250 parameters\n• 1: &lt;nn_relu&gt; #0 parameters\n• 2: &lt;nn_dropout&gt; #0 parameters\n• 3: &lt;nn_linear&gt; #2,550 parameters\n• 4: &lt;nn_relu&gt; #0 parameters\n• 5: &lt;nn_dropout&gt; #0 parameters\n• 6: &lt;nn_linear&gt; #153 parameters\n\n\nBesides the trained network, the $model of the learner also contains the $state_dict() of the optimizer and other information.\nHaving trained the neural network on the training set, we can now make predictions on the test set:\n\npredictions &lt;- lrn_mlp$predict(tsk_iris, row_ids = split$test)\npredictions$score(msr(\"classif.acc\"))\n\nclassif.acc \n        0.5 \n\n\nUsing the benchmarking facilities of mlr3, we can also easily compare the classification tree with our MLP:\n\nrsmp_cv &lt;- rsmp(\"cv\", folds = 3)\n\nbenchmark_grid &lt;- benchmark_grid(\n  tasks = tsk_iris,\n  learners = list(lrn_tree, lrn_mlp),\n  resampling = rsmp_cv\n)\n\nrr_benchmark &lt;- benchmark(benchmark_grid)\n\nresults_benchmark &lt;- rr_benchmark$aggregate(msr(\"classif.acc\"))\n\nresults_benchmark\n\n      nr task_id    learner_id resampling_id iters classif.acc\n   &lt;int&gt;  &lt;char&gt;        &lt;char&gt;        &lt;char&gt; &lt;int&gt;       &lt;num&gt;\n1:     1    iris classif.rpart            cv     3   0.9533333\n2:     2    iris   classif.mlp            cv     3   0.6666667\nHidden columns: resample_result\n\n\n\n\nValidation Performance\nTracking validation performance is crucial for understanding how well your neural network is learning and to detect issues such as overfitting. In the mlr3 machine learning framework, this can be easily done by specifying the $validate field of a Learner. Note that this is not possible for all Learners, but only for those with the \"validation\" property. This includes boosting algorithms such as XGBoost or CatBoost, and also the mlr3torch learners.\nBelow, we set the validation ratio to 30% of the training data, specify the measures to record, and set the callbacks of the learner to record the history of the training process.\n\nlrn_mlp$configure(\n  validate = 0.3,\n  callbacks = t_clbk(\"history\"),\n  predict_type = \"prob\",\n  measures_valid = msr(\"classif.logloss\"),\n  measures_train = msr(\"classif.logloss\")\n)\n\n\n\n\n\n\n\nTip\n\n\n\nThe $configure() method of a Learner allows you to simultaneously set fields and hyperparameters of a learner.\n\n\nWhen we now train the learner, 30% of the training data is used for validation, and the loss is recorded in the history of the learner.\n\nlrn_mlp$train(tsk_iris)\n\nAfter training, the results of the callback can be accessed via the model.\n\nhead(lrn_mlp$model$callbacks$history)\n\nKey: &lt;epoch&gt;\n   epoch train.classif.logloss valid.classif.logloss\n   &lt;num&gt;                 &lt;num&gt;                 &lt;num&gt;\n1:     1              1.391005              1.186262\n2:     2              1.418184              1.142519\n3:     3              1.185711              1.108973\n4:     4              1.391693              1.080590\n5:     5              1.200895              1.056801\n6:     6              1.172990              1.038634\n\n\nAdditionally, the final validation scores can be accessed via the $internal_valid_scores field of the learner.\n\nlrn_mlp$internal_valid_scores\n\n$classif.logloss\n[1] 0.8735379\n\n\n\n\nDefining a Custom Torch Learner\nmlr3torch also allows defining custom architectures by assembling special PipeOps in a Graph. As a starting point in the graph, we need to mark the entry of the Neural Network using an ingress pipeop. Because we are working with a task with only one numeric feature, we can use po(\"torch_ingress_num\"). There also exist inputs for categorical features (po(\"torch_ingress_cat\")) and generic tensors (po(\"torch_ingress_ltnsr\")).\n\ningress &lt;- po(\"torch_ingress_num\")\n\nThe next steps in the graph are the actual layers of the neural network.\n\narchitecture &lt;- po(\"nn_linear_1\", out_features = 100) %&gt;&gt;%\n  po(\"nn_relu_1\") %&gt;&gt;%\n  po(\"nn_linear_2\", out_features = 100) %&gt;&gt;%\n  po(\"nn_relu_2\") %&gt;&gt;%\n  po(\"nn_head\")\n\narchitecture$plot(horizontal = TRUE)\n\n\n\n\n\n\n\n\nAfter specifying the architecture, we need to set the remaining parts for the learner, which are the loss, optimizer, and the remaining training configuration such as the epochs, device, or the batch size.\n\ngraph &lt;- ingress %&gt;&gt;% architecture %&gt;&gt;%\n  po(\"torch_loss\", loss = \"cross_entropy\") %&gt;&gt;%\n  po(\"torch_optimizer\", optimizer = t_opt(\"adamw\")) %&gt;&gt;%\n  po(\"torch_model_classif\", epochs = 10, batch_size = 256)\n\nJust like before, we can convert the graph into a Learner using as_learner() and train it on the task:\n\nglrn &lt;- as_learner(graph)\nglrn$train(tsk_iris, row_ids = split$train)\n\n\n\nWorking with Non-Tabular Data\nIn the mlr3 ecosystem, the data of a task is always stored in a data.frame or data.table. To work with non-tabular data, the mlr3torch package offers a custom datatype, the lazy_tensor, which can be stored in a data.table.\nAs an example to showcase this, we can use the CIFAR-10 dataset, which is a dataset of 60,000 32x32 color images in 10 classes, with 6,000 images per class.\n\ntsk_cifar &lt;- tsk(\"cifar10\")\ntsk_cifar\n\n&lt;TaskClassif:cifar10&gt; (60000 x 2): CIFAR-10 Classification\n* Target: class\n* Properties: multiclass\n* Features (1):\n  - lt (1): image\n\n\nThe image below shows some examples from the dataset:\n\n\n\n\n\n\n\nTip\n\n\n\nTo avoid having to re-download the dataset every time, you can set the mlr3torch.cache option to TRUE.\n\noptions(mlr3torch.cache = TRUE)\n\n\n\nWhen accessing the data, only the images are represented as lazy_tensors, the labels are still stored as a factor column:\n\ntsk_cifar$head()\n\n        class           image\n       &lt;fctr&gt;   &lt;lazy_tensor&gt;\n1:       frog &lt;tnsr[3x32x32]&gt;\n2:      truck &lt;tnsr[3x32x32]&gt;\n3:      truck &lt;tnsr[3x32x32]&gt;\n4:       deer &lt;tnsr[3x32x32]&gt;\n5: automobile &lt;tnsr[3x32x32]&gt;\n6: automobile &lt;tnsr[3x32x32]&gt;\n\n\nA lazy_tensor column is built upon torch::dataset class that we have seen earlier. This means it does not necessarily store the data in memory, but only stores the information on how to load the data.\n\nimage_vec &lt;- tsk_cifar$data(cols = \"image\")[[1L]]\nhead(image_vec)\n\n&lt;ltnsr[6]&gt;\n[1] &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt;\n\n\nTo access the data as torch_tensors, we can call the materialize() function. We see that the underlying tensors are integer-valued.\n\nimage_tensor &lt;- materialize(image_vec)[[1L]]\nstr(image_tensor)\n\nLong [1:3, 1:32, 1:32]\n\n\nTo construct the CIFAR-10 task ourselves, we need to first:\n\nConstruct a torch::dataset that returns the images as torch_tensors.\nCreate a factor() vector that contains the labels.\n\nWe can access the CIFAR-10 dataset via the torchvision package. For simplicity, we will only load the training data.\n\ncifar10_original &lt;- torchvision::cifar10_dataset(root = \"data/cifar10\", train = TRUE, download = TRUE)\nimage_array &lt;- cifar10_original$x\nstr(image_array)\n\n int [1:50000, 1:32, 1:32, 1:3] 59 154 255 28 170 159 164 28 134 125 ...\n\n\nThe array contains 50,000 images (rows) of shape 32x32x3. The last dimension contains the channels, i.e., the RGB values of the pixels. We reshape this so the channel dimension is the first dimension, which is the standard format for images in torch.\n\nimage_array &lt;- aperm(image_array, c(1, 4, 2, 3))\ndim(image_array)\n\n[1] 50000     3    32    32\n\n\nNext, we convert it to doubles in the range [0, 1] by dividing by 255.\n\nimage_array &lt;- image_array / 255\n\nNext, we create a torch::dataset() that loads individual images as a torch_tensor. To convert this to a lazy_tensor later, the $.getitem() method needs to return a named list.\n\ndataset_cifar10 &lt;- dataset(\"cifar10\",\n  initialize = function(x) {\n    self$x &lt;- x\n  },\n  .getitem = function(i) {\n    list(image = torch_tensor(self$x[i, , , ]))\n  },\n  .length = function() {\n    nrow(self$x)\n  }\n)\n\nThe above object is not yet a dataset, but a dataset constructor, so we create the actual dataset by calling it with the image array.\n\ncifar10 &lt;- dataset_cifar10(image_array)\n\nWe can check that this works correctly by accessing the first image.\n\nstr(cifar10$.getitem(1))\n\nList of 1\n $ image:Float [1:3, 1:32, 1:32]\n\n\nTo convert this to a lazy_tensor, we can use the as_lazy_tensor() function. The only thing we need to specify is the output shapes of the tensors, which we set to c(NA, 3, 32, 32). The NA is used to indicate that the first dimension (batch dimension) can be of any size.\n\ncifar10_lt &lt;- as_lazy_tensor(cifar10, dataset_shapes = list(image = c(NA, 3, 32, 32)))\nhead(cifar10_lt)\n\n&lt;ltnsr[6]&gt;\n[1] &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt;\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo check that transformations on images were applied correctly, it can be useful to inspect the images, e.g., using torchvision::tensor_image_browse().\n\n\nNext, we create the factor vector that contains the labels. For that, we use the data stored in the original torchvision::cifar10_dataset() object.\n\nlabels &lt;- factor(cifar10_original$y, labels = cifar10_original$classes[1:10])\nstr(labels)\n\n Factor w/ 10 levels \"airplane\",\"automobile\",..: 7 10 10 5 2 2 3 8 9 4 ...\n\n\nNext, we create a data.table that contains the images and labels.\n\ncifar10_dt &lt;- data.table(image = cifar10_lt, label = labels)\nhead(cifar10_dt)\n\n             image      label\n     &lt;lazy_tensor&gt;     &lt;fctr&gt;\n1: &lt;tnsr[3x32x32]&gt;       frog\n2: &lt;tnsr[3x32x32]&gt;      truck\n3: &lt;tnsr[3x32x32]&gt;      truck\n4: &lt;tnsr[3x32x32]&gt;       deer\n5: &lt;tnsr[3x32x32]&gt; automobile\n6: &lt;tnsr[3x32x32]&gt; automobile\n\n\nThis table can now be converted to an mlr3::Task using the as_task_&lt;type&gt; converters.\n\ntsk_cifar &lt;- as_task_classif(cifar10_dt, id = \"cifar10\", target = \"label\")\ntsk_cifar\n\n&lt;TaskClassif:cifar10&gt; (50000 x 2)\n* Target: label\n* Properties: multiclass\n* Features (1):\n  - lt (1): image\n\n\nWe will now try to train a simple multi-layer perceptron – the one we have defined above – on the images. One problem that we have is that the images are of shape (3, 32, 32), but the nn_linear layer expects a flat input of size 3072 (\\(3 \\times 32^2\\)).\nThis is where the lazy_tensor datatype comes in handy. We can use preprocessing PipeOps – in this case po(\"trafo_reshape\") – to transform the data. Here, the -1 in the shape c(-1, 3072) indicates that the first dimension (batch dimension) can be of any size.\n\nreshaper &lt;- po(\"trafo_reshape\", shape = c(-1, 3072))\ntsk_cifar_flat &lt;- reshaper$train(list(tsk_cifar))[[1L]]\ntsk_cifar_flat$head()\n\n        label         image\n       &lt;fctr&gt; &lt;lazy_tensor&gt;\n1:       frog  &lt;tnsr[3072]&gt;\n2:      truck  &lt;tnsr[3072]&gt;\n3:      truck  &lt;tnsr[3072]&gt;\n4:       deer  &lt;tnsr[3072]&gt;\n5: automobile  &lt;tnsr[3072]&gt;\n6: automobile  &lt;tnsr[3072]&gt;\n\n\nThis transformation is not applied eagerly, but only when the data is actually loaded.\n\n\n\n\n\n\nNote\n\n\n\nIn this case, as all the images are stored in memory, we could have also applied the transformation directly to the array representing the images, but decided not to do this for demonstration purposes.\n\n\nWe can now use almost the same graph that we used for the iris dataset for the flattened CIFAR-10 task. The only thing we need to exchange is the ingress operator, as the new task has as data a lazy_tensor instead of numeric vectors.\n\ngraph &lt;- po(\"torch_ingress_ltnsr\") %&gt;&gt;% architecture %&gt;&gt;%\n  po(\"torch_loss\", loss = t_loss(\"cross_entropy\")) %&gt;&gt;%\n  po(\"torch_optimizer\", optimizer = t_opt(\"adam\")) %&gt;&gt;%\n  po(\"torch_model_classif\", epochs = 10, batch_size = 256, predict_type = \"prob\")\nglrn &lt;- as_learner(graph)\n\n\nglrn$train(tsk_cifar_flat)\n\nAlternatively, we can integrate the flattening step into the graph from which the GraphLearner was created.\n\ngraph_with_flattening &lt;- reshaper %&gt;&gt;% graph\nglrn_with_flattening &lt;- as_learner(graph_with_flattening)\n\nThis learner can now be applied directly to the (unflattened) task.\n\nglrn_with_flattening$train(tsk_cifar)\n\n\n\nSeemless Integration: Resampling and Tuning\nBecause neural networks are also just another Learner, we can seamlessly leverage the functionalities from the mlr3 ecosystem. We can, for example, not only train the learner on a task, but also resample it.\n\nresample(tsk_cifar, glrn_with_flattening, rsmp(\"cv\", folds = 3))\n\n&lt;ResampleResult&gt; with 3 resampling iterations\n task_id\n cifar10\n cifar10\n cifar10\n                                                                                                                           learner_id\n trafo_reshape.torch_ingress_ltnsr.nn_linear_1.nn_relu_1.nn_linear_2.nn_relu_2.nn_head.torch_loss.torch_optimizer.torch_model_classif\n trafo_reshape.torch_ingress_ltnsr.nn_linear_1.nn_relu_1.nn_linear_2.nn_relu_2.nn_head.torch_loss.torch_optimizer.torch_model_classif\n trafo_reshape.torch_ingress_ltnsr.nn_linear_1.nn_relu_1.nn_linear_2.nn_relu_2.nn_head.torch_loss.torch_optimizer.torch_model_classif\n resampling_id iteration     prediction_test warnings errors\n            cv         1 &lt;PredictionClassif&gt;        0      0\n            cv         2 &lt;PredictionClassif&gt;        0      0\n            cv         3 &lt;PredictionClassif&gt;        0      0\n\n\nOr if we have the computational resources to optimize the hyperparameters of the neural network, we can also use the mlr3tuning package. Just like earlier, we need to define the tuner, search search space, resampling strategy, tuning measure and our budget. Here, we only tune the latent dimensions of the first two layers and the number of epochs.\n\n\n\n\n\n\nNote\n\n\n\nIn practice, we would not optimizer the number of epochs using offline tuning as shown here, but instead using early stopping, which is significantly more efficient and will be covered later.\n\n\nWe here show a different way of defining the search space than earlier, but could have achieved the same, by setting the respective hyperparameters to to_tune() values.\n\nsearch_space &lt;- ps(\n  nn_linear_1.out_features = p_int(50, 500),\n  nn_linear_2.out_features = p_int(50, 500),\n  torch_model_classif.epochs = p_int(10, 100)\n)\n\nNext, we define a simple random search tuner, use 3-fold cross-validation and use the logloss as the tuning measure and set the budget to 30 evaluations.\n\ntuner &lt;- tnr(\"random_search\")\nresampling &lt;- rsmp(\"cv\", folds = 3)\nmeasure &lt;- msr(\"classif.logloss\")\nevals &lt;- 30\n\n\nti &lt;- tune(\n  task = tsk_cifar,\n  learner = glrn_with_flattening,\n  resampling = resampling,\n  measure = measure,\n  search_space = search_space,\n  tuner = tuner,\n  term_evals = evals\n)\n\n\n\nSaving an mlr3torch Learner\nWe have seen earlier that torch tensors cannot simply be saved using saveRDS(). The same also applies to the mlr3torch learners. To save an mlr3torch learner, we need to call the $marshal() method first.\n\npth &lt;- tempfile(fileext = \".rds\")\nglrn_with_flattening$marshal()\nsaveRDS(glrn_with_flattening, pth)\n\nAfterward, we can load the learner again using readRDS() and then call the $unmarshal() method.\n\nglrn_with_flattening &lt;- readRDS(pth)\nglrn_with_flattening$unmarshal()"
  },
  {
    "objectID": "notebooks/7-training-efficiency-exercise-task.html",
    "href": "notebooks/7-training-efficiency-exercise-task.html",
    "title": "Training Efficiency",
    "section": "",
    "text": "Question 1: Validation\nIn this exercise, we will once again train a simple multi-layer perceptron on the Indian Liver Patient Dataset (ILPD). Create a learner that:\n\nUses 2 hidden layers with 100 neurons each.\nUtilizes a batch size of 128.\nTrains for 200 epochs.\nEmploys a validation set co mprising 30% of the data.\nTrack the validation log-loss.\nUtilizes trace-jitting to speed up the training process.\nEmploys the history callback to record the training and validation log-loss during training.\n\nAfterward, plot the validation log-loss, which is accessible via learner$model$callbacks$history.\nBelow, we create the task and remove the gender feature again for simplicity.\n\nlibrary(mlr3verse)\nlibrary(mlr3torch)\nilpd_num &lt;- tsk(\"ilpd\")\nilpd_num$select(setdiff(ilpd_num$feature_names, \"gender\"))\nilpd_num\n\n&lt;TaskClassif:ilpd&gt; (583 x 10): Indian Liver Patient Data\n* Target: diseased\n* Properties: twoclass\n* Features (9):\n  - dbl (5): albumin, albumin_globulin_ratio, direct_bilirubin, total_bilirubin, total_protein\n  - int (4): age, alanine_transaminase, alkaline_phosphatase, aspartate_transaminase\n\n\nQuestion 2: Early Stopping\nEnable early stopping to prevent overfitting and re-train the learner (using a patience of 10). Print the final validation performance of the learner and the early stopped results. You can consult the documentation of LearnerTorch on how to access these (see section Active Bindings).\n\n\nHint\n\nYou can enable early stopping by setting the patience parameter.\n\nQuestion 3: Early Stopping and Dropout Tuning\nWhile early stopping in itself is already useful, mlr3torch also allows you to simultaneously tune the number of epochs using early stopping while tuning other hyperparameters via traditional hyperparameter tuning from mlr3tuning.\nOne thing we have not mentioned so far is that the MLP learner also uses a dropout layer. The dropout probability can be configured via the p parameter.\nYour task is to tune the dropout probability p in the range \\([0, 1]\\) and the epochs using early stopping (using the configuration from the previous exercise) with an upper bound of 100 epochs.\nTo adapt this to work with early stopping, you need to set the:\n\nepochs to to_tune(upper = &lt;value&gt;, internal = TRUE): This tells the Tuner that the learner will tune the number of epochs itself.\n$validate field of the \"test\" so the same data is used for tuning and validation.\nTuning measure to msr(\"internal_valid_score\", minimize = TRUE). We set minimize to TRUE because we have used the log-loss as a validation measure.\n\nApart from this, the tuning works just like in tutorial 5. Use 3-fold cross-validation and evaluate 10 configurations using random search. Finally, print the optimal configuration."
  },
  {
    "objectID": "notebooks/7-training-efficiency.html",
    "href": "notebooks/7-training-efficiency.html",
    "title": "Training Efficiency",
    "section": "",
    "text": "Methods for increasing training efficiency can be roughly split into:"
  },
  {
    "objectID": "notebooks/7-training-efficiency.html#parallel-processing",
    "href": "notebooks/7-training-efficiency.html#parallel-processing",
    "title": "Training Efficiency",
    "section": "Parallel Processing",
    "text": "Parallel Processing\n\nGraphical Processing Unit (GPU)\nUsing a GPU is crucial when training relatively large neural networks because GPUs are specifically designed to handle the parallel processing required for complex computations. To use a GPU in mlr3torch, we can set the device parameter to “cuda”. By default, it is set to “auto”, which will use a GPU if available and otherwise fall back to the CPU.\n\n\n\n\n\n\nTip\n\n\n\nTo check if a GPU is available, we can use the torch::cuda_is_available() function.\n\nlibrary(torch)\ncuda_is_available()\n\n[1] FALSE\n\n\nIf you have an M1 Mac (or later), you can also use the available graphics card by setting the device parameter to \"mps\". You can check this by running:\n\nbackends_mps_is_available()\n\n[1] TRUE\n\n\n\n\nTo demonstrate the speed improvements obtained by using a GPU, we conduct a large matrix operation on a GPU and a CPU. We start by randomly sampling a matrix of size 1000x1000.\n\nx_cpu = torch_randn(1000, 1000, device = \"cpu\")\n\nBelow, we perform a matrix multiplication on the CPU and the GPU and compare the timings.\n\n# this will only run if a GPU is available\nx_cuda = x_cpu$cuda()\n\nbench::mark(\n  cpu = x_cpu$matmul(x_cpu),\n  cuda = x_cuda$matmul(x_cuda)\n)\n\n\n\nCPU Threads\nTraining large networks on a CPU is not a recommended approach, but it can be a viable option for smaller networks. You can still use multiple threads to speed up the execution of operations. Please be aware that the code below will not execute on macOS, as setting the number of threads is not supported on this operating system.\n\n# this will be skipped on macOS\nbench::mark(\n  {torch_set_num_threads(1L); x_cpu$matmul(x_cpu)},\n  {torch_set_num_threads(16L); x_cpu$matmul(x_cpu)}\n)\n\ntorch also allows for interop-parallelization, but this is more advanced and code needs to be written in a specific way.\n\n\n\n\n\n\nQuiz: Number of Threads\n\n\n\nQuestion 1: On a CPU with 4 cores, does it make sense to set the number of threads to values greater than 4? Explain your answer.\n\n\nClick for answer\n\nOn a CPU with 4 cores, at most 4 threads can run in parallel. Using more threads than the number of cores will not speed up the execution of operations.\n\nQuestion 2: On a CPU with 64 cores, is it always the case that using 64 threads is better than using 32 threads?\n\n\nClick for answer\n\nNot necessarily. Using more threads will mean that:\n\nThe threads need to communicate and synchronize, which increases the runtime.\nMore resources are used for the computation, which decreases the runtime.\n\nThe optimal number of threads is a trade-off between these two effects."
  },
  {
    "objectID": "notebooks/7-training-efficiency.html#efficient-data-loading",
    "href": "notebooks/7-training-efficiency.html#efficient-data-loading",
    "title": "Training Efficiency",
    "section": "Efficient Data Loading",
    "text": "Efficient Data Loading\nBesides parallelizing the computation of operations in the forward and backward pass, another possible bottleneck is the loading of data. There are various ways to improve data loading speed:\n\nImprove the implementation of the dataset class\nParallelize the data loading process\nIncrease the speed of data transfer to the GPU\n\nThese approaches will now be discussed.\n\nEfficient Dataset Implementation\nWhen implementing a dataset, we need to define:\n\nHow we store and load the data\nWhether implementing loading of a batch is beneficial\n\n\n\n\n\n\n\nQuiz: Data Loading\n\n\n\nThe tiny imagenet dataset is a dataset of 100,000 images of size 64x64. It is a subset of the famous imagenet dataset. Below, we show some examples from it:\n\nWe will now consider different ways to write a torch::dataset implementation for this data. Assume we have some image paths stored in a character vector as well as in an array where they are already loaded into memory.\n\nstr(image_paths)\n\n chr [1:100] \"/Users/sebi/Library/Caches/org.R-project.R/R/mlr3torch/datasets/tiny_imagenet/raw/tiny-imagenet-200/train/n0144\"| __truncated__ ...\n\nstr(image_array)\n\n num [1:100, 1:3, 1:64, 1:64] 1 0.0784 0.4706 0.5608 0.5647 ...\n\n\nAn individual image can, for example, be loaded using the torchvision::base_loader() function:\n\nlibrary(torchvision)\nstr(base_loader(image_paths[1]))\n\n num [1:64, 1:64, 1:3] 1 1 1 1 1 ...\n\n\nQuestion 1: Reading From Disk or RAM\nWhich of the following is the faster way to load the images? Explain why.\n\nLoading the images from disk:\n\nds_disk = dataset(\"image_paths\",\n  initialize = function(image_paths) {\n    self$image_paths = image_paths\n  },\n  .getitem = function(i) {\n    torch_tensor(torchvision::base_loader(self$image_paths[i]))\n  },\n  .length = function() {\n    length(self$image_paths)\n  }\n)(image_paths)\n\nLoading the images from an array:\n\nds_ram = dataset(\"image_array\",\n  initialize = function(image_array) {\n    self$image_array = image_array\n  },\n  .getitem = function(i) {\n    torch_tensor(self$image_array[i, , , ])\n  },\n  .length = function() {\n    nrow(self$image_array)\n  }\n)(image_array)\n\n\n\n\nClick for answer\n\nGenerally, loading images from RAM is significantly faster than loading them from disk. Although the benchmark presented below may seem somewhat ‘unfair’ since ds_ram has already loaded the images into memory, this difference is evident in practice. When iterating over the dataset for multiple epochs, the first method will need to reload the images from disk for each epoch, while the second method only requires a single loading of the images into memory.\n\niter = function(ds, ..., epochs = 1) {\n  dl = torch::dataloader(ds, batch_size = 16, ...)\n  for (epoch in seq_len(epochs)) {\n    coro::loop(for(batch in dl) {\n      batch\n    })\n  }\n}\nbench::mark(\n  disk = iter(ds_disk, epochs = 10),\n  ram = iter(ds_ram, epochs = 10),\n  check = FALSE\n)\n\nWarning: Some expressions had a GC in every iteration; so filtering is disabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 disk          198ms    200ms      4.89   108.7MB     9.79\n2 ram           172ms    176ms      5.67    94.4MB    11.3 \n\n\n\nQuestion 2: (Don’t) Copy that\nConsider now the next dataset implementation:\n\nds_tensor = dataset(\"tensor\",\n  initialize = function(image_array) {\n    self$tensor = torch_tensor(image_array)\n  },\n  .getitem = function(i) {\n    self$tensor[i, ..]\n  },\n  .length = function() {\n    nrow(self$tensor)\n  }\n)(image_array)\n\nDo you think this implementation is faster or slower than the ds_ram implementation? Explain why.\n\n\nClick for answer\n\nThis implementation is faster than the ds_ram implementation. This is because the ds_tensor implementation copies the R array to a torch tensor only once, whereas the ds_ram implementation copies the R array to a torch tensor for each item.\n\nbench::mark(\n  tensor = iter(ds_tensor),\n  array = iter(ds_ram),\n  check = FALSE\n)\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 tensor       3.55ms    3.8ms     262.    76.19KB     11.6\n2 array       14.56ms   15.5ms      63.6    9.44MB     13.8\n\n\n\nQuestion 3: $.getbatch() vs $.getitem()\nWhich implementation is faster? Explain why.\n\nds_tensor_batch = dataset(\"tensor_batch\",\n  initialize = function(image_array) {\n    self$tensor = torch_tensor(image_array)\n  },\n  .getbatch = function(i) {\n    self$tensor[i, .., drop = FALSE]\n  },\n  .length = function() {\n    nrow(self$tensor)\n  }\n)(image_array)\n\n\n\nClick for answer\n\nThe $.getbatch() implementation is faster than the $.getitem() implementation. This is because when using the $.getitem() method, the batch for indices ids is obtained by calling $.getitem(id) for each index in ids and then stacking them together, which requires a new tensor allocation. Slicing the tensor, however, avoids this allocation when shuffle = TRUE (which is also the default).\n\nbench::mark(\n  getbatch = iter(ds_tensor_batch),\n  getitem = iter(ds_tensor),\n  check = FALSE\n)\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 getbatch     1.42ms   1.77ms      551.    12.3KB     4.65\n2 getitem      3.76ms   4.18ms      218.    54.7KB     7.05\n\n\n\n\n\n\n\nParallel Data Loading\nIn Deep Learning, datasets can be very large, and it might therefore be the case that the data is simply too large to fit into memory. In this case, we can use parallel data loading to speed up the data loading process. Instead of loading the data sequentially in the main process, other R processes will be started that execute the data loading. For example, if we set num_workers = 4L, 4 R processes will be started that load the data, while the main process is free to train the model. These processes then send the batches to the main process. The image below visualizes this process:\n\nCreating such a parallel dataloader is as easy as setting the num_workers parameter to a value greater than 0.\n\n\n\n\n\n\nNote\n\n\n\nNote that in the current implementation, parallel data loading is only beneficial when it is slow, e.g., because of loading the data from disk or because of expensive preprocessing. This will hopefully be improved in the future (by a faster implementation of the parallel dataloader).\n\n\n\n\nMoving Data to the GPU\nOne thing we have ignored so far is that when training using a GPU, the data needs to be moved from RAM to the GPU. This is because a GPU has its own memory (VRAM), and the data needs to be moved to this memory before it can be used for training. The moving of the data to the GPU cannot be done on the processes that are loading the data but must be done in the main process, i.e., after the batch was received from (possibly parallelized) dataloader. One way to speed up the data loading process is to pin the memory of the data to the GPU. Before a tensor can be moved from RAM to VRAM, it needs to be in so-called page-locked memory, which can be enabled using the pin_memory parameter of dataloader()..\n\n\niter_cuda = function(ds, pin_memory = TRUE) {\n  dl = torch::dataloader(ds, batch_size = 16, pin_memory = pin_memory)\n  coro::loop(for(batch in dl) {\n    batch$cuda()\n  })\n}\n\nbench::mark(\n  not_pinned = iter_cuda(ds_disk, pin_memory = FALSE),\n  pinned = iter_cuda(ds_disk, pin_memory = TRUE)\n)\n\n\n\n\n\n\n\nNote\n\n\n\nIn order to use parallel data loading or memory pinning with mlr3torch, these parameters can simply be specified in the learner:\n\nlrn(\"classif.mlp\", num_workers = 8L, pin_memory = TRUE, device = \"cuda\")"
  },
  {
    "objectID": "notebooks/7-training-efficiency.html#jit-compilation-ignite-optimizers",
    "href": "notebooks/7-training-efficiency.html#jit-compilation-ignite-optimizers",
    "title": "Training Efficiency",
    "section": "JIT Compilation & Ignite Optimizers",
    "text": "JIT Compilation & Ignite Optimizers\nSome special care needs to be taken when using torch (or mlr3torch) in order to get good performance. In the future, this will hopefully not be necessary anymore, but is currently required.\n\n‘Ignite’ Optimizers\nIn torch, different versions of optimizers exist:\n\noptim_adamw\n\n&lt;optim_adamw&gt; object generator\n  Inherits from: &lt;inherit&gt;\n  Public:\n    initialize: function (params, lr = 0.001, betas = c(0.9, 0.999), eps = 1e-08, \n    loop_fun: function (group, param, g, p) \n    step: function (closure = NULL) \n    clone: function (deep = FALSE) \n  Parent env: &lt;environment: 0x121963190&gt;\n  Locked objects: FALSE\n  Locked class: FALSE\n  Portable: TRUE\n\noptim_ignite_adamw\n\n&lt;optim_ignite_adamw&gt; object generator\n&lt;optim_ignite&gt; object generator\n  Inherits from: &lt;inherit&gt;\n  Public:\n    initialize: function (params, lr = 0.001, betas = c(0.9, 0.999), eps = 1e-08, \n    clone: function (deep = FALSE) \n  Private:\n    .config_names: lr betas eps weight_decay amsgrad\n    .state_names: exp_avg exp_avg_sq max_exp_avg_sq step\n    .optim: function (params, ...) \n    .get_states: function (opt) \n    .set_states: function (opt, params, states) \n    .add_param_group: function (opt, params, lr, betas, eps, weight_decay, amsgrad) \n    .assert_params: function (lr, betas, eps, weight_decay, amsgrad) \n    .set_param_group_options: function (opt, list) \n    .zero_grad: function (opt) \n    .get_param_groups: function (ptr) \n  Parent env: &lt;environment: 0x12472df48&gt;\n  Locked objects: FALSE\n  Locked class: FALSE\n  Portable: TRUE\n\n\nThe ‘ignite’ indicates that the optimizer is a version that is optimized for performance. Not for all optimizers does an ignite version exist, but for the most common ones, there does.\nBelow, we compare the performance of the default optimizer and the ignite optimizer and see that the latter is considerably faster.\n\nadamw = as_torch_optimizer(torch::optim_adamw)\nignite_adamw = as_torch_optimizer(torch::optim_ignite_adamw)\n\nlearner = lrn(\"classif.mlp\", epochs = 10, neurons = c(100, 100), batch_size = 32, optimizer = adamw)\n\nlearner_ignite = learner$clone(deep = TRUE)\nlearner_ignite$configure(\n  optimizer = ignite_adamw\n)\ntask_sonar = tsk(\"sonar\")\n\nbench::mark(\n  learner$train(task_sonar),\n  learner_ignite$train(task_sonar),\n  check = FALSE\n)\n\nWarning: Some expressions had a GC in every iteration; so filtering is disabled.\n\n\n# A tibble: 2 × 6\n  expression                            min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;                       &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 learner$train(task_sonar)           417ms    419ms      2.39    15.3MB     7.16\n2 learner_ignite$train(task_sonar)    193ms    196ms      4.79    10.7MB     6.39\n\n\n\n\nJIT Compilation\nJIT (Just-In-Time) compilation is a runtime optimization technique that compiles code into machine code during execution rather than beforehand. This has different advantages:\n\nBy JIT-compiling a model, some operations can be optimized for performance.\nA JIT-compiled model can be saved and executed without an R dependency for deployment (only LibTorch is required), e.g., in a C++ application.\nRunning a JIT-compiled model in R is faster because the whole network is executed in C++ instead of R.\n\nIn torch, this can either be done using TorchScript or by tracing a model. We will briefly discuss both approaches, but for more information, see the torch documentation.\n\nTorchScript\nTorchScript is a subset of Python – i.e., its own programming language – that can be used to define compiled functions. In R, this is available via the jit_compile function.\n\nf = jit_compile(\"\ndef f(x, w, bias):\n  return x @ w + bias\n\")$f\n\nx = torch_randn(10, 10)\nw = torch_randn(10, 1)\nbias = torch_randn(1)\n\nout = f(x, w, bias)\nstr(out)\n\nFloat [1:10, 1:1]\n\n\nBesides syntax, there are some notable differences between TorchScript and R to be aware of:\n\nIn TorchScript, indexing tensors is 0-based, and\nTorchScript is statically typed, so you need to specify the types of the arguments, unless they are tensors, which is the default.\n\nBelow, we define a function that takes a list of tensors and calculates their sum.\n\nsum_jit = jit_compile(\"\ndef sum_jit(xs: List[Tensor]):\n  output = torch.zeros_like(xs[0])\n  for x in xs:\n    output = output + x\n  return output\n\")$sum_jit\n\nsum_jit(list(torch_randn(1), torch_randn(1)))\n\ntorch_tensor\n-0.7121\n[ CPUFloatType{1} ]\n\n\n\n\nTracing\nThe alternative to writing TorchScript is to write your module in R and to use jit_trace to compile it.\n\nf2 = function(x, w, bias) {\n  x$matmul(w) + bias\n}\n# need to provide some example input\n# arguments are passed by position\nf2 = jit_trace(f2, torch_randn(10, 10), torch_randn(10, 100), torch_randn(100))\nout2 = f2(x, w, bias)\ntorch_equal(out, out2)\n\n[1] TRUE\n\n\nAn advantage of trace-compilation is that it can be applied to modules, which is currently not possible with jit_compile.\n\nnet = nn_sequential(\n  nn_linear(10, 100),\n  nn_relu(),\n  nn_linear(100, 10)\n)\nnet_jit = jit_trace(net, torch_randn(10, 10))\n\ntorch_equal(net(x), net_jit(x))\n\n[1] TRUE\n\n\nHowever, trace-compilation is restrictive because it only records operations applied to torch tensors and is unaware of R control flow, so you need to be careful when using it. Furthermore, it only accepts torch tensors as arguments. For many simple modules, trace-compilation should usually work. You can also check this by running the original and trace-jitted module on some example inputs and see if they return the same result.\n\n\n\n\n\n\nNote\n\n\n\nA trace-jitted module does respect the mode of the network, i.e., whether it is training or evaluating.\n\n\nIn mlr3torch, trace compilation is also available and can be enabled by setting jit_trace = TRUE in the learner.\n\nlearner = lrn(\"classif.mlp\", jit_trace = TRUE)\n\nYou can also combine TorchScript with tracing:\n\nnet_both = nn_module(\n  initialize = function() {\n    self$linear = nn_linear(1, 1)\n  },\n  forward = function(x) {\n    self$linear(sum_jit(x))\n  }\n)()\n\nnet_both(list(torch_randn(1), torch_randn(1)))\n\ntorch_tensor\n 1.0027\n[ CPUFloatType{1} ][ grad_fn = &lt;ViewBackward0&gt; ]\n\nnet_both(list(torch_randn(1)))\n\ntorch_tensor\n0.01 *\n 8.5286\n[ CPUFloatType{1} ][ grad_fn = &lt;ViewBackward0&gt; ]\n\n\n\n\n\n\n\n\nQuiz: Just In Time\n\n\n\nQuestion 1: Consider the trace-jitted function below. Can you predict the output of the last two lines? Can you explain why this happens?\n\nf = function(a, b, multiply) {\n  if (multiply$item()) {\n    a * b\n  } else {\n    a + b\n  }\n}\nfjit = jit_trace(f, torch_tensor(1), torch_tensor(2), torch_tensor(TRUE))\n\nfjit(torch_tensor(2), torch_tensor(3), torch_tensor(TRUE))\nfjit(torch_tensor(2), torch_tensor(3), torch_tensor(FALSE))\n\n\n\nClick for answer\n\n\nfjit(torch_tensor(2), torch_tensor(3), torch_tensor(TRUE))\n\ntorch_tensor\n 6\n[ CPUFloatType{1} ]\n\nfjit(torch_tensor(2), torch_tensor(3), torch_tensor(FALSE))\n\ntorch_tensor\n 6\n[ CPUFloatType{1} ]\n\n\n\nQuestion 2: Answer the same question for the following function:\n\nf = function(a, b, multiply) {\n  torch_where(multiply, a * b, a + b)\n}\nfjit = jit_trace(f, torch_tensor(1), torch_tensor(2), torch_tensor(TRUE))\n\nfjit(torch_tensor(2), torch_tensor(3), torch_tensor(TRUE))\nfjit(torch_tensor(2), torch_tensor(3), torch_tensor(FALSE))\n\n\n\nClick for answer\n\n\nfjit(torch_tensor(2), torch_tensor(3), torch_tensor(TRUE))\n\ntorch_tensor\n 6\n[ CPUFloatType{1} ]\n\nfjit(torch_tensor(2), torch_tensor(3), torch_tensor(FALSE))\n\ntorch_tensor\n 5\n[ CPUFloatType{1} ]\n\n\n\n\n\n\n\n\nMixed Precision Training\nAnother way to speed up the training process is to use mixed precision training. This technique involves training the model using both 16-bit and 32-bit floating point numbers. This allows reducing the memory footprint of the model and speeding up the training process. We won’t cover this here, but refer to the torch documentation that explains how to do this."
  },
  {
    "objectID": "notebooks/7-training-efficiency.html#methodological-approaches",
    "href": "notebooks/7-training-efficiency.html#methodological-approaches",
    "title": "Training Efficiency",
    "section": "Methodological Approaches",
    "text": "Methodological Approaches\n\nValidation and Early Stopping\nFor more details on this topic, see the corresponding chapter in the mlr3 book.\nAs we have already seen in one of the previous notebooks, in deep learning, some part of the data is often used for validation purposes. This allows monitoring the performance of the model on unseen data.\nIn mlr3torch, we can track the performance of the model on a validation set by specifying:\n\nvalidate, which is the ratio of the data that is used for validation\nmeasures_valid, which is a list of measures to evaluate the validation performance\neval_freq, which is the frequency at which the validation is performed\ncallbacks, which is a list of callbacks to use during training, in this case, we use the t_clbk(\"history\") callback, which records the performance of the model on the validation set at regular intervals, enabling us to monitor and analyze the model’s performance over time.\n\n\ntask = tsk(\"sonar\")\n\nmlp_learner = lrn(\"classif.mlp\",\n  neurons = c(50, 50), batch_size = 256, epochs = 400,\n  optimizer = t_opt(\"adam\", lr = 0.003),\n  predict_type = \"prob\", jit_trace = TRUE,\n  # Validation / Performance Monitoring\n  validate = 0.3, # how much data to use for validation\n  measures_valid = msr(\"classif.logloss\"), # how to evaluate train performance\n  measures_train = msr(\"classif.logloss\"), # how to evaluate validation performance\n  callbacks = t_clbk(\"history\"), # history callbacks save train and validation performance\n  eval_freq = 10 # after how many training epochs to perform validation\n)\nmlp_learner$train(task)\nhistory = mlp_learner$model$callbacks$history\nhead(history)\n\nKey: &lt;epoch&gt;\n   epoch train.classif.logloss valid.classif.logloss\n   &lt;num&gt;                 &lt;num&gt;                 &lt;num&gt;\n1:    10             0.6912086             0.6644606\n2:    20             0.6462799             0.6247280\n3:    30             0.5461739             0.5523349\n4:    40             0.5138571             0.4894380\n5:    50             0.4346012             0.4469736\n6:    60             0.3748591             0.4227901\n\n\nBelow we plot the training and validation for the different epochs:\n\n\n\n\n\n\n\n\n\nInstead of only monitoring the validation loss (and watching it get worse and worse), we can also stop the training process dynamically when the validation loss begins to increase. This regularization technique is called early stopping, and it prevents overfitting during the training of iteratively trained machine learning models.\nThe key configuration option for early stopping is the patience parameter, which defines the number of epochs to wait after the last improvement in validation loss before stopping the training. For example, if the patience is set to 5, the training will continue for 5 additional epochs after the last observed improvement in validation loss. If no improvement is seen during this period, training will be halted.\nAdvantages of early stopping include:\n\nPrevention of Overfitting: By stopping training when the model starts to overfit, we can achieve better generalization on unseen data.\nResource Efficiency: It saves computational resources by avoiding unnecessary training epochs once the model performance has plateaued.\n\nNow, let’s train the learner again using early stopping with a patience of 5 epochs:\n\nmlp_learner$param_set$set_values(\n  patience = 5\n)\nmlp_learner$train(task)\nmlp_learner$internal_tuned_values$epochs\n\n[1] 160\n\n\nBeyond only tuning the number of epochs, mlr3’s internal tuning mechanism also allows tuning the number of epochs internally while using an offline tuning method to optimize other hyperparameters. To use this, we can set the parameters we want to tune using to_tune(), but need to set internal = TRUE for the epochs parameter.\n\nlibrary(mlr3tuning)\nmlp_learner$param_set$set_values(\n  epochs = to_tune(upper = 100, internal = TRUE),\n  opt.lr = to_tune(lower = 1e-4, upper = 1e-1, logscale = TRUE)\n)\n\nWe could now pass this learner to a tuner as usual."
  },
  {
    "objectID": "notebooks/7-training-efficiency.html#data-augmentation",
    "href": "notebooks/7-training-efficiency.html#data-augmentation",
    "title": "Training Efficiency",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nOne other important technique to improve the performance of deep learning models is data augmentation. It is a technique used to increase the diversity and quantity of training data without actually collecting new data. By applying various transformations to the existing dataset, data augmentation helps improve the generalization capabilities of machine learning models, reduce overfitting, and enhance model robustness. This is especially crucial when you have limited data. We will here demonstrate this using images, but the concept can also be applied to other data types.\nAugmentation operations for images can consist of rotation, flipping, translating, grey scaling, etc. Which data augmentation is admissible, depends on the task:\n\nIf the modeling task is to predict whether there is a mark in the top right corner of an image, vertical or horizontal flipping is not admissible.\nIf the goal is to predict whether there is a mark somewhere in the image, it would be admissible.\n\nIn other words, the data augmentation must be compatible with the invariances of the machine learning problem. More formally, we can apply a function \\(g\\) to the data \\(x\\) to a data point \\(g(x)\\) if for the true relationship \\(f\\) we have that \\(f(g(x)) = f(x)\\).\nIn mlr3torch, data augmentation is available via PipeOps of the form po(\"augment_\"). Currently, only augmentation operators from the torchvision package are available, but you can also add your own.\n\naugment = po(\"augment_random_resized_crop\") %&gt;&gt;%\n  po(\"augment_random_horizontal_flip\") %&gt;&gt;%\n  po(\"augment_random_vertical_flip\")\n\nWe can just create a new GraphLearner that includes the augmentation steps as well as a ResNet-18 learner. When this learner is trained, it will apply the augmentation operations to the training data.\n\nresnet_augmented = as_learner(augment %&gt;&gt;% lrn(\"classif.resnet18\"))\n\n\n\n\n\n\n\nQuiz: Data Augmentation\n\n\n\nQuestion 1: Do you think data augmentation should be applied to the validation set?\n\n\nClick for answer\n\nNo, as the purpose of data augmentation is not to improve an individual prediction, it will not be applied during test time and hence also not to the validation set. Looking at the performance of augmented validation data is, however, also not a mistake."
  },
  {
    "objectID": "notebooks/8-usecase.html",
    "href": "notebooks/8-usecase.html",
    "title": "Practical Use Case",
    "section": "",
    "text": "Brief recap of what is important to get good and fast performance in mlr3torch:\n\nUse jit-compilation to speed up the training"
  },
  {
    "objectID": "notebooks/setup-guide.html",
    "href": "notebooks/setup-guide.html",
    "title": "Setup Guide",
    "section": "",
    "text": "In order to be able to successfully run the notebooks and solve the exercises, make sure that you have the right environment set up."
  },
  {
    "objectID": "notebooks/setup-guide.html#r-version",
    "href": "notebooks/setup-guide.html#r-version",
    "title": "Setup Guide",
    "section": "R Version",
    "text": "R Version\nThese notebooks were developed using R 4.4.2, so ideally you should use the same version. You can check your current R version by running R --version in your terminal. If you have the same R version, you can skip the rest of this section.\nA convenient way to simultaneously maintain different versions of R and to easily switch between them is to use rig, the R installation manager. It’s documentation contains instructions on how to install it on Windows, macOS, and Linux.\nAfter having installed rig, you can install R 4.4.2 and make it the default by running the following command in your terminal:\nrig add 4.4.2\nrig default 4.4.2\nVerify that the R version is 4.4.2 by running R --version in your terminal."
  },
  {
    "objectID": "notebooks/setup-guide.html#libraries",
    "href": "notebooks/setup-guide.html#libraries",
    "title": "Setup Guide",
    "section": "Libraries",
    "text": "Libraries\nFor managing libraries, we use the renv package. If you don’t have renv installed, you can install it by running the following command:\ninstall.packages(\"renv\")\nNext, initialize the renv environment by running the following command:\nrenv::init()\nFinally, restore the renv environment by running the following command. This might take some time as it downloads all the required libraries.\nrenv::restore()\nTo be able to use the torch package, you need to also run this additional command:\ntorch::install_torch()\nIf this completes successfully, you are ready to go! 🚀\nTo optionally check whether you have GPU support installed (this is not necessary for the exercises), run the following command:\ntorch::cuda_is_available()"
  }
]
