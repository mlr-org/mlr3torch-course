[
  {
    "objectID": "notebooks/5-cnn.html",
    "href": "notebooks/5-cnn.html",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "In this notebook, we will learn about convolutional neural networks (CNNs) that are used for image classification tasks."
  },
  {
    "objectID": "notebooks/5-cnn.html#architecture-design",
    "href": "notebooks/5-cnn.html#architecture-design",
    "title": "Convolutional Neural Networks",
    "section": "Architecture Design",
    "text": "Architecture Design\nAnother essential aspect of training neural networks efficiently and effectively is the design of the network architecture, which can be a challenging task. However, for many problems, there are predefined architectures that perform well and can be used. Unless there is a specific reason to design a new architecture, it is recommended to use such an architecture.\n\n\n\n\n\n\nNote\n\n\n\nBecause the Python deep learning ecosystem is so large, many more architectures are implemented in Python than in R. One way to use them in R is to simply translate the PyTorch code to (R-)torch. While PyTorch and (R-)torch are quite similar, there are some differences, e.g., 1-based and 0-based indexing. The torch website contains a brief tutorial on this topic.\n\n\nNonetheless, we will cover important techniques that can be used to speed up the training process, namely batch normalization and dropout.\nTo illustrate its effectiveness, we will define a simple CNN, with and without batch normalization, train it on CIFAR-10, and compare their performances.\nTo build the neural networks, we will use mlr3torch, which allows building architectures from PipeOps. Recall that the po(\"torch_ingress_ltnsr\") is a special PipeOp that marks the input of the neural network. Note that po(\"nn_relu_1\") is equivalent to po(\"nn_relu\", id = \"nn_relu_1\"). We need to specify unique IDs for each PipeOp as this is required in mlr3pipelines graphs.\n\ncnn_bn = po(\"torch_ingress_ltnsr\") %&gt;&gt;%\n  po(\"nn_conv2d_1\", out_channels = 32, kernel_size = 3, stride = 1, padding = 1) %&gt;&gt;%\n  po(\"nn_batch_norm2d_1\") %&gt;&gt;%\n  po(\"nn_relu_1\") %&gt;&gt;%\n  po(\"nn_max_pool2d_1\", kernel_size = 2, stride = 2) %&gt;&gt;%\n  po(\"nn_conv2d_2\", out_channels = 64, kernel_size = 3, stride = 1, padding = 1) %&gt;&gt;%\n  po(\"nn_batch_norm2d_2\") %&gt;&gt;%\n  po(\"nn_relu_2\") %&gt;&gt;%\n  po(\"nn_max_pool2d_2\", kernel_size = 2, stride = 2)\n\ncnn = po(\"torch_ingress_ltnsr\") %&gt;&gt;%\n  po(\"nn_conv2d_1\", out_channels = 32, kernel_size = 3, stride = 1, padding = 1) %&gt;&gt;%\n  po(\"nn_relu_1\") %&gt;&gt;%\n  po(\"nn_max_pool2d_1\", kernel_size = 2, stride = 2) %&gt;&gt;%\n  po(\"nn_conv2d\", out_channels = 64, kernel_size = 3, stride = 1, padding = 1) %&gt;&gt;%\n  po(\"nn_relu_2\") %&gt;&gt;%\n  po(\"nn_max_pool2d_2\", kernel_size = 2, stride = 2)\n\nhead = po(\"nn_flatten\") %&gt;&gt;%\n  po(\"nn_linear\", out_features = 128) %&gt;&gt;%\n  po(\"nn_relu\") %&gt;&gt;%\n  po(\"nn_head\")\n\nmodel = po(\"torch_optimizer\", optimizer = t_opt(\"adam\", lr = 0.003)) %&gt;&gt;%\n  po(\"torch_model_classif\",\n    epochs = 100,\n    batch_size = 256,\n    predict_type = \"prob\",\n    device = \"cuda\"\n  )\n\nWe evaluate the two models on the CIFAR-10 image classification task that we have introduced earlier. There, the goal is to classify images into 10 different classes.\n\nnet_bn = as_learner(cnn_bn %&gt;&gt;% head %&gt;&gt;% model)\nnet_bn$id = \"net_bn\"\nnet = as_learner(cnn %&gt;&gt;% head %&gt;&gt;% model)\nnet$id = \"net\"\n\ncifar10 = tsk(\"cifar10\")\nresampling = rsmp(\"holdout\")$instantiate(cifar10)\n\ndesign = benchmark_grid(\n  task = cifar10,\n  learner = list(net_bn, net),\n  resampling = resampling\n)\ndesign\n\n      task learner resampling\n    &lt;char&gt;  &lt;char&gt;     &lt;char&gt;\n1: cifar10  net_bn    holdout\n2: cifar10     net    holdout\n\n\n\nbmr = benchmark(design)\nbmr$aggregate()"
  },
  {
    "objectID": "notebooks/5-cnn.html#transfer-learning",
    "href": "notebooks/5-cnn.html#transfer-learning",
    "title": "Convolutional Neural Networks",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nTransfer learning is a powerful technique in machine learning where a pre-trained model developed for a specific task is reused as the starting point for a model on a second, related task. Instead of training a model from scratch, which can be time-consuming and computationally expensive, transfer learning leverages the knowledge gained from a previously learned task to improve learning efficiency and performance on a new task.\nThe advantages of transfer learning are:\n\nReduced Training Time: Leveraging a pre-trained model can significantly decrease the time required to train a new model, as the foundational feature extraction layers are already optimized.\nImproved Performance: Transfer learning can enhance model performance, especially when the new task has limited training data. The pre-trained model’s knowledge helps in achieving better generalization.\nResource Efficiency: Utilizing pre-trained models reduces the computational resources needed, making it feasible to develop sophisticated models without extensive hardware.\n\nWhen the model is then trained on a new task, only the last layer is replaced with a new output layer to adjust for the new task.\nThis is visualized below:\n\nSource\nmlr3torch offers various pretrained image networks that are available through the torchvision package. The ResNet-18 model is a popular pre-trained model that was pretrained on ImageNet. We can use the pretrained weights by setting the pretrained parameter to TRUE.\n\nresnet = lrn(\"classif.resnet18\",\n  pretrained = TRUE,\n  epochs = 2,\n  batch_size = 256,\n  validate = 0.3,\n  measures_valid = msr(\"classif.logloss\"),\n  device = \"cuda\",\n  predict_type = \"prob\",\n  id = \"pretrained\"\n)\nresnet_no_pretrain = resnet$clone(deep = TRUE)\nresnet_no_pretrain$param_set$set_values(\n  pretrained = FALSE\n)\nresnet_no_pretrain$id = \"not_pretrained\"\n\ngrid = benchmark_grid(\n  task = tsk(\"cifar10\"),\n  learner = list(resnet, resnet_no_pretrain),\n  resampling = rsmp(\"insample\")\n)\n\nbmr = benchmark(grid, store_models = TRUE)\nbmr$aggregate()\n\nWhen fine-tuning a pretrained model like ResNet-18, it’s common to observe instabilities in gradients, which can manifest as fluctuating validation performance.\nTo address this, one can for example freeze the pretrained layers (for some epochs) and only train the new output head. In mlr3torch, this can be achieved by using the t_clbk(\"unfreeze\") callback.\n\n\n\n\n\n\nIn-Context Learning\n\n\n\nLarge foundation models (such as GPT-4) even allow performing tasks on which they were not pretrained on without any finetuning. This is referred to as in-context learning or zero-shot learning. There, the task is fed into the model during inference: “Hey ChatGPT, is What is the sentiment of this sentence. Return -1 for sad, 0 for neutral, 1 for happy: ”"
  },
  {
    "objectID": "notebooks/5-cnn.html#convolutional-layers",
    "href": "notebooks/5-cnn.html#convolutional-layers",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\nThe central component of a CNN is the convolutional layer. It works by sliding a kernel over the input image, performing element-wise multiplication with the overlapping pixel values, and summing the results to produce a single output value for each position of the kernel.\n\nSource\nCNNs encode several strong inductive biases about visual data:\n\nLocality: Nearby pixels are more likely to be related than distant ones.\nTranslation Invariance: Features should be detected regardless of their position.\nHierarchical Composition: Complex patterns are built from simpler ones.\n\nThese biases make CNNs particularly effective for image-related tasks because they align with our understanding of how visual information is structured. As an example, we will apply a convolutional layer to an image from MNIST.\n\n\n\n\n\n\n\n\n\nSuch an image is a 3D tensor with dimensions [1, 28, 28], where the first dimension are the number of channels (would be 3 for RGB images, but MNIST is grayscale), and the other two dimensions are the spatial dimensions of the image.\n\nstr(image)\n\nFloat [1:1, 1:28, 1:28]\n\n\nTo create a convolutional layer for a 2D image, we can use the nn_conv2d function.\n\nconv_layer &lt;- nn_conv2d(in_channels = 1, out_channels = 1, kernel_size = 3, padding = 1)\nstr(conv_layer(image))\n\nFloat [1:1, 1:28, 1:28]\n\n\nBecause we have encoded more information about the structural relationship between the input tensor and the output tensor (the same filter is applied to the entire image), the convolutional layer has far fewer parameters than a fully connected layer.\n\nconv_layer$parameters\n\n$weight\ntorch_tensor\n(1,1,.,.) = \n  0.2229 -0.0028  0.1567\n  0.3100  0.3226  0.1191\n -0.3081  0.1674 -0.1031\n[ CPUFloatType{1,1,3,3} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n 0.1376\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n\nBelow, we show the output of the first convolutional layer from a (trained) ResNet18 model applied to an image from MNIST (which has 28x28 pixels).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeights of a Fully Connected Layer\n\n\n\nQuestion 1: How many parameters does a fully connected layer with the same number of inputs (1, 28, 28) and outputs (1, 28, 28) have?\n\n\nAnswer\n\nThe input has \\(28 \\times 28 = 784\\) pixels and the output as well. The weights of the fully connected layer are a \\(784 \\times 784\\) matrix and the bias also has 784 elements, so the number of parameters is \\(784 \\times 784 + 784 = 615440\\), much more than our simple convolutional kernel."
  },
  {
    "objectID": "notebooks/5-cnn.html#max-pooling",
    "href": "notebooks/5-cnn.html#max-pooling",
    "title": "Convolutional Neural Networks",
    "section": "Max Pooling",
    "text": "Max Pooling\nWhile convolutional layers extract local features from an image by applying a kernel over the input, max pooling is used to downsample the feature maps. Instead of applying a filter, max pooling simply partitions the input into non-overlapping (or sometimes overlapping) regions and selects the maximum value from each region. Below, we demonstrate it in action. We start by applying a convolutional layer to an MNIST image to obtain feature maps and then apply a 2×2 max pooling layer (with stride 2) on the convolution output. The two results are visualized side-by-side.\n\n# Create a max pooling layer with a 2x2 kernel and stride 2\npool_layer &lt;- nn_max_pool2d(kernel_size = 2, stride = 2)\n\n# Apply the convolutional layer to the image (as before)\nconv_output &lt;- conv_layer(image)\n\n# Now apply the max pooling layer to the result from the convolution.\npooled_output &lt;- pool_layer(conv_output)\n\n# Visualize the original convolution output and the pooled output.\np_conv &lt;- plot_2d_image(conv_output$squeeze())\np_pool &lt;- plot_2d_image(pooled_output$squeeze())\n\n# Use cowplot to display the two images side-by-side for easy comparison.\ncowplot::plot_grid(p_conv, p_pool, labels = c(\"Convolution Output\", \"After Max Pooling\"))"
  },
  {
    "objectID": "notebooks/4-optimizer.html",
    "href": "notebooks/4-optimizer.html",
    "title": "Optimization & Regularization",
    "section": "",
    "text": "In this notebook, we will focus on the optimization and regularization aspects of deep learning.\nOptimizers are algorithms that iteratively adjust the parameters of a neural network to minimize the loss function during training. They define how the networks learn from the data.\nLet’s denote \\(\\hat{\\mathcal{R}}(\\theta)\\) as the empirical risk function, which assigns the empirical risk given data \\(\\{(x^{(i)}, y^{(i)})\\}_{i = 1}^n\\) to a parameter vector \\(\\theta\\). Here, \\(f_\\theta\\) is the model’s prediction function, \\(x^{(i)}\\) is the \\(i\\)-th sample in the training data, and \\(y^{(i)}\\) is the corresponding target value. \\[\\hat{\\mathcal{R}}(\\theta) = \\frac{1}{n} \\sum_{i=1}^n L(f_\\theta(x^{(i)}), y^{(i)}) \\]\nOften, the empirical risk function is extended with a regularization term. Regularization in machine learning and statistics is used to prevent overfitting by adding a penalty term to the risk function, which discourages overly complex models that might fit noise in the training data. It helps improve generalization to unseen data. One common regularizer is the L2 norm of the parameter vector, which penalizes large coefficients by adding the squared magnitude of the coefficients to the loss function:\n\\[\n\\hat{\\mathcal{R}}_{\\text{reg}}(\\theta) = \\hat{\\mathcal{R}}(\\theta) + \\lambda \\sum_{j=1}^p \\theta_j^2\n\\]\nHere, \\(\\lambda\\) controls the strength of the regularization, i.e., the trade-off between fitting the training data and keeping the parameters small. This encourages the model to prefer less complex solutions, where complexity is measured by the L2 norm of the coefficients. As a result, parameter vectors will have entries closer to the zero vector, a concept known as parameter shrinkage.\nWhile the goal of the risk function is to define what we want, it’s the optimizer’s job to find the parameter vector \\(\\theta^*\\) that minimizes the empirical risk function. For simplicity, we will now refer to both the regularized and unregularized risk function as \\(\\hat{\\mathcal{R}}\\).\n\\[\\theta^* = \\arg \\min_\\theta \\hat{\\mathcal{R}}(\\theta)\\]\nThis is done by iteratively updating the parameter vector \\(\\theta\\) using the gradient of the loss function with respect to the parameter vector. The simplified update formula for a parameter \\(\\theta\\) at time step \\(t\\) is given by:\n\\[\\theta_{t+1} = \\theta_t - \\eta \\frac{\\partial \\hat{\\mathcal{R}}}{\\partial \\theta_t}\\]\nWhere:\nThe optimizers used in practice differ from the above formula, as:\nBefore we cover these more advanced approaches (specifically their implementation in AdamW), we will first focus on the vanilla version of Stochastic Gradient Descent (SGD)."
  },
  {
    "objectID": "notebooks/4-optimizer.html#mini-batch-effects-in-sgd",
    "href": "notebooks/4-optimizer.html#mini-batch-effects-in-sgd",
    "title": "Optimization & Regularization",
    "section": "Mini-Batch Effects in SGD",
    "text": "Mini-Batch Effects in SGD\nWhen using mini-batches, the gradient becomes a noisy estimate of the gradient over the full dataset. With \\(\\nabla L^{(i)}_t := \\frac{\\partial L^{(i)}}{\\partial \\theta_t}\\) being the gradient of the loss function with respect to the entire parameter vector estimated using \\((x^{(i)}, y^{(i)})\\), the mini-batch gradient is given by:\n\\[\\nabla L^B_t = \\frac{1}{|B|} \\sum_{i \\in B} \\nabla L^{(i)}_t\\]\nwhere \\(B\\) is the batch of samples and \\(|B|\\) is the batch size.\nThe update formula for SGD is then given by:\n\\[\\theta_{t+1} = \\theta_t - \\eta \\nabla L^B_t\\]\nThis is visualized in the image below:\n\n\n\n\n\n\n\nQuiz: Vanilla SGD\n\n\n\nQuestion 1: What happens when the batch size is too small or too large?\n\n\nClick for answer\n\nTrade-offs with Batch Size:\n\nLarger batches provide more accurate gradient estimates.\nSmaller batches introduce more noise but allow more frequent parameter updates.\n\n\nQuestion 2: The mini-batch gradient is an approximation of the gradient over the full dataset. Does the latter also approximate something? If so, what?\n\n\nClick for answer\n\nIn machine learning, we assume that the data is drawn from a distribution \\(P\\). The gradient over the full dataset approximates the expectation over this distribution:\n\\[\\nabla \\mathcal{R} = \\mathbb{E}_{x \\sim P} \\nabla \\mathcal{L}(f_\\theta(x), y)\\]\n\n\n\nBecause deep learning models can have many parameters and computing gradients is expensive, understanding the effects of different batch sizes and convergence is important. The computational cost (which we define as the time it takes to perform one optimization step) of a gradient update using a batch size \\(b\\) consists of:\n\nLoading the batch into memory (if the data does not fit into RAM).\nThe forward pass of the model.\nThe backward pass of the model.\nThe update of the parameters.\n\nWe will discuss point 1 later, and point 4 does not depend on the batch size, so we can ignore it.\n\n\n\n\n\n\nQuiz: Bang for Your Buck\n\n\n\nQuestion 1: True or false: The cost (duration) of performing a gradient update using a batch size of \\(2\\) is twice the cost of a batch size of \\(1\\).\n\n\nClick for answer\n\nFalse. Because GPUs can perform many operations simultaneously, the cost of performing a gradient update using a batch size of \\(2\\) is not twice the cost of a batch size of \\(1\\). The cost depends on many factors, but if the model is small, the cost of a batch with 2 observations might be almost the same as one with one observation.\n\nQuestion 2: The standard error of the mini-batch gradient estimate (which characterizes the precision of the gradient estimate) can be written as:\n\\[\\text{SE}_{\\nabla L^B_t} = \\frac{\\sigma_{\\nabla L_t}}{\\sqrt{|B|}}\\]\nwhere \\(\\sigma_{\\nabla L_t}\\) is the standard deviation of the gradient estimate relative to the batch size.\nDescribe the dynamics of the standard error when increasing the batch size: How do you need to increase a batch size from \\(1\\) to achieve half the standard error? What about increasing a batch size from \\(100\\)?\n\n\nClick for answer\n\nThe standard error decreases as the batch size increases, but with diminishing returns. To halve the standard error:\n\nIncrease the batch size from \\(1\\) to \\(4\\).\nIncrease the batch size from \\(100\\) to \\(400\\).\n\nThis is because the standard error is inversely proportional to the square root of the batch size."
  },
  {
    "objectID": "notebooks/4-optimizer.html#mini-batch-gradient-descent-its-not-all-about-runtime",
    "href": "notebooks/4-optimizer.html#mini-batch-gradient-descent-its-not-all-about-runtime",
    "title": "Optimization & Regularization",
    "section": "Mini-Batch Gradient Descent: It’s not all about runtime",
    "text": "Mini-Batch Gradient Descent: It’s not all about runtime\nAs we have now covered some of the dynamics of a simple gradient-based optimizer, we can examine the final parameter vector \\(\\theta^*\\) that the optimizer converges to. When using a gradient-based optimizer, the updates will stop once the gradient is close to zero. We will now discuss the type of solutions where this is true and their properties.\nWe need to distinguish saddle points from local minima from global minima:\n\nIn deep learning, where high-dimensional parameter spaces are common, saddle points are more likely to occur than local minima. However, due to the stochastic nature of SGD, optimizers will find local minima instead of saddle points.\n\n\n\n\n\n\nQuiz: Local vs. Global Minima, Generalization\n\n\n\nQuestion 1: Do you believe SGD will find local or global minima? Explain your reasoning.\n\n\nClick for answer\n\nBecause the gradient only has local information about the loss function, SGD finds local minima.\n\nQuestion 2: Assuming we have found a \\(\\theta^*\\) that has low training loss, does this ensure that we have found a good model?\n\n\nClick for answer\n\nNo, because we only know that the model has low training loss, but not necessarily low test loss.\n\n\n\nSGD has been empirically shown to find solutions that generalize well to unseen data. This phenomenon is attributed to the implicit regularization effects of SGD, where the noise introduced by mini-batch sampling helps guide the optimizer towards broader minima with smaller L2 norms. These broader minima are typically associated with better generalization performance compared to sharp minima.\n\nSource\nThese properties are also known as implicit regularization of SGD. Regularization generally refers to techniques that prevent overfitting and improve generalization. There are also explicit regularization techniques, which we will cover next.\n\nWeight Decay\nOne modification to the SGD update formula is the so-called weight decay, which is equivalent to adding a regularization penalty term to the loss function as we have seen earlier.\n\n\n\n\n\n\nNote\n\n\n\nFor more complex optimizers such as Adam, weight decay is not equivalent to adding a regularization penalty term to the loss function (Loshchilov 2017). However, the main idea of both approaches is still to shrink the weights to \\(0\\) during training.\n\n\nIf we integrate weight decay into the gradient update formula, we get the following:\n\\[\\theta_{t+1} = \\theta_t - \\eta \\big(\\frac{\\partial L}{\\partial \\theta_t} - \\lambda \\theta_t\\big)\\]\nThis formula shows that the weight decay term (\\(- \\lambda \\theta_t\\)) effectively shrinks the weights during each update, helping to prevent overfitting.\n\nSource\n\n\nMomentum\nMomentum is a technique that helps accelerate gradient descent by using an exponential moving average of past gradients. Like a ball rolling down a hill, momentum helps the optimizer:\n\nMove faster through areas of consistent gradient direction.\nPush through sharp local minima and saddle points.\nDampen oscillations in areas where the gradient frequently changes direction.\n\nThe exponential moving momentum update can be expressed mathematically as:\n\\[\n(1 - \\beta) \\sum_{\\tau=1}^{t} \\beta^{t-\\tau} \\nabla_{\\theta} \\mathcal{L}(\\theta_{\\tau-1})\n\\]\nIn order to avoid having to keep track of all the gradients, we can calculate the update in two steps as follows:\n\\[\nv_t = \\beta_1 v_{t-1} + (1 - \\beta_1) \\nabla_\\theta L(\\theta_t)\n\\]\n\\[\n\\theta_{t+1} = \\theta_t - \\eta \\frac{v_t}{1 - \\beta_1^t}\n\\]\nThe hyperparameter \\(\\beta_1\\) is the momentum decay rate (typically 0.9), \\(v_t\\) is the exponential moving average of gradients, and \\(\\eta\\) is the learning rate as before. Note that dividing by \\(1 - \\beta_1^t\\) counteracts a bias because \\(v_0\\) is initialized to \\(0\\).\n\nSource\n\n\nAdaptive Learning Rates\nAdaptive learning rate methods automatically adjust the learning rate for each parameter during training. This is particularly useful because:\n\nDifferent parameters may require different learning rates.\nThe optimal learning rate often changes during training.\n\nBefore, we had one global learning rate \\(\\eta\\) for all parameters. However, learning rates are now allowed to:\n\nChange over time.\nBe different for different parameters.\n\nOur vanilla SGD update formula is now generalized to handle adaptive learning rates:\n\\[\\theta_{t+1} = \\theta_t - \\eta_t \\cdot \\frac{\\nabla_\\theta L(\\theta_t)}{\\sqrt{v_t} + \\epsilon}\\]\nHere, \\(\\eta_t\\) is now not a scalar learning rate, but a vector of learning rates for each parameter, and ‘\\(\\cdot\\)’ denotes the element-wise multiplication. Further, \\(\\epsilon\\) is a small constant for numerical stability.\nIn AdamW, the adaptive learning rate is controlled by the second moment estimate (squared gradients):\n\\[v_t = \\beta_2 v_{t-1} + (1-\\beta_2)(g_t)^2\\] \\[\\hat{\\eta}_t = \\eta \\frac{1}{\\sqrt{v_t + \\epsilon}}\\]\nIn words, this means: In steep directions where the gradient is large, the learning rate is small and vice versa. The parameters \\(\\beta_2\\) and \\(\\epsilon\\) are hyperparameters that control the decay rate and numerical stability of the second moment estimate.\n\nWhen combining weight decay, adaptive learning rates, and momentum, we get the AdamW optimizer. It therefore has parameters:\n\nlr: The learning rate.\nweight_decay: The weight decay parameter.\nbetas: The momentum parameters (\\(\\beta_1\\) and \\(\\beta_2\\)).\neps: The numerical stability parameter.\n\nNote that AdamW also has another configuration parameter amsgrad, which is disabled by default in torch, but which can help with convergence."
  },
  {
    "objectID": "notebooks/4-optimizer.html#learning-rate-schedules",
    "href": "notebooks/4-optimizer.html#learning-rate-schedules",
    "title": "Optimization & Regularization",
    "section": "Learning Rate Schedules",
    "text": "Learning Rate Schedules\nWhile we have already covered dynamic learning rates, it can still be beneficial to use a learning rate scheduler to further improve convergence. Like for adaptive learning rates, the learning rate is then not a constant scalar, but a function of the current epoch or iteration. Note that the learning rate schedulers discussed here can also be combined with adaptive learning rates such as in AdamW and are not mutually exclusive.\n\\[\\theta_{t+1} = \\theta_t - \\eta_t \\cdot \\frac{\\nabla_\\theta L(\\theta_t)}{\\sqrt{v_t} + \\epsilon}\\]\nDecaying learning rates:\nThis includes gradient decay, cosine annealing, and cyclical learning rates. The general idea is to start with a high learning rate and then gradually decrease it over time.\nWarmup:\nWarmup is a technique that gradually increases the learning rate from a small value to a larger value over a specified number of epochs. This ensures that in the beginning, where the weights are randomly initialized, the learning rate is not too high.\nCyclical Learning Rates:\nCyclical learning rates are a technique that involves periodically increasing and decreasing the learning rate. This can help the optimizer to traverse saddle points faster and find better solutions.\nThe different schedules are visualized below:\n\n\n\n\n\n\n\n\n\nIn torch, learning rate schedulers are prefixed by lr_, such as the simple lr_step, where the learning rate is multiplied by a factor of gamma every step_size epochs. In order to use them, we need to pass the optimizer to the scheduler and specify additional arguments.\n\nscheduler = lr_step(opt, step_size = 2, gamma = 0.1)\n\nThe main API of a learning rate scheduler is the $step() method, which updates the learning rate. For some schedulers, this needs to be called after each optimization step, for others after each epoch. You can find this out by consulting the documentation of the specific scheduler.\n\nopt$param_groups[[1L]]$lr\n\n[1] 0.2\n\nscheduler$step()\nopt$param_groups[[1L]]$lr\n\n[1] 0.2\n\nscheduler$step()\nopt$param_groups[[1L]]$lr\n\n[1] 0.02"
  },
  {
    "objectID": "notebooks/4-optimizer.html#saving-an-optimizer",
    "href": "notebooks/4-optimizer.html#saving-an-optimizer",
    "title": "Optimization & Regularization",
    "section": "Saving an Optimizer",
    "text": "Saving an Optimizer\nIn order to resume training at a later stage, we can save the optimizer’s state using $state_dict().\n\nstate_dict = opt$state_dict()\n\nThis state dictionary contains:\n\nThe $param_groups which contains the parameters and their associated hyperparameters.\nThe $state which contains the optimizer’s internal state, such as the momentum and second moment estimates.\n\n\nstate_dict$param_groups[[1L]]\n\n$params\n[1] 1 2\n\n$lr\n[1] 0.02\n\n$betas\n[1] 0.900 0.999\n\n$eps\n[1] 1e-08\n\n$weight_decay\n[1] 0.01\n\n$amsgrad\n[1] FALSE\n\n$initial_lr\n[1] 0.2\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is possible to set different parameters (such as learning rate) for different parameter groups.\n\no2 = optim_adamw(list(\n  list(params = torch_tensor(1), lr = 1),\n  list(params = torch_tensor(2), lr = 2)\n))\no2$param_groups[[1L]]$lr\n\n[1] 1\n\no2$param_groups[[2L]]$lr\n\n[1] 2\n\n\n\n\nThe $state field contains the state for each parameter:\n\nstate_dict$state\n\n$`1`\n$`1`$step\ntorch_tensor\n1\n[ CPUFloatType{} ]\n\n$`1`$exp_avg\ntorch_tensor\n0.01 *\n 5.9926\n[ CPUFloatType{1,1} ]\n\n$`1`$exp_avg_sq\ntorch_tensor\n0.0001 *\n 3.5912\n[ CPUFloatType{1,1} ]\n\n\n$`2`\n$`2`$step\ntorch_tensor\n1\n[ CPUFloatType{} ]\n\n$`2`$exp_avg\ntorch_tensor\n0.01 *\n 2.1779\n[ CPUFloatType{1} ]\n\n$`2`$exp_avg_sq\ntorch_tensor\n1e-05 *\n 4.7433\n[ CPUFloatType{1} ]\n\n\nJust like for the nn_module, we can save the optimizer state using torch_save().\n\npth = tempfile(fileext = \".pth\")\ntorch_save(state_dict, pth)\n\n\n\n\n\n\n\nWarning\n\n\n\nGenerally, we don’t want to save the whole optimizer, as this also contains the weight tensors of the model that one usually wants to save separately.\n\n\nWe can load the optimizer state again using torch_load().\n\nstate_dict2 = torch_load(pth)\nopt2 &lt;- optim_adamw(model$parameters, lr = 0.2)\nopt2$load_state_dict(state_dict2)"
  },
  {
    "objectID": "notebooks/4-optimizer.html#embedded-regularization-techniques",
    "href": "notebooks/4-optimizer.html#embedded-regularization-techniques",
    "title": "Optimization & Regularization",
    "section": "Embedded Regularization Techniques",
    "text": "Embedded Regularization Techniques\nBesides the explicit regularization effects of weight decay and the implicit regularization effects of mini-batch gradient descent, there are also other regularization techniques that improve generalization of deep neural networks. Here, we focus on dropout and layer normalization, which are both embedded in neural network architectures.\n\nDropout\nDropout is a regularization technique used to prevent overfitting in neural networks. During each training iteration, dropout randomly “drops” a subset of neurons by setting their activations to zero with a specified probability. This forces the network to distribute the learned representations more evenly across neurons. Dropout is most commonly used in the context of fully connected layers.\n\n\n\n\n\nSource\nNote that neurons are only dropped when the module is in train mode, not in eval mode.\n\nx = torch_randn(10, 5)\ndropout = nn_dropout(p = 0.5)\ndropout(x)\n\ntorch_tensor\n-0.0000  3.7545  0.0000  0.0000  0.0585\n 0.0000  0.0000  1.8560 -0.0000  1.0625\n 0.0000  0.3431  0.8545 -1.1291 -0.2658\n-0.0383  2.3208 -0.3911  0.0000  0.0000\n 0.6493 -0.0000  2.7401  0.0000  0.0000\n 0.0000  0.0000 -0.0000  2.9860  0.0000\n-0.0000 -0.0000  0.0000  0.0000 -0.0000\n-0.8820  1.6693 -0.9363 -0.0480 -0.7703\n-0.9378 -0.0000  0.0000  0.0000 -0.0000\n-0.5347  0.0000  0.0000  1.0171 -2.8396\n[ CPUFloatType{10,5} ]\n\ndropout$eval()\ndropout(x)\n\ntorch_tensor\n-0.4076  1.8772  2.1884  0.3678  0.0293\n 0.2379  0.3571  0.9280 -0.9107  0.5313\n 0.1559  0.1716  0.4273 -0.5645 -0.1329\n-0.0192  1.1604 -0.1956  1.2885  0.3373\n 0.3247 -1.6603  1.3701  1.0716  0.2485\n 0.3478  2.6682 -0.4942  1.4930  0.1250\n-0.4989 -1.0193  0.3470  0.2577 -0.4072\n-0.4410  0.8347 -0.4681 -0.0240 -0.3851\n-0.4689 -0.1484  1.8486  0.8354 -1.1521\n-0.2673  0.6042  0.4407  0.5085 -1.4198\n[ CPUFloatType{10,5} ]\n\n\n\n\n\n\n\n\nQuiz: Dropout\n\n\n\nQuestion 1: Worse Training Loss: You are training a neural network with and without dropout. The training loss is higher with dropout, is this a bug?\n\n\nClick for answer\n\nNot necessarily, as dropout is a regularization technique that prevents overfitting. Its goal is to reduce the generalization performance of the model and not to improve training performance.\n\n\n\n\n\nBatch Normalization\nBatch Normalization is an important technique in deep learning that contributed significantly to speeding up the training process, especially in convolutional neural networks that are covered in the next chapter. During training, batch normalization introduces noise into the network by normalizing each mini-batch independently. Besides faster congerence, batch normalization also acts as a regularizer, where the model learns to be less sensitive to the specific details of the training data, thus reducing overfitting.\nThe formula for batch normalization (during training) is given by:\n\\[\n\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n\\]\nwhere:\n\n\\(\\hat{x}\\) is the normalized output,\n\\(x\\) is the input,\n\\(\\mu_B\\) is the mean of the batch,\n\\(\\sigma_B^2\\) is the variance of the batch,\n\\(\\epsilon\\) is a small constant added for numerical stability.\n\nDuring inference, the module uses the running mean and variance of the training data to normalize the input.\nIn torch, different versions of batch normalization exist for different dimensions of the input tensor. Below, we illustrate the batch normalization module using a 1D input tensor (the batch dimension does not count here):\n\nx = torch_randn(10, 5)\nbn = nn_batch_norm1d(num_features = 5)\nbn(x)\n\ntorch_tensor\n-0.6913 -0.3529  1.3228  0.7018 -0.3640\n 0.6402 -2.2711  1.4172 -1.1553 -0.5314\n-0.4089 -0.3850 -0.2296  1.0142 -1.8982\n-0.1549  0.5493 -1.8220 -0.3830  0.8069\n 1.3912 -0.1972  0.0693  1.3237 -1.2300\n-1.9593  0.9884 -0.0327  0.3435  1.2988\n 1.6690  0.6317 -0.5951 -1.1106  0.9754\n-0.3695  1.6290 -1.1394 -0.9881  0.7726\n-0.3884 -0.4044  1.0969  1.3280  0.6179\n 0.2720 -0.1878 -0.0873 -1.0742 -0.4482\n[ CPUFloatType{10,5} ][ grad_fn = &lt;NativeBatchNormBackward0&gt; ]\n\n\n\n\n\n\n\n\nQuiz: Batch Normalization\n\n\n\nQuestion 1: Earlier we have learned that nn_modules have buffers and parameters, where only the latter are learned with gradient descent. Do you think the mean and variance are parameters or buffers?\n\n\nClick for answer\n\nThey are both buffers as they only store the variance and running mean of all training samples seen, i.e., they are not updated using gradient information.\n\nQuestion 2: Training vs. Evaluation Mode: While many nn_modules behave the same way irrespective of their mode, batch normalization is an example of a module that behaves differently during training and evaluation. During training, the module uses the mean and variance of the current batch, while during evaluation, it uses the running mean and variance of all training samples seen.\n\nbn(x[1:10, ])\n\ntorch_tensor\n-0.6913 -0.3529  1.3228  0.7018 -0.3640\n 0.6402 -2.2711  1.4172 -1.1553 -0.5314\n-0.4089 -0.3850 -0.2296  1.0142 -1.8982\n-0.1549  0.5493 -1.8220 -0.3830  0.8069\n 1.3912 -0.1972  0.0693  1.3237 -1.2300\n-1.9593  0.9884 -0.0327  0.3435  1.2988\n 1.6690  0.6317 -0.5951 -1.1106  0.9754\n-0.3695  1.6290 -1.1394 -0.9881  0.7726\n-0.3884 -0.4044  1.0969  1.3280  0.6179\n 0.2720 -0.1878 -0.0873 -1.0742 -0.4482\n[ CPUFloatType{10,5} ][ grad_fn = &lt;NativeBatchNormBackward0&gt; ]\n\n\nWhich of the following statements is true and why?\n\nbn$eval()\nequal1 = torch_equal(\n  torch_cat(list(bn(x[1:2, ]), bn(x[3:4, ]))),\n  bn(x[1:4, ])\n)\nbn$train()\nequal2 = torch_equal(\n  torch_cat(list(bn(x[1:2, ]), bn(x[3:4, ]))),\n  bn(x[1:4, ])\n)\n\n\n\nClick for answer\n\n\nc(equal1, equal2)\n\n[1]  TRUE FALSE\n\n\nThe first statement is true because, in evaluation mode, the module uses the running mean and variance of all training samples seen. The second statement is false because the first tensor uses different means and variances for rows 1-2 and 3-4, while the second tensor uses the same mean and variance for all rows.\n\n\n\nTo demonstrate these dropout, we apply them to a simple spam classification task. The data has one binary target variable type (spam or no spam) and 57 numerical features.\n\nc(nrow(spam), ncol(spam))\n\n[1] 4601   58\n\ntable(spam$type)\n\n\n   spam nonspam \n   1813    2788 \n\n\nBelow, we create a simple neural network with two hidden layers of dimension 100, ReLU activation and optionally dropout and batch normalization.\n\nnn_reg &lt;- nn_module(\"nn_reg\",\n  initialize = function(dropout, batch_norm) {\n    self$net &lt;- nn_sequential(\n      nn_linear(in_features = 57, out_features = 100),\n      if (batch_norm) nn_batch_norm1d(num_features = 100) else nn_identity(),\n      if (dropout) nn_dropout(p = 0.5) else nn_identity(),\n      nn_relu(),\n      nn_linear(in_features = 100, out_features = 100),\n      if (batch_norm) nn_batch_norm1d(num_features = 100) else nn_identity(),\n      if (dropout) nn_dropout(p = 0.5) else nn_identity(),\n      nn_relu(),\n      nn_linear(in_features = 100, out_features = 2)\n    )\n  },\n  forward = function(x) {\n    self$net(x)\n  }\n)\nnn_drop &lt;- nn_reg(dropout = TRUE, batch_norm = FALSE)\nnn_batch &lt;- nn_reg(dropout = FALSE, batch_norm = TRUE)\nnn_both &lt;- nn_reg(dropout = TRUE, batch_norm = TRUE)\nnn_vanilla &lt;- nn_reg(dropout = FALSE, batch_norm = FALSE)\n\nWe evaluate the performance of the four neural networks created above using subsampling with 10 repetitions and an 80/20 train/test split. We don’t show the specific training code here, but only the resulting confidence intervals for the accuracy. While the intervals are too wide to be able to draw any final conclusions and we are only looking at a single dataset, both normalization techniques tend to lead to better results. Also, both normalization techniques improve stability, so that the confidence intervals are narrower."
  }
]