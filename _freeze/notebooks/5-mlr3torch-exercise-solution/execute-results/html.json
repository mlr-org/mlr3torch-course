{
  "hash": "82a43718cf694c85422e0a1e6826db9f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Training Neural Networks with mlr3torch\"\nsolutions: true\n---\n\n---\ntitle: \"Training Neural Networks with mlr3torch\"\n---\n\n\n\n\n\n\n**Question 1:** Hello World!\n\nIn this exercise, you will train your first neural network with `mlr3torch`.\n\nAs a task, we will use the 'Indian Liver Patient' dataset where the goal is to predict whether a patient has liver disease or not.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\nlibrary(mlr3torch)\nilpd <- tsk(\"ilpd\")\nilpd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskClassif:ilpd> (583 x 11): Indian Liver Patient Data\n* Target: diseased\n* Properties: twoclass\n* Features (10):\n  - dbl (5): albumin, albumin_globulin_ratio, direct_bilirubin, total_bilirubin, total_protein\n  - int (4): age, alanine_transaminase, alkaline_phosphatase, aspartate_transaminase\n  - fct (1): gender\n```\n\n\n:::\n\n```{.r .cell-code}\nautoplot(ilpd)\n```\n\n::: {.cell-output-display}\n![](5-mlr3torch-exercise-solution_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe remove the *gender* column from the task, so we need to only deal with numeric features for now.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nilpd_num = ilpd$clone(deep = TRUE)\nilpd_num$select(setdiff(ilpd_num$feature_names, \"gender\"))\n```\n:::\n\n\n\nYour task is to train a simple multi layer perceptron (`lrn(\"classif.mlp\")`) with 2 hidden layers with 100 neurons each.\nSet the batch size to 32, the learning rate to 0.001 and the number of epochs to 20.\nThen, resample the learner on the task with a cross-validation with 5 folds and evaluate the results using classification error and false positive rate (FPR).\nIs the result good?\n\n<details>\n<summary>Hint</summary>\nThe parameter for the learning rate is `opt.lr`\n</details>\n\n\n::: {.content-visible when-meta=solutions}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp <- lrn(\"classif.mlp\",\n  neurons = c(100, 100),\n  batch_size = 32,\n  epochs = 20,\n  opt.lr = 0.001\n)\ncv10 <- rsmp(\"cv\", folds = 10)\nrr1 <- resample(task = ilpd_num, learner = mlp, resampling = cv10)\nrr1$aggregate(msrs(c(\"classif.ce\", \"classif.fpr\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n classif.ce classif.fpr \n  0.3706312   0.5849387 \n```\n\n\n:::\n:::\n\n\nWhile the classification error is low, this is not a good measure due to the imbalanced class distribution.\nThis is confirmed by the FPR, which is relatively high.\n:::\n\n**Question 2:** Preprocessing\n\nIn the previous question, we have operated on the `ilpd_num` task where we excluded the categorical `gender` column.\nThis was done because the MLP learner operates on numeric features only.\nYou will now create a more complex `GraphLearner` that also incudes one-hot encoding of the `gender` column before applying the MLP.\nResample this learner on the original `ilpd` task and evaluate the results using the same measures as before.\n\n<details>\n<summary>Hint</summary>\nConcatenate `po(\"encode\")` with a `lrn(\"classif.mlp\")` using `%>>%` to create the `GraphLearner`.\nFor available options on the encoding, see `po(\"encode\")$help()`.\n</details>\n\n:::{.content-visible when-meta=solutions}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nencoder <- po(\"encode\", method = \"one-hot\")\nglrn <- as_learner(encoder %>>% mlp)\nrr2 <- resample(ilpd, glrn, cv10)\nrr2$aggregate(msrs(c(\"classif.ce\", \"classif.fpr\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n classif.ce classif.fpr \n  0.3137347   0.8365021 \n```\n\n\n:::\n:::\n\n:::\n\n**Question 3**: Benchmarking\n\nInstead of resampling a single learner, the goal is now to compare the performance of the MLP with a simple classification tree.\nCreate a benchmark design and compare the performance of the two learners.\n\n<details>\n<summary>Hint</summary>\nCreate a classification tree via `lrn(\"classif.rpart\")`.\nA benchmark design can be created via `benchmark_grid()`.\nTo run a benchmark, pass the design to `benchmark()`.\n</details>\n\n\n:::{.content-visible when-meta=solutions}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndesign <- benchmark_grid(\n  task = ilpd,\n  learner = list(glrn, lrn(\"classif.rpart\", predict_type = \"prob\")),\n  resampling = cv10\n)\nbmr <- benchmark(design)\nbmr$aggregate(msrs(c(\"classif.ce\", \"classif.tpr\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      nr task_id         learner_id resampling_id iters classif.ce classif.tpr\n   <int>  <char>             <char>        <char> <int>      <num>       <num>\n1:     1    ilpd encode.classif.mlp            cv    10  0.2812683   0.9145299\n2:     2    ilpd      classif.rpart            cv    10  0.3035652   0.8301628\nHidden columns: resample_result\n```\n\n\n:::\n:::\n\n:::\n\n\n**Question 4:** Iris as a Lazy Tensor\n\nCreate a version of the iris task where the 4 features are represented as a single lazy tensor column.\n\n:::{.content-visible when-meta=solutions}\n\n**Solution:**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nX <- as.matrix(iris[, 1:4])\ndt_class <- dataset(\"iris\",\n  initialize = function(x) {\n    self$x <- torch_tensor(x)\n  },\n  .getbatch = function(index) {\n    list(x = self$x[index, , drop = FALSE])\n  },\n  .length = function() {\n    nrow(self$x)\n  }\n)\ndt <- dt_class(X)\nlt <- as_lazy_tensor(dt, dataset_shapes = list(x = c(NA, 4)))\ndt <- data.table(\n  x = lt,\n  Species = iris$Species\n)\ntask <- as_task_classif(dt, target = \"Species\")\ntask\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskClassif:dt> (150 x 2)\n* Target: Species\n* Properties: multiclass\n* Features (1):\n  - lt (1): x\n```\n\n\n:::\n:::\n\n:::\n\n\n**Question 5:** Custom Architecture\n\nCreate a network with one hidden layer with 100 neurons and a sigmoid activation function by assempling `PipeOp`s in a `Graph`.\nConvert the `Graph` to a `Learner` and train the network for 10 epochs using Adam with a learning rate of 0.001 and a batch size of 32.\n\n:::{.content-visible when-meta=solutions}\n**Solution:**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph <- po(\"torch_ingress_ltnsr\") %>>%\n  po(\"nn_linear\", out_features = 100) %>>%\n  po(\"nn_sigmoid\") %>>%\n  po(\"nn_head\") %>>%\n  po(\"torch_loss\", t_loss(\"cross_entropy\")) %>>%\n  po(\"torch_optimizer\", optimizer = t_opt(\"adam\"), lr = 0.001) %>>%\n  po(\"torch_model_classif\", batch_size = 32, epochs = 10)\n\nglrn <- as_learner(graph)\nglrn$train(task)\n```\n:::\n\n:::\n\n",
    "supporting": [
      "5-mlr3torch-exercise-solution_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}